{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6bc8f2",
   "metadata": {},
   "source": [
    "# Session Explorer\n",
    "This notebook lets you explore recorded robot sessions, scrub through frames, run the autoencoder and predictor models on stored observations, and train the models on selected frames/history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957658",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "1. Pick a session and load it.\n",
    "2. Use the playback controls to scrub through frames.\n",
    "3. (Optional) Load model checkpoints, then run the Autoencoder and Predictor sections using the current frame selection.\n",
    "4. (Optional) Use the Training sections to train models on current frame/history until a loss threshold is met or for a specified number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8527353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from functools import lru_cache\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import config\n",
    "from models import MaskedAutoencoderViT, TransformerActionConditionedPredictor\n",
    "from adaptive_world_model import AdaptiveWorldModel\n",
    "from robot_interface import RobotInterface\n",
    "\n",
    "# Additional imports for training\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6399c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "No autoencoder checkpoint found\n"
     ]
    }
   ],
   "source": [
    "SESSIONS_BASE_DIR = config.RECORDING_BASE_DIR\n",
    "DEFAULT_AUTOENCODER_PATH = os.path.join(config.DEFAULT_CHECKPOINT_DIR, \"autoencoder.pth\")\n",
    "DEFAULT_PREDICTOR_PATH = os.path.join(config.DEFAULT_CHECKPOINT_DIR, \"predictor_0.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a stub robot interface for the AdaptiveWorldModel\n",
    "class StubRobotInterface(RobotInterface):\n",
    "    \"\"\"Stub robot interface for notebook training purposes\"\"\"\n",
    "    def get_observation(self):\n",
    "        # Return a dummy observation - not used in training\n",
    "        return {\"frame\": np.zeros((224, 224, 3), dtype=np.uint8)}\n",
    "    \n",
    "    def execute_action(self, action):\n",
    "        # Do nothing - not used in training\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        # Return minimal action space - not used in training\n",
    "        return [{\"motor_left\": 0, \"motor_right\": 0, \"duration\": 0.2}]\n",
    "    \n",
    "    def cleanup(self):\n",
    "        pass\n",
    "\n",
    "# Instantiate AdaptiveWorldModel for training access\n",
    "stub_robot = StubRobotInterface()\n",
    "adaptive_world_model = AdaptiveWorldModel(stub_robot, wandb_project=None, checkpoint_dir=config.DEFAULT_CHECKPOINT_DIR, interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2f58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility helpers for loading sessions, caching frames, and preparing model inputs\n",
    "def list_session_dirs(base_dir):\n",
    "    # Return sorted session directory names.\n",
    "    if not os.path.exists(base_dir):\n",
    "        return []\n",
    "    entries = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        path = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(path) and name.startswith(\"session_\"):\n",
    "            entries.append(name)\n",
    "    entries.sort()\n",
    "    return entries\n",
    "\n",
    "def load_session_metadata(session_dir):\n",
    "    meta_path = os.path.join(session_dir, \"session_meta.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def load_session_events(session_dir):\n",
    "    # Load all events from shard files and sort them by step.\n",
    "    pattern = os.path.join(session_dir, \"events_shard_*.jsonl\")\n",
    "    shard_files = sorted(glob.glob(pattern))\n",
    "    events = []\n",
    "    for shard_path in shard_files:\n",
    "        with open(shard_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                events.append(json.loads(line))\n",
    "    events.sort(key=lambda evt: evt.get(\"step\", 0))\n",
    "    return events\n",
    "\n",
    "def extract_observations(events, session_dir):\n",
    "    observations = []\n",
    "    for idx, event in enumerate(events):\n",
    "        if event.get(\"type\") != \"observation\":\n",
    "            continue\n",
    "        data = event.get(\"data\", {})\n",
    "        frame_path = data.get(\"frame_path\")\n",
    "        if not frame_path:\n",
    "            continue\n",
    "        observations.append({\n",
    "            \"observation_index\": len(observations),\n",
    "            \"event_index\": idx,\n",
    "            \"step\": event.get(\"step\", idx),\n",
    "            \"timestamp\": event.get(\"timestamp\"),\n",
    "            \"frame_path\": frame_path,\n",
    "            \"full_path\": os.path.join(session_dir, frame_path),\n",
    "        })\n",
    "    return observations\n",
    "\n",
    "def extract_actions(events):\n",
    "    actions = []\n",
    "    for idx, event in enumerate(events):\n",
    "        if event.get(\"type\") != \"action\":\n",
    "            continue\n",
    "        actions.append({\n",
    "            \"action_index\": len(actions),\n",
    "            \"event_index\": idx,\n",
    "            \"step\": event.get(\"step\", idx),\n",
    "            \"timestamp\": event.get(\"timestamp\"),\n",
    "            \"action\": event.get(\"data\", {}),\n",
    "        })\n",
    "    return actions\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def load_frame_bytes(full_path):\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Frame file not found: {full_path}\")\n",
    "    with open(full_path, \"rb\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_frame_image(full_path):\n",
    "    return Image.open(io.BytesIO(load_frame_bytes(full_path))).convert(\"RGB\")\n",
    "\n",
    "tensor_cache = {}\n",
    "\n",
    "def get_frame_tensor(session_dir, frame_path):\n",
    "    # Return normalized (C,H,W) tensor for a frame, cached on CPU.\n",
    "    key = (session_dir, frame_path)\n",
    "    if key not in tensor_cache:\n",
    "        full_path = os.path.join(session_dir, frame_path)\n",
    "        pil_img = load_frame_image(full_path)\n",
    "        tensor_cache[key] = config.TRANSFORM(pil_img)\n",
    "    return tensor_cache[key]\n",
    "\n",
    "def tensor_to_numpy_image(tensor):\n",
    "    if tensor.ndim == 4:\n",
    "        tensor = tensor[0]\n",
    "    tensor = tensor.detach().cpu().float()\n",
    "    tensor = tensor * 0.5 + 0.5\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "    return tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "def format_timestamp(ts):\n",
    "    if ts is None:\n",
    "        return \"N/A\"\n",
    "    try:\n",
    "        return datetime.datetime.fromtimestamp(ts).isoformat()\n",
    "    except Exception:\n",
    "        return str(ts)\n",
    "\n",
    "def describe_action(action):\n",
    "    if not action:\n",
    "        return \"{}\"\n",
    "    parts = []\n",
    "    for key in sorted(action.keys()):\n",
    "        parts.append(f\"{key}: {action[key]}\")\n",
    "    return \", \".join(parts)\n",
    "\n",
    "def canonical_action_key(action):\n",
    "    if not action:\n",
    "        return ()\n",
    "    return tuple(sorted(action.items()))\n",
    "\n",
    "def get_action_space(session_state):\n",
    "    metadata_actions = session_state.get(\"metadata\", {}).get(\"action_space\") or []\n",
    "    if metadata_actions:\n",
    "        return metadata_actions\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for action_entry in session_state.get(\"actions\", []):\n",
    "        action = action_entry.get(\"action\", {})\n",
    "        key = canonical_action_key(action)\n",
    "        if key and key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(action)\n",
    "    return unique\n",
    "\n",
    "def format_action_label(action):\n",
    "    if not action:\n",
    "        return \"{}\"\n",
    "    parts = []\n",
    "    for key in sorted(action.keys()):\n",
    "        parts.append(f\"{key}={action[key]}\")\n",
    "    return \", \".join(parts)\n",
    "\n",
    "def clone_action(action):\n",
    "    if not action:\n",
    "        return {}\n",
    "    return {key: float(value) if isinstance(value, (int, float)) else value for key, value in action.items()}\n",
    "\n",
    "def actions_equal(action_a, action_b):\n",
    "    return canonical_action_key(action_a) == canonical_action_key(action_b)\n",
    "\n",
    "def load_autoencoder_model(path, device):\n",
    "    model = MaskedAutoencoderViT()\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_predictor_model(path, device):\n",
    "    model = TransformerActionConditionedPredictor()\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    level = checkpoint.get(\"level\")\n",
    "    if level is not None:\n",
    "        model.level = level\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def decode_features_to_image(autoencoder, predicted_features):\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        num_patches = autoencoder.patch_embed.num_patches\n",
    "        ids_restore = torch.arange(num_patches, device=predicted_features.device).unsqueeze(0).repeat(predicted_features.shape[0], 1)\n",
    "        pred_patches = autoencoder.forward_decoder(predicted_features, ids_restore)\n",
    "        decoded = autoencoder.unpatchify(pred_patches)\n",
    "    return decoded\n",
    "\n",
    "def build_predictor_sequence(session_state, target_obs_index, desired_length):\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    events = session_state.get(\"events\", [])\n",
    "    if not observations:\n",
    "        return [], [], \"No observations loaded.\"\n",
    "    if target_obs_index < 0 or target_obs_index >= len(observations):\n",
    "        return [], [], \"Selected observation is out of range.\"\n",
    "    desired_length = max(2, desired_length)\n",
    "    selected_obs = [observations[target_obs_index]]\n",
    "    action_dicts = []\n",
    "    current_idx = target_obs_index\n",
    "    current_event_index = observations[target_obs_index][\"event_index\"]\n",
    "    while len(selected_obs) < desired_length and current_idx > 0:\n",
    "        prev_idx = current_idx - 1\n",
    "        found = False\n",
    "        while prev_idx >= 0:\n",
    "            prev_obs = observations[prev_idx]\n",
    "            prev_event_index = prev_obs[\"event_index\"]\n",
    "            actions_between = [events[i] for i in range(prev_event_index + 1, current_event_index) if events[i].get(\"type\") == \"action\"]\n",
    "            if len(actions_between) == 1:\n",
    "                action_dicts.insert(0, actions_between[0].get(\"data\", {}))\n",
    "                selected_obs.insert(0, prev_obs)\n",
    "                current_idx = prev_idx\n",
    "                current_event_index = prev_event_index\n",
    "                found = True\n",
    "                break\n",
    "            prev_idx -= 1\n",
    "        if not found:\n",
    "            break\n",
    "    if len(selected_obs) < 2:\n",
    "        return [], [], \"Could not assemble a history with actions between frames. Choose a later frame.\"\n",
    "    return selected_obs, action_dicts, None\n",
    "\n",
    "def find_action_between_events(events, start_event_index, end_event_index):\n",
    "    \"\"\"Return the recorded action between two observation events, falling back to the prior action.\"\"\"\n",
    "    between_actions = [\n",
    "        event for event in events[start_event_index + 1:end_event_index]\n",
    "        if event.get(\"type\") == \"action\"\n",
    "    ]\n",
    "    if between_actions:\n",
    "        return clone_action(between_actions[-1].get(\"data\", {})), \"between\"\n",
    "\n",
    "    for idx in range(start_event_index, -1, -1):\n",
    "        event = events[idx]\n",
    "        if event.get(\"type\") == \"action\":\n",
    "            return clone_action(event.get(\"data\", {})), \"previous\"\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_future_action_for_prediction(session_state, target_obs_index):\n",
    "    \"\"\"Return the action to pair with the next observation for prediction.\"\"\"\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    events = session_state.get(\"events\", [])\n",
    "    if target_obs_index < 0 or target_obs_index >= len(observations) - 1:\n",
    "        return None, None\n",
    "    current_obs = observations[target_obs_index]\n",
    "    next_obs = observations[target_obs_index + 1]\n",
    "    return find_action_between_events(events, current_obs[\"event_index\"], next_obs[\"event_index\"])\n",
    "\n",
    "def visualize_autoencoder_weights(autoencoder):\n",
    "    \"\"\"Visualize key autoencoder weights for monitoring changes\"\"\"\n",
    "    if autoencoder is None:\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get patch embedding weights (first layer)\n",
    "        patch_embed_weight = autoencoder.patch_embed.proj.weight.detach().cpu()\n",
    "        \n",
    "        # Get cls token and pos embed\n",
    "        cls_token = autoencoder.cls_token.detach().cpu()\n",
    "        pos_embed = autoencoder.pos_embed.detach().cpu()\n",
    "        \n",
    "        # Get some decoder weights\n",
    "        decoder_embed_weight = autoencoder.decoder_embed.weight.detach().cpu() if hasattr(autoencoder, 'decoder_embed') else None\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Patch embedding weights - show first 16 filters\n",
    "        patch_weights_vis = patch_embed_weight[:16].view(16, 3, 16, 16)\n",
    "        patch_grid = torch.cat([torch.cat([patch_weights_vis[i*4+j] for j in range(4)], dim=2) for i in range(4)], dim=1)\n",
    "        patch_grid = (patch_grid - patch_grid.min()) / (patch_grid.max() - patch_grid.min())\n",
    "        axes[0, 0].imshow(patch_grid.permute(1, 2, 0))\n",
    "        axes[0, 0].set_title(\"Patch Embed Weights (16 filters)\")\n",
    "        axes[0, 0].axis(\"off\")\n",
    "        \n",
    "        # Patch embedding weight statistics\n",
    "        axes[0, 1].hist(patch_embed_weight.flatten().numpy(), bins=50, alpha=0.7)\n",
    "        axes[0, 1].set_title(f\"Patch Embed Weight Distribution\\nMean: {patch_embed_weight.mean():.6f}, Std: {patch_embed_weight.std():.6f}\")\n",
    "        axes[0, 1].set_xlabel(\"Weight Value\")\n",
    "        axes[0, 1].set_ylabel(\"Count\")\n",
    "        \n",
    "        # CLS token visualization\n",
    "        cls_reshaped = cls_token.view(-1).numpy()\n",
    "        axes[0, 2].plot(cls_reshaped)\n",
    "        axes[0, 2].set_title(f\"CLS Token\\nMean: {cls_token.mean():.6f}, Std: {cls_token.std():.6f}\")\n",
    "        axes[0, 2].set_xlabel(\"Dimension\")\n",
    "        axes[0, 2].set_ylabel(\"Value\")\n",
    "        \n",
    "        # Position embedding visualization (first 100 dimensions)\n",
    "        pos_vis = pos_embed[0, :, :100].numpy()\n",
    "        im = axes[1, 0].imshow(pos_vis, aspect='auto', cmap='coolwarm')\n",
    "        axes[1, 0].set_title(f\"Position Embeddings (first 100 dims)\\nShape: {pos_embed.shape}\")\n",
    "        axes[1, 0].set_xlabel(\"Embedding Dimension\")\n",
    "        axes[1, 0].set_ylabel(\"Position\")\n",
    "        plt.colorbar(im, ax=axes[1, 0])\n",
    "        \n",
    "        # Decoder embedding weights if available\n",
    "        if decoder_embed_weight is not None:\n",
    "            axes[1, 1].hist(decoder_embed_weight.flatten().numpy(), bins=50, alpha=0.7, color='orange')\n",
    "            axes[1, 1].set_title(f\"Decoder Embed Weight Distribution\\nMean: {decoder_embed_weight.mean():.6f}, Std: {decoder_embed_weight.std():.6f}\")\n",
    "            axes[1, 1].set_xlabel(\"Weight Value\")\n",
    "            axes[1, 1].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, \"Decoder weights\\nnot available\", ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title(\"Decoder Weights\")\n",
    "        \n",
    "        # Weight norms across layers\n",
    "        layer_norms = []\n",
    "        layer_names = []\n",
    "        for name, param in autoencoder.named_parameters():\n",
    "            if 'weight' in name and param.dim() >= 2:\n",
    "                layer_norms.append(param.norm().item())\n",
    "                layer_names.append(name.split('.')[-2] if '.' in name else name)\n",
    "        \n",
    "        if layer_norms:\n",
    "            axes[1, 2].bar(range(len(layer_norms)), layer_norms)\n",
    "            axes[1, 2].set_title(\"Layer Weight Norms\")\n",
    "            axes[1, 2].set_xlabel(\"Layer\")\n",
    "            axes[1, 2].set_ylabel(\"L2 Norm\")\n",
    "            axes[1, 2].set_xticks(range(0, len(layer_names), max(1, len(layer_names)//5)))\n",
    "            axes[1, 2].set_xticklabels([layer_names[i] for i in range(0, len(layer_names), max(1, len(layer_names)//5))], rotation=45)\n",
    "        else:\n",
    "            axes[1, 2].text(0.5, 0.5, \"No weight layers\\nfound\", ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "            axes[1, 2].set_title(\"Layer Weight Norms\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Return weight statistics for comparison\n",
    "        stats = {\n",
    "            'patch_embed_mean': patch_embed_weight.mean().item(),\n",
    "            'patch_embed_std': patch_embed_weight.std().item(),\n",
    "            'cls_token_mean': cls_token.mean().item(),\n",
    "            'cls_token_std': cls_token.std().item(),\n",
    "            'pos_embed_mean': pos_embed.mean().item(),\n",
    "            'pos_embed_std': pos_embed.std().item(),\n",
    "            'layer_norms': dict(zip(layer_names, layer_norms)) if layer_norms else {}\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "def visualize_predictor_weights(predictor):\n",
    "    \"\"\"Visualize key predictor (transformer) weights for monitoring changes\"\"\"\n",
    "    if predictor is None:\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get various transformer weights based on actual architecture\n",
    "        # Action embedding weights  \n",
    "        action_embed_weight = predictor.action_embedding.weight.detach().cpu() if hasattr(predictor, 'action_embedding') else None\n",
    "        \n",
    "        # Position embedding weights\n",
    "        pos_embed_weight = predictor.position_embedding.weight.detach().cpu() if hasattr(predictor, 'position_embedding') else None\n",
    "        \n",
    "        # Token type embedding weights\n",
    "        token_type_weight = predictor.token_type_embedding.weight.detach().cpu() if hasattr(predictor, 'token_type_embedding') else None\n",
    "        \n",
    "        # Future query parameter\n",
    "        future_query_weight = predictor.future_query.detach().cpu() if hasattr(predictor, 'future_query') else None\n",
    "        \n",
    "        # Get first transformer layer weights (TransformerEncoderLayer structure)\n",
    "        first_layer_self_attn_weight = None\n",
    "        first_layer_linear1_weight = None\n",
    "        first_layer_linear2_weight = None\n",
    "        \n",
    "        if hasattr(predictor, 'transformer_layers') and len(predictor.transformer_layers) > 0:\n",
    "            first_layer = predictor.transformer_layers[0]\n",
    "            \n",
    "            # Self-attention weights (in_proj_weight contains Q, K, V)\n",
    "            if hasattr(first_layer, 'self_attn') and hasattr(first_layer.self_attn, 'in_proj_weight'):\n",
    "                first_layer_self_attn_weight = first_layer.self_attn.in_proj_weight.detach().cpu()\n",
    "            \n",
    "            # MLP weights\n",
    "            if hasattr(first_layer, 'linear1') and hasattr(first_layer.linear1, 'weight'):\n",
    "                first_layer_linear1_weight = first_layer.linear1.weight.detach().cpu()\n",
    "            if hasattr(first_layer, 'linear2') and hasattr(first_layer.linear2, 'weight'):\n",
    "                first_layer_linear2_weight = first_layer.linear2.weight.detach().cpu()\n",
    "        \n",
    "        # Output head weights\n",
    "        output_head_weight = predictor.output_head.weight.detach().cpu() if hasattr(predictor, 'output_head') and hasattr(predictor.output_head, 'weight') else None\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Action embedding weights\n",
    "        if action_embed_weight is not None:\n",
    "            axes[0, 0].hist(action_embed_weight.flatten().numpy(), bins=50, alpha=0.7, color='blue')\n",
    "            axes[0, 0].set_title(f\"Action Embed Weight Distribution\\nMean: {action_embed_weight.mean():.6f}, Std: {action_embed_weight.std():.6f}\")\n",
    "            axes[0, 0].set_xlabel(\"Weight Value\")\n",
    "            axes[0, 0].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, \"Action embedding\\nweights not found\", ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "            axes[0, 0].set_title(\"Action Embedding Weights\")\n",
    "        \n",
    "        # Position embedding weights\n",
    "        if pos_embed_weight is not None:\n",
    "            axes[0, 1].hist(pos_embed_weight.flatten().numpy(), bins=50, alpha=0.7, color='green')\n",
    "            axes[0, 1].set_title(f\"Position Embed Weight Distribution\\nMean: {pos_embed_weight.mean():.6f}, Std: {pos_embed_weight.std():.6f}\")\n",
    "            axes[0, 1].set_xlabel(\"Weight Value\")\n",
    "            axes[0, 1].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, \"Position embedding\\nweights not found\", ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title(\"Position Embedding Weights\")\n",
    "        \n",
    "        # First transformer layer self-attention weights\n",
    "        if first_layer_self_attn_weight is not None:\n",
    "            axes[0, 2].hist(first_layer_self_attn_weight.flatten().numpy(), bins=50, alpha=0.7, color='red')\n",
    "            axes[0, 2].set_title(f\"First Layer Self-Attn Weights\\nMean: {first_layer_self_attn_weight.mean():.6f}, Std: {first_layer_self_attn_weight.std():.6f}\")\n",
    "            axes[0, 2].set_xlabel(\"Weight Value\")\n",
    "            axes[0, 2].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[0, 2].text(0.5, 0.5, \"Self-attention\\nweights not found\", ha='center', va='center', transform=axes[0, 2].transAxes)\n",
    "            axes[0, 2].set_title(\"First Layer Self-Attn\")\n",
    "        \n",
    "        # First transformer layer linear1 (MLP) weights\n",
    "        if first_layer_linear1_weight is not None:\n",
    "            axes[1, 0].hist(first_layer_linear1_weight.flatten().numpy(), bins=50, alpha=0.7, color='purple')\n",
    "            axes[1, 0].set_title(f\"First Layer Linear1 Weights\\nMean: {first_layer_linear1_weight.mean():.6f}, Std: {first_layer_linear1_weight.std():.6f}\")\n",
    "            axes[1, 0].set_xlabel(\"Weight Value\")\n",
    "            axes[1, 0].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, \"Linear1 weights\\nnot found\", ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title(\"First Layer Linear1\")\n",
    "        \n",
    "        # Output head weights\n",
    "        if output_head_weight is not None:\n",
    "            axes[1, 1].hist(output_head_weight.flatten().numpy(), bins=50, alpha=0.7, color='orange')\n",
    "            axes[1, 1].set_title(f\"Output Head Weight Distribution\\nMean: {output_head_weight.mean():.6f}, Std: {output_head_weight.std():.6f}\")\n",
    "            axes[1, 1].set_xlabel(\"Weight Value\")\n",
    "            axes[1, 1].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, \"Output head\\nweights not found\", ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title(\"Output Head Weights\")\n",
    "        \n",
    "        # Weight norms across all layers\n",
    "        layer_norms = []\n",
    "        layer_names = []\n",
    "        for name, param in predictor.named_parameters():\n",
    "            if 'weight' in name and param.dim() >= 2:\n",
    "                layer_norms.append(param.norm().item())\n",
    "                # Shorten layer names for display\n",
    "                short_name = name.split('.')[-1] if '.' in name else name\n",
    "                if len(short_name) > 15:\n",
    "                    short_name = short_name[:12] + \"...\"\n",
    "                layer_names.append(short_name)\n",
    "        \n",
    "        if layer_norms:\n",
    "            axes[1, 2].bar(range(len(layer_norms)), layer_norms)\n",
    "            axes[1, 2].set_title(\"Layer Weight Norms\")\n",
    "            axes[1, 2].set_xlabel(\"Layer\")\n",
    "            axes[1, 2].set_ylabel(\"L2 Norm\")\n",
    "            # Show every 5th label to avoid overcrowding\n",
    "            step = max(1, len(layer_names)//8)\n",
    "            axes[1, 2].set_xticks(range(0, len(layer_names), step))\n",
    "            axes[1, 2].set_xticklabels([layer_names[i] for i in range(0, len(layer_names), step)], rotation=45, ha='right')\n",
    "        else:\n",
    "            axes[1, 2].text(0.5, 0.5, \"No weight layers\\nfound\", ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "            axes[1, 2].set_title(\"Layer Weight Norms\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Return weight statistics for comparison\n",
    "        stats = {}\n",
    "        if action_embed_weight is not None:\n",
    "            stats.update({\n",
    "                'action_embed_mean': action_embed_weight.mean().item(),\n",
    "                'action_embed_std': action_embed_weight.std().item(),\n",
    "            })\n",
    "        if pos_embed_weight is not None:\n",
    "            stats.update({\n",
    "                'pos_embed_mean': pos_embed_weight.mean().item(),\n",
    "                'pos_embed_std': pos_embed_weight.std().item(),\n",
    "            })\n",
    "        if first_layer_self_attn_weight is not None:\n",
    "            stats.update({\n",
    "                'first_self_attn_mean': first_layer_self_attn_weight.mean().item(),\n",
    "                'first_self_attn_std': first_layer_self_attn_weight.std().item(),\n",
    "            })\n",
    "        if first_layer_linear1_weight is not None:\n",
    "            stats.update({\n",
    "                'first_linear1_mean': first_layer_linear1_weight.mean().item(),\n",
    "                'first_linear1_std': first_layer_linear1_weight.std().item(),\n",
    "            })\n",
    "        if output_head_weight is not None:\n",
    "            stats.update({\n",
    "                'output_head_mean': output_head_weight.mean().item(),\n",
    "                'output_head_std': output_head_weight.std().item(),\n",
    "            })\n",
    "        if layer_norms:\n",
    "            stats['layer_norms'] = dict(zip(layer_names, layer_norms))\n",
    "        \n",
    "        return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ixrc3gkvood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper functions using AdaptiveWorldModel\n",
    "def train_autoencoder_step_wrapper(frame_tensor):\n",
    "    \"\"\"Single autoencoder training step using AdaptiveWorldModel\"\"\"\n",
    "    # Convert tensor to numpy frame for AdaptiveWorldModel\n",
    "    frame_numpy = tensor_to_numpy_image(frame_tensor)\n",
    "    \n",
    "    # Use AdaptiveWorldModel's train_autoencoder method\n",
    "    loss = adaptive_world_model.train_autoencoder(frame_numpy)\n",
    "    return loss\n",
    "\n",
    "def train_predictor_step_wrapper(target_idx, history_features, history_actions):\n",
    "    \"\"\"Single predictor training step using AdaptiveWorldModel\"\"\"\n",
    "    # Get target frame\n",
    "    next_obs = session_state[\"observations\"][target_idx + 1]\n",
    "    target_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "    target_frame = tensor_to_numpy_image(target_tensor)\n",
    "    \n",
    "    # Set up prediction context in AdaptiveWorldModel\n",
    "    adaptive_world_model.observation_history = []\n",
    "    adaptive_world_model.action_history = []\n",
    "    \n",
    "    # Add history to the world model\n",
    "    for i, (feat, action) in enumerate(zip(history_features, history_actions)):\n",
    "        # Convert feature back to frame if needed\n",
    "        obs = session_state[\"observations\"][target_idx - len(history_features) + 1 + i]\n",
    "        frame_tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0)\n",
    "        frame_numpy = tensor_to_numpy_image(frame_tensor)\n",
    "        \n",
    "        adaptive_world_model.observation_history.append(frame_numpy)\n",
    "        adaptive_world_model.action_history.append(action)\n",
    "    \n",
    "    # Train predictor level 0\n",
    "    loss = adaptive_world_model.train_predictor(0, target_tensor)\n",
    "    return loss\n",
    "\n",
    "def format_loss(loss_value):\n",
    "    \"\"\"Format loss value for display\"\"\"\n",
    "    if loss_value < 0.001:\n",
    "        return f\"{loss_value:.2e}\"\n",
    "    else:\n",
    "        return f\"{loss_value:.6f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ob17o8vzup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Training Section using AdaptiveWorldModel\n",
    "training_widgets = {}\n",
    "\n",
    "def on_train_autoencoder_threshold(_):\n",
    "    \"\"\"Train autoencoder until threshold is met using AdaptiveWorldModel\"\"\"\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    if autoencoder is None:\n",
    "        with training_widgets[\"autoencoder_training_output\"]:\n",
    "            training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "        return\n",
    "    \n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with training_widgets[\"autoencoder_training_output\"]:\n",
    "            training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        return\n",
    "    \n",
    "    # Get training parameters\n",
    "    threshold = training_widgets[\"autoencoder_threshold\"].value\n",
    "    max_steps = training_widgets[\"autoencoder_max_steps\"].value\n",
    "    \n",
    "    # Setup for training\n",
    "    idx = frame_slider.value\n",
    "    observation = session_state.get(\"observations\", [])[idx]\n",
    "    frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "    \n",
    "    with training_widgets[\"autoencoder_training_output\"]:\n",
    "        training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "        \n",
    "        # Show pre-training weights\n",
    "        display(Markdown(f\"**Training autoencoder using AdaptiveWorldModel on frame {idx+1} (step {observation['step']})**\"))\n",
    "        display(Markdown(f\"Target threshold: {format_loss(threshold)}, Max steps: {max_steps}\"))\n",
    "        display(Markdown(\"### Pre-Training Network Weights\"))\n",
    "        pre_stats = visualize_autoencoder_weights(autoencoder)\n",
    "        \n",
    "        display(Markdown(\"**Using AdaptiveWorldModel.train_autoencoder() method with randomized masking**\"))\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(range(max_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress:\n",
    "            loss = train_autoencoder_step_wrapper(frame_tensor)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "            \n",
    "            # Check if threshold met\n",
    "            if loss <= threshold:\n",
    "                progress.close()\n",
    "                break\n",
    "        else:\n",
    "            progress.close()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float('inf')\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"**Training completed after {len(losses)} steps in {end_time-start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Final loss: {format_loss(final_loss)}\"))\n",
    "        \n",
    "        if final_loss <= threshold:\n",
    "            display(Markdown(f\"✅ **Target threshold {format_loss(threshold)} achieved!**\"))\n",
    "        else:\n",
    "            display(Markdown(f\"⚠️ **Target threshold {format_loss(threshold)} not reached after {max_steps} steps**\"))\n",
    "        \n",
    "        # Show post-training weights\n",
    "        display(Markdown(\"### Post-Training Network Weights\"))\n",
    "        post_stats = visualize_autoencoder_weights(autoencoder)\n",
    "        \n",
    "        # Compare weight changes\n",
    "        if pre_stats and post_stats:\n",
    "            weight_changes = {\n",
    "                'patch_embed_mean_change': abs(post_stats['patch_embed_mean'] - pre_stats['patch_embed_mean']),\n",
    "                'patch_embed_std_change': abs(post_stats['patch_embed_std'] - pre_stats['patch_embed_std']),\n",
    "                'cls_token_mean_change': abs(post_stats['cls_token_mean'] - pre_stats['cls_token_mean']),\n",
    "                'cls_token_std_change': abs(post_stats['cls_token_std'] - pre_stats['cls_token_std']),\n",
    "            }\n",
    "            changes_text = f\"\"\"\n",
    "**Weight Changes:**\n",
    "- Patch Embed Mean: {weight_changes['patch_embed_mean_change']:.8f}\n",
    "- Patch Embed Std: {weight_changes['patch_embed_std_change']:.8f}\n",
    "- CLS Token Mean: {weight_changes['cls_token_mean_change']:.8f}\n",
    "- CLS Token Std: {weight_changes['cls_token_std_change']:.8f}\n",
    "            \"\"\"\n",
    "            display(Markdown(changes_text))\n",
    "        \n",
    "        # Plot training progress\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Reconstruction Loss\")\n",
    "            ax.set_title(\"Autoencoder Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.axhline(y=threshold, color='r', linestyle='--', alpha=0.7, label=f'Target: {format_loss(threshold)}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show final reconstruction\n",
    "        adaptive_world_model.autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = adaptive_world_model.autoencoder.reconstruct(frame_tensor)\n",
    "        \n",
    "        original_img = tensor_to_numpy_image(frame_tensor)\n",
    "        reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"After Training (Loss: {format_loss(final_loss)})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def on_train_autoencoder_steps(_):\n",
    "    \"\"\"Train autoencoder for specified number of steps using AdaptiveWorldModel\"\"\"\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    if autoencoder is None:\n",
    "        with training_widgets[\"autoencoder_training_output\"]:\n",
    "            training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "        return\n",
    "    \n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with training_widgets[\"autoencoder_training_output\"]:\n",
    "            training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        return\n",
    "    \n",
    "    # Get training parameters\n",
    "    num_steps = training_widgets[\"autoencoder_steps\"].value\n",
    "    \n",
    "    # Setup for training\n",
    "    idx = frame_slider.value\n",
    "    observation = session_state.get(\"observations\", [])[idx]\n",
    "    frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "    \n",
    "    with training_widgets[\"autoencoder_training_output\"]:\n",
    "        training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "        \n",
    "        # Show pre-training weights\n",
    "        display(Markdown(f\"**Training autoencoder using AdaptiveWorldModel on frame {idx+1} (step {observation['step']}) for {num_steps} steps**\"))\n",
    "        display(Markdown(\"### Pre-Training Network Weights\"))\n",
    "        pre_stats = visualize_autoencoder_weights(autoencoder)\n",
    "        \n",
    "        display(Markdown(\"**Using AdaptiveWorldModel.train_autoencoder() method with randomized masking**\"))\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(range(num_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress:\n",
    "            loss = train_autoencoder_step_wrapper(frame_tensor)\n",
    "            losses.append(loss)\n",
    "            progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "        \n",
    "        progress.close()\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float('inf')\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"**Training completed in {end_time-start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Initial loss: {format_loss(losses[0])}, Final loss: {format_loss(final_loss)}\"))\n",
    "        \n",
    "        # Show post-training weights\n",
    "        display(Markdown(\"### Post-Training Network Weights\"))\n",
    "        post_stats = visualize_autoencoder_weights(autoencoder)\n",
    "        \n",
    "        # Compare weight changes\n",
    "        if pre_stats and post_stats:\n",
    "            weight_changes = {\n",
    "                'patch_embed_mean_change': abs(post_stats['patch_embed_mean'] - pre_stats['patch_embed_mean']),\n",
    "                'patch_embed_std_change': abs(post_stats['patch_embed_std'] - pre_stats['patch_embed_std']),\n",
    "                'cls_token_mean_change': abs(post_stats['cls_token_mean'] - pre_stats['cls_token_mean']),\n",
    "                'cls_token_std_change': abs(post_stats['cls_token_std'] - pre_stats['cls_token_std']),\n",
    "            }\n",
    "            changes_text = f\"\"\"\n",
    "**Weight Changes:**\n",
    "- Patch Embed Mean: {weight_changes['patch_embed_mean_change']:.8f}\n",
    "- Patch Embed Std: {weight_changes['patch_embed_std_change']:.8f}\n",
    "- CLS Token Mean: {weight_changes['cls_token_mean_change']:.8f}\n",
    "- CLS Token Std: {weight_changes['cls_token_std_change']:.8f}\n",
    "            \"\"\"\n",
    "            display(Markdown(changes_text))\n",
    "        \n",
    "        # Plot training progress\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Reconstruction Loss\")\n",
    "            ax.set_title(\"Autoencoder Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show final reconstruction\n",
    "        adaptive_world_model.autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = adaptive_world_model.autoencoder.reconstruct(frame_tensor)\n",
    "        \n",
    "        original_img = tensor_to_numpy_image(frame_tensor)\n",
    "        reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"After Training (Loss: {format_loss(final_loss)})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create autoencoder training widgets\n",
    "autoencoder_threshold = widgets.FloatText(value=0.0005, description=\"Threshold\", step=0.0001, style={'description_width': '100px'})\n",
    "autoencoder_max_steps = widgets.IntText(value=1000, description=\"Max Steps\", style={'description_width': '100px'})\n",
    "autoencoder_steps = widgets.IntText(value=100, description=\"Steps\", style={'description_width': '100px'})\n",
    "\n",
    "train_autoencoder_threshold_button = widgets.Button(description=\"Train to Threshold\", button_style=\"warning\", icon=\"target\")\n",
    "train_autoencoder_steps_button = widgets.Button(description=\"Train N Steps\", button_style=\"warning\", icon=\"forward\")\n",
    "autoencoder_training_output = widgets.Output()\n",
    "\n",
    "training_widgets.update({\n",
    "    \"autoencoder_threshold\": autoencoder_threshold,\n",
    "    \"autoencoder_max_steps\": autoencoder_max_steps,\n",
    "    \"autoencoder_steps\": autoencoder_steps,\n",
    "    \"autoencoder_training_output\": autoencoder_training_output,\n",
    "})\n",
    "\n",
    "train_autoencoder_threshold_button.on_click(on_train_autoencoder_threshold)\n",
    "train_autoencoder_steps_button.on_click(on_train_autoencoder_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flq0j1fn1ql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor Training Section using AdaptiveWorldModel\n",
    "def on_train_predictor_threshold(_):\n",
    "    \"\"\"Train predictor until threshold is met using AdaptiveWorldModel\"\"\"\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    predictor = adaptive_world_model.predictors[0] if adaptive_world_model.predictors else None\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    \n",
    "    with training_widgets[\"predictor_training_output\"]:\n",
    "        training_widgets[\"predictor_training_output\"].clear_output()\n",
    "        \n",
    "        if autoencoder is None or predictor is None:\n",
    "            display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "            return\n",
    "        if frame_slider is None:\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "            return\n",
    "        \n",
    "        # Get training parameters\n",
    "        threshold = training_widgets[\"predictor_threshold\"].value\n",
    "        max_steps = training_widgets[\"predictor_max_steps\"].value\n",
    "        \n",
    "        target_idx = frame_slider.value\n",
    "        history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "        desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "        \n",
    "        selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "        if error:\n",
    "            display(Markdown(f\"**Cannot train predictor:** {error}\"))\n",
    "            return\n",
    "        \n",
    "        # Check if we have a next frame for training target\n",
    "        if target_idx + 1 >= len(session_state[\"observations\"]):\n",
    "            display(Markdown(\"**Cannot train predictor:** No next frame available as training target.\"))\n",
    "            return\n",
    "        \n",
    "        # Get target frame tensor\n",
    "        next_obs = session_state[\"observations\"][target_idx + 1]\n",
    "        target_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get feature history and setup context\n",
    "        feature_history = []\n",
    "        for obs in selected_obs:\n",
    "            tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "            autoencoder.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded = autoencoder.encode(tensor).detach()\n",
    "            feature_history.append(encoded)\n",
    "\n",
    "        recorded_future_action, action_source = get_future_action_for_prediction(session_state, target_idx)\n",
    "        info_message = None\n",
    "        if recorded_future_action is None:\n",
    "            info_message = \"No recorded action between current and next frame; using empty action.\"\n",
    "            recorded_future_action = {}\n",
    "        elif action_source == \"previous\":\n",
    "            info_message = \"Using the most recent action prior to the current frame.\"\n",
    "        if info_message:\n",
    "            display(Markdown(info_message))\n",
    "\n",
    "        history_actions_with_future = [clone_action(action) for action in action_dicts]\n",
    "        history_actions_with_future.append(clone_action(recorded_future_action))\n",
    "\n",
    "        display(Markdown(f\"**Training predictor using AdaptiveWorldModel on history ending at frame {target_idx+1} (step {selected_obs[-1]['step']})**\"))\n",
    "        display(Markdown(f\"Target threshold: {format_loss(threshold)}, Max steps: {max_steps}\"))\n",
    "        display(Markdown(f\"History length: {len(selected_obs)} frames\"))\n",
    "        \n",
    "        # Show pre-training weights\n",
    "        display(Markdown(\"### Pre-Training Predictor Network Weights\"))\n",
    "        pre_stats = visualize_predictor_weights(predictor)\n",
    "        \n",
    "        display(Markdown(f\"**Using AdaptiveWorldModel.train_predictor() method with joint training**\"))\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(range(max_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress:\n",
    "            # Use AdaptiveWorldModel's train_predictor method\n",
    "            try:\n",
    "                # Set up prediction context for fresh predictions\n",
    "                predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "                \n",
    "                loss = adaptive_world_model.train_predictor(\n",
    "                    level=0,\n",
    "                    current_frame_tensor=target_tensor,\n",
    "                    predicted_features=predicted_features,\n",
    "                    history_features=feature_history,\n",
    "                    history_actions=history_actions_with_future\n",
    "                )\n",
    "                losses.append(loss)\n",
    "                \n",
    "                progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "                \n",
    "                # Check if threshold met\n",
    "                if loss <= threshold:\n",
    "                    progress.close()\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                progress.close()\n",
    "                display(Markdown(f\"**Training error:** {str(e)}\"))\n",
    "                return\n",
    "        else:\n",
    "            progress.close()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float('inf')\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"**Training completed after {len(losses)} steps in {end_time-start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Final total loss: {format_loss(final_loss)}\"))\n",
    "        \n",
    "        if final_loss <= threshold:\n",
    "            display(Markdown(f\"✅ **Target threshold {format_loss(threshold)} achieved!**\"))\n",
    "        else:\n",
    "            display(Markdown(f\"⚠️ **Target threshold {format_loss(threshold)} not reached after {max_steps} steps**\"))\n",
    "        \n",
    "        # Show post-training weights\n",
    "        display(Markdown(\"### Post-Training Predictor Network Weights\"))\n",
    "        post_stats = visualize_predictor_weights(predictor)\n",
    "        \n",
    "        # Compare weight changes\n",
    "        if pre_stats and post_stats:\n",
    "            weight_changes = {}\n",
    "            if 'action_embed_mean' in pre_stats and 'action_embed_mean' in post_stats:\n",
    "                weight_changes.update({\n",
    "                    'action_embed_mean_change': abs(post_stats['action_embed_mean'] - pre_stats['action_embed_mean']),\n",
    "                    'action_embed_std_change': abs(post_stats['action_embed_std'] - pre_stats['action_embed_std']),\n",
    "                })\n",
    "            if 'pos_embed_mean' in pre_stats and 'pos_embed_mean' in post_stats:\n",
    "                weight_changes.update({\n",
    "                    'pos_embed_mean_change': abs(post_stats['pos_embed_mean'] - pre_stats['pos_embed_mean']),\n",
    "                    'pos_embed_std_change': abs(post_stats['pos_embed_std'] - pre_stats['pos_embed_std']),\n",
    "                })\n",
    "            if 'first_self_attn_mean' in pre_stats and 'first_self_attn_mean' in post_stats:\n",
    "                weight_changes.update({\n",
    "                    'first_self_attn_mean_change': abs(post_stats['first_self_attn_mean'] - pre_stats['first_self_attn_mean']),\n",
    "                    'first_self_attn_std_change': abs(post_stats['first_self_attn_std'] - pre_stats['first_self_attn_std']),\n",
    "                })\n",
    "            if 'first_linear1_mean' in pre_stats and 'first_linear1_mean' in post_stats:\n",
    "                weight_changes.update({\n",
    "                    'first_linear1_mean_change': abs(post_stats['first_linear1_mean'] - pre_stats['first_linear1_mean']),\n",
    "                    'first_linear1_std_change': abs(post_stats['first_linear1_std'] - pre_stats['first_linear1_std']),\n",
    "                })\n",
    "            \n",
    "            if weight_changes:\n",
    "                changes_lines = [\"**Weight Changes:**\"]\n",
    "                if 'action_embed_mean_change' in weight_changes:\n",
    "                    changes_lines.extend([\n",
    "                        f\"- Action Embed Mean: {weight_changes['action_embed_mean_change']:.8f}\",\n",
    "                        f\"- Action Embed Std: {weight_changes['action_embed_std_change']:.8f}\",\n",
    "                    ])\n",
    "                if 'pos_embed_mean_change' in weight_changes:\n",
    "                    changes_lines.extend([\n",
    "                        f\"- Position Embed Mean: {weight_changes['pos_embed_mean_change']:.8f}\",\n",
    "                        f\"- Position Embed Std: {weight_changes['pos_embed_std_change']:.8f}\",\n",
    "                    ])\n",
    "                if 'first_self_attn_mean_change' in weight_changes:\n",
    "                    changes_lines.extend([\n",
    "                        f\"- First Self-Attn Mean: {weight_changes['first_self_attn_mean_change']:.8f}\",\n",
    "                        f\"- First Self-Attn Std: {weight_changes['first_self_attn_std_change']:.8f}\",\n",
    "                    ])\n",
    "                if 'first_linear1_mean_change' in weight_changes:\n",
    "                    changes_lines.extend([\n",
    "                        f\"- First Linear1 Mean: {weight_changes['first_linear1_mean_change']:.8f}\",\n",
    "                        f\"- First Linear1 Std: {weight_changes['first_linear1_std_change']:.8f}\",\n",
    "                    ])\n",
    "                display(Markdown(\"\\n\".join(changes_lines)))\n",
    "        \n",
    "        # Plot training progress\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses, label=\"Total Loss\")\n",
    "            ax.axhline(y=threshold, color='r', linestyle='--', alpha=0.7, label=f'Target: {format_loss(threshold)}')\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.set_title(\"Predictor Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show prediction comparison\n",
    "        predictor.eval()\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "            predicted_frame = decode_features_to_image(autoencoder, predicted_features)\n",
    "        \n",
    "        predicted_img = tensor_to_numpy_image(predicted_frame)\n",
    "        target_img = tensor_to_numpy_image(target_tensor)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(predicted_img)\n",
    "        axes[0].set_title(f\"Predicted (Loss: {format_loss(final_loss)})\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(target_img)\n",
    "        axes[1].set_title(f\"Actual (step {next_obs['step']})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def on_train_predictor_steps(_):\n",
    "    \"\"\"Train predictor for specified number of steps using AdaptiveWorldModel\"\"\"\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    predictor = adaptive_world_model.predictors[0] if adaptive_world_model.predictors else None\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    \n",
    "    with training_widgets[\"predictor_training_output\"]:\n",
    "        training_widgets[\"predictor_training_output\"].clear_output()\n",
    "        \n",
    "        if autoencoder is None or predictor is None:\n",
    "            display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "            return\n",
    "        if frame_slider is None:\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "            return\n",
    "        \n",
    "        # Get training parameters\n",
    "        num_steps = training_widgets[\"predictor_steps\"].value\n",
    "        \n",
    "        target_idx = frame_slider.value\n",
    "        history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "        desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "        \n",
    "        selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "        if error:\n",
    "            display(Markdown(f\"**Cannot train predictor:** {error}\"))\n",
    "            return\n",
    "        \n",
    "        # Check if we have a next frame for training target\n",
    "        if target_idx + 1 >= len(session_state[\"observations\"]):\n",
    "            display(Markdown(\"**Cannot train predictor:** No next frame available as training target.\"))\n",
    "            return\n",
    "        \n",
    "        # Get target frame tensor\n",
    "        next_obs = session_state[\"observations\"][target_idx + 1]\n",
    "        target_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Get feature history and setup context\n",
    "        feature_history = []\n",
    "        for obs in selected_obs:\n",
    "            tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "            autoencoder.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded = autoencoder.encode(tensor).detach()\n",
    "            feature_history.append(encoded)\n",
    "\n",
    "        recorded_future_action, action_source = get_future_action_for_prediction(session_state, target_idx)\n",
    "        info_message = None\n",
    "        if recorded_future_action is None:\n",
    "            info_message = \"No recorded action between current and next frame; using empty action.\"\n",
    "            recorded_future_action = {}\n",
    "        elif action_source == \"previous\":\n",
    "            info_message = \"Using the most recent action prior to the current frame.\"\n",
    "        if info_message:\n",
    "            display(Markdown(info_message))\n",
    "\n",
    "        history_actions_with_future = [clone_action(action) for action in action_dicts]\n",
    "        history_actions_with_future.append(clone_action(recorded_future_action))\n",
    "\n",
    "        display(Markdown(f\"**Training predictor using AdaptiveWorldModel on history ending at frame {target_idx+1} (step {selected_obs[-1]['step']}) for {num_steps} steps**\"))\n",
    "        display(Markdown(f\"History length: {len(selected_obs)} frames\"))\n",
    "        \n",
    "        # Show pre-training weights\n",
    "        display(Markdown(\"### Pre-Training Predictor Network Weights\"))\n",
    "        pre_stats = visualize_predictor_weights(predictor)\n",
    "        \n",
    "        display(Markdown(f\"**Using AdaptiveWorldModel.train_predictor() method with joint training**\"))\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(range(num_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress:\n",
    "            # Use AdaptiveWorldModel's train_predictor method\n",
    "            try:\n",
    "                # Set up prediction context for fresh predictions\n",
    "                predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "                \n",
    "                loss = adaptive_world_model.train_predictor(\n",
    "                    level=0,\n",
    "                    current_frame_tensor=target_tensor,\n",
    "                    predicted_features=predicted_features,\n",
    "                    history_features=feature_history,\n",
    "                    history_actions=history_actions_with_future\n",
    "                )\n",
    "                losses.append(loss)\n",
    "                \n",
    "                progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "            except Exception as e:\n",
    "                progress.close()\n",
    "                display(Markdown(f\"**Training error:** {str(e)}\"))\n",
    "                return\n",
    "        \n",
    "        progress.close()\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float('inf')\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"**Training completed in {end_time-start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Initial total loss: {format_loss(losses[0])}, Final total loss: {format_loss(final_loss)}\"))\n",
    "        \n",
    "        # Show post-training weights\n",
    "        display(Markdown(\"### Post-Training Predictor Network Weights\"))\n",
    "        post_stats = visualize_predictor_weights(predictor)\n",
    "        \n",
    "        # Compare weight changes\n",
    "        if pre_stats and post_stats:\n",
    "            weight_changes = {}\n",
    "            if 'action_embed_mean' in pre_stats and 'action_embed_mean' in post_stats:\n",
    "                weight_changes.update({\n",
    "                    'action_embed_mean_change': abs(post_stats['action_embed_mean'] - pre_stats['action_embed_mean']),\n",
    "                    'action_embed_std_change': abs(post_stats['action_embed_std'] - pre_stats['action_embed_std']),\n",
    "                })\n",
    "            if 'pos_embed_mean' in pre_stats and 'pos_embed_mean' in post_stats:\n",
    "                weight_changes.update({\n",
    "                    'pos_embed_mean_change': abs(post_stats['pos_embed_mean'] - pre_stats['pos_embed_mean']),\n",
    "                    'pos_embed_std_change': abs(post_stats['pos_embed_std'] - pre_stats['pos_embed_std']),\n",
    "                })\n",
    "            if 'first_self_attn_mean' in pre_stats and 'first_self_attn_mean' in post_stats:\n",
    "                weight_changes.update({\n",
    "                    'first_self_attn_mean_change': abs(post_stats['first_self_attn_mean'] - pre_stats['first_self_attn_mean']),\n",
    "                    'first_self_attn_std_change': abs(post_stats['first_self_attn_std'] - pre_stats['first_self_attn_std']),\n",
    "                })\n",
    "            if 'first_linear1_mean' in pre_stats and 'first_linear1_mean' in post_stats:\n",
    "                weight_changes.update({\n",
    "                    'first_linear1_mean_change': abs(post_stats['first_linear1_mean'] - pre_stats['first_linear1_mean']),\n",
    "                    'first_linear1_std_change': abs(post_stats['first_linear1_std'] - pre_stats['first_linear1_std']),\n",
    "                })\n",
    "            \n",
    "            if weight_changes:\n",
    "                changes_lines = [\"**Weight Changes:**\"]\n",
    "                if 'action_embed_mean_change' in weight_changes:\n",
    "                    changes_lines.extend([\n",
    "                        f\"- Action Embed Mean: {weight_changes['action_embed_mean_change']:.8f}\",\n",
    "                        f\"- Action Embed Std: {weight_changes['action_embed_std_change']:.8f}\",\n",
    "                    ])\n",
    "                if 'pos_embed_mean_change' in weight_changes:\n",
    "                    changes_lines.extend([\n",
    "                        f\"- Position Embed Mean: {weight_changes['pos_embed_mean_change']:.8f}\",\n",
    "                        f\"- Position Embed Std: {weight_changes['pos_embed_std_change']:.8f}\",\n",
    "                    ])\n",
    "                if 'first_self_attn_mean_change' in weight_changes:\n",
    "                    changes_lines.extend([\n",
    "                        f\"- First Self-Attn Mean: {weight_changes['first_self_attn_mean_change']:.8f}\",\n",
    "                        f\"- First Self-Attn Std: {weight_changes['first_self_attn_std_change']:.8f}\",\n",
    "                    ])\n",
    "                if 'first_linear1_mean_change' in weight_changes:\n",
    "                    changes_lines.extend([\n",
    "                        f\"- First Linear1 Mean: {weight_changes['first_linear1_mean_change']:.8f}\",\n",
    "                        f\"- First Linear1 Std: {weight_changes['first_linear1_std_change']:.8f}\",\n",
    "                    ])\n",
    "                display(Markdown(\"\\n\".join(changes_lines)))\n",
    "        \n",
    "        # Plot training progress\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Total Loss\")\n",
    "            ax.set_title(\"Predictor Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show prediction comparison\n",
    "        predictor.eval()\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "            predicted_frame = decode_features_to_image(autoencoder, predicted_features)\n",
    "        \n",
    "        predicted_img = tensor_to_numpy_image(predicted_frame)\n",
    "        target_img = tensor_to_numpy_image(target_tensor)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(predicted_img)\n",
    "        axes[0].set_title(f\"Predicted (Loss: {format_loss(final_loss)})\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(target_img)\n",
    "        axes[1].set_title(f\"Actual (step {next_obs['step']})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create predictor training widgets (same as before)\n",
    "predictor_threshold = widgets.FloatText(value=0.0005, description=\"Threshold\", step=0.0001, style={'description_width': '100px'})\n",
    "predictor_max_steps = widgets.IntText(value=1000, description=\"Max Steps\", style={'description_width': '100px'})\n",
    "predictor_steps = widgets.IntText(value=100, description=\"Steps\", style={'description_width': '100px'})\n",
    "\n",
    "train_predictor_threshold_button = widgets.Button(description=\"Train to Threshold\", button_style=\"danger\", icon=\"target\")\n",
    "train_predictor_steps_button = widgets.Button(description=\"Train N Steps\", button_style=\"danger\", icon=\"forward\")\n",
    "predictor_training_output = widgets.Output()\n",
    "\n",
    "training_widgets.update({\n",
    "    \"predictor_threshold\": predictor_threshold,\n",
    "    \"predictor_max_steps\": predictor_max_steps,\n",
    "    \"predictor_steps\": predictor_steps,\n",
    "    \"predictor_training_output\": predictor_training_output,\n",
    "})\n",
    "\n",
    "train_predictor_threshold_button.on_click(on_train_predictor_threshold)\n",
    "train_predictor_steps_button.on_click(on_train_predictor_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ac20c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## Session Explorer Interface"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Session Selection"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e32b5a6c17c445d3a85d9049ddd5eec1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Dropdown(description='Session:', options=('', 'session_20250921_140434', 'session_20250921_1405…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d032989e30d74a578931dda01ddb11b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntSlider(value=0, description='Frame (0-0)', max=0, style=SliderStyle(description_width='100px…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03fe272ffa464d3385f9b8aaa44467d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Model Loading"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52b8215ec3724180918815af522fd8d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Text(value='saved/checkpoints\\\\autoencoder.pth', description='Autoencoder:', style=TextStyle(de…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1a3a2e97e374ece9a83179cb4f129d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Button(button_style='primary', description='Load Models', icon='download', style=ButtonStyle())"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90f6d947d52c452cb987c8f24e170128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Inference"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcdd0f4a3182425083ea1778ce984608",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='success', description='Run Autoencoder', icon='play', style=ButtonStyle())…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Autoencoder Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e60f13ba7d748de899e2f8659858290",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Predictor Results"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "097d83d244c4492589c82fa30fcadc41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "### Training Sections"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Autoencoder Training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1c216a523b44a4db4ddbf8d76c9ff82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatText(value=0.0005, description='Threshold', step=0.0001, style=DescriptionStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d58a011e778842f786ac6692c4ab63db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntText(value=100, description='Steps', style=DescriptionStyle(description_width='100px')),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4eab951d8c7041dd93f3801bcc93ce62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='warning', description='Train to Threshold', icon='target', style=ButtonSty…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b865237f39342f2a34481964cc166b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "#### Predictor Training"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd23350fac9b4ad7844565ef7deff586",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatText(value=0.0005, description='Threshold', step=0.0001, style=DescriptionStyle(descriptio…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "216f278c34a044189e2649cb210dd238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntText(value=100, description='Steps', style=DescriptionStyle(description_width='100px')),))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bb6b7acb64640bc9c8a4759313ad06c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(Button(button_style='danger', description='Train to Threshold', icon='target', style=ButtonStyl…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84c69494717643298a210645bc3fcee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Main Session Explorer Interface\n",
    "session_state = {}\n",
    "session_widgets = {}\n",
    "\n",
    "def on_load_session_change(change):\n",
    "    selected_session = change[\"new\"]\n",
    "    if not selected_session:\n",
    "        return\n",
    "    session_dir = os.path.join(SESSIONS_BASE_DIR, selected_session)\n",
    "    if not os.path.exists(session_dir):\n",
    "        return\n",
    "\n",
    "    # Load session data\n",
    "    events = load_session_events(session_dir)\n",
    "    observations = extract_observations(events, session_dir)\n",
    "    actions = extract_actions(events)\n",
    "    metadata = load_session_metadata(session_dir)\n",
    "\n",
    "    # Update session state\n",
    "    session_state.clear()\n",
    "    session_state.update({\n",
    "        \"session_dir\": session_dir,\n",
    "        \"session_name\": selected_session,\n",
    "        \"events\": events,\n",
    "        \"observations\": observations,\n",
    "        \"actions\": actions,\n",
    "        \"metadata\": metadata,\n",
    "    })\n",
    "\n",
    "    # Update widgets\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    history_slider = session_widgets.get(\"history_slider\")\n",
    "\n",
    "    if frame_slider:\n",
    "        frame_slider.max = max(0, len(observations) - 1)\n",
    "        frame_slider.value = min(frame_slider.value, frame_slider.max)\n",
    "        frame_slider.description = f\"Frame (0-{frame_slider.max})\"\n",
    "\n",
    "    if history_slider:\n",
    "        history_slider.max = min(10, len(observations))\n",
    "        history_slider.value = min(history_slider.value, history_slider.max)\n",
    "\n",
    "    with session_widgets[\"output\"]:\n",
    "        session_widgets[\"output\"].clear_output()\n",
    "        display(Markdown(f\"**Loaded session:** {selected_session}\"))\n",
    "        display(Markdown(f\"**Observations:** {len(observations)}\"))\n",
    "        display(Markdown(f\"**Actions:** {len(actions)}\"))\n",
    "        if metadata:\n",
    "            display(Markdown(f\"**Metadata:** {len(metadata)} keys\"))\n",
    "\n",
    "def on_frame_change(change):\n",
    "    idx = change[\"new\"]\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    if not observations or idx >= len(observations):\n",
    "        return\n",
    "\n",
    "    observation = observations[idx]\n",
    "    frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"])\n",
    "    frame_image = tensor_to_numpy_image(frame_tensor.unsqueeze(0))\n",
    "\n",
    "    with session_widgets[\"output\"]:\n",
    "        session_widgets[\"output\"].clear_output()\n",
    "\n",
    "        display(Markdown(f\"**Frame {idx+1}/{len(observations)}** (step {observation['step']})\"))\n",
    "        display(Markdown(f\"**Timestamp:** {format_timestamp(observation.get('timestamp'))}\"))\n",
    "\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
    "        ax.imshow(frame_image)\n",
    "        ax.set_title(f\"Observation {idx+1} (step {observation['step']})\")\n",
    "        ax.axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def on_load_models(_):\n",
    "    # Load models into AdaptiveWorldModel\n",
    "    autoencoder_path = session_widgets[\"autoencoder_path\"].value\n",
    "    predictor_path = session_widgets[\"predictor_path\"].value\n",
    "\n",
    "    with session_widgets[\"model_output\"]:\n",
    "        session_widgets[\"model_output\"].clear_output()\n",
    "\n",
    "        try:\n",
    "            # Load autoencoder if specified and exists\n",
    "            if autoencoder_path and os.path.exists(autoencoder_path):\n",
    "                adaptive_world_model.autoencoder = load_autoencoder_model(autoencoder_path, device)\n",
    "                display(Markdown(f\"✅ **Loaded autoencoder** from {autoencoder_path}\"))\n",
    "            elif autoencoder_path:\n",
    "                display(Markdown(f\"❌ **Autoencoder file not found:** {autoencoder_path}\"))\n",
    "            else:\n",
    "                display(Markdown(\"⚠️ **No autoencoder path specified**\"))\n",
    "\n",
    "            # Load predictor if specified and exists\n",
    "            if predictor_path and os.path.exists(predictor_path):\n",
    "                predictor = load_predictor_model(predictor_path, device)\n",
    "                adaptive_world_model.predictors = [predictor]\n",
    "                display(Markdown(f\"✅ **Loaded predictor** from {predictor_path}\"))\n",
    "            elif predictor_path:\n",
    "                display(Markdown(f\"❌ **Predictor file not found:** {predictor_path}\"))\n",
    "            else:\n",
    "                display(Markdown(\"⚠️ **No predictor path specified**\"))\n",
    "\n",
    "        except Exception as e:\n",
    "            display(Markdown(f\"❌ **Error loading models:** {str(e)}\"))\n",
    "\n",
    "def on_run_autoencoder(_):\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "\n",
    "    with session_widgets[\"autoencoder_output\"]:\n",
    "        session_widgets[\"autoencoder_output\"].clear_output()\n",
    "\n",
    "        if autoencoder is None:\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "            return\n",
    "        if frame_slider is None:\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "            return\n",
    "\n",
    "        idx = frame_slider.value\n",
    "        observation = session_state.get(\"observations\", [])[idx]\n",
    "        frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = autoencoder.reconstruct(frame_tensor)\n",
    "            loss = torch.nn.functional.mse_loss(reconstructed, frame_tensor).item()\n",
    "\n",
    "        original_img = tensor_to_numpy_image(frame_tensor)\n",
    "        reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "\n",
    "        display(Markdown(f\"**Autoencoder Inference on Frame {idx+1} (step {observation['step']})**\"))\n",
    "        display(Markdown(f\"**Reconstruction Loss:** {loss:.6f}\"))\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"Reconstructed (Loss: {loss:.6f})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Show autoencoder weight visualization\n",
    "        display(Markdown(\"### Autoencoder Network Weight Visualization\"))\n",
    "        autoencoder_weight_stats = visualize_autoencoder_weights(autoencoder)\n",
    "\n",
    "        # Display autoencoder weight statistics\n",
    "        if autoencoder_weight_stats:\n",
    "            stats_text = f\"\"\"\n",
    "**Autoencoder Weight Statistics:**\n",
    "- Patch Embed: Mean={autoencoder_weight_stats['patch_embed_mean']:.6f}, Std={autoencoder_weight_stats['patch_embed_std']:.6f}\n",
    "- CLS Token: Mean={autoencoder_weight_stats['cls_token_mean']:.6f}, Std={autoencoder_weight_stats['cls_token_std']:.6f}\n",
    "- Position Embed: Mean={autoencoder_weight_stats['pos_embed_mean']:.6f}, Std={autoencoder_weight_stats['pos_embed_std']:.6f}\n",
    "            \"\"\"\n",
    "            display(Markdown(stats_text))\n",
    "\n",
    "def on_run_predictor(_):\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    predictor = adaptive_world_model.predictors[0] if adaptive_world_model.predictors else None\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "\n",
    "    with session_widgets[\"predictor_output\"]:\n",
    "        session_widgets[\"predictor_output\"].clear_output()\n",
    "\n",
    "        if autoencoder is None or predictor is None:\n",
    "            display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "            return\n",
    "        if frame_slider is None:\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "            return\n",
    "\n",
    "        target_idx = frame_slider.value\n",
    "        history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "        desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "\n",
    "        selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "        if error:\n",
    "            display(Markdown(f\"**Cannot run predictor:** {error}\"))\n",
    "            return\n",
    "\n",
    "        # Get features for history frames\n",
    "        feature_history = []\n",
    "        for obs in selected_obs:\n",
    "            tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "            autoencoder.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded = autoencoder.encode(tensor).detach()\n",
    "            feature_history.append(encoded)\n",
    "\n",
    "        display(Markdown(f\"**Predictor Inference on History ending at Frame {target_idx+1} (step {selected_obs[-1]['step']})**\"))\n",
    "        display(Markdown(f\"**History length:** {len(selected_obs)} frames\"))\n",
    "\n",
    "        # Show history sequence\n",
    "        history_images = []\n",
    "        for obs in selected_obs:\n",
    "            tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"])\n",
    "            history_images.append(tensor_to_numpy_image(tensor.unsqueeze(0)))\n",
    "\n",
    "        # Visualize history\n",
    "        fig, axes = plt.subplots(1, len(history_images), figsize=(3 * len(history_images), 3))\n",
    "        if len(history_images) == 1:\n",
    "            axes = [axes]\n",
    "        for i, img in enumerate(history_images):\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f\"Frame {selected_obs[i]['observation_index']+1}\")\n",
    "            axes[i].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Get recorded action between current and next frame if available\n",
    "        recorded_action, action_source = get_future_action_for_prediction(session_state, target_idx)\n",
    "        if recorded_action is None:\n",
    "            display(Markdown(\"*No recorded action between current and next frame; using empty action.*\"))\n",
    "            recorded_action = {}\n",
    "        elif action_source == \"previous\":\n",
    "            display(Markdown(\"*Using the most recent action prior to the current frame.*\"))\n",
    "\n",
    "        # Generate predictions for all possible actions\n",
    "        action_space = get_action_space(session_state)\n",
    "        all_predictions = []\n",
    "\n",
    "        if action_space:\n",
    "            for action in action_space:\n",
    "                history_actions_with_future = [clone_action(a) for a in action_dicts]\n",
    "                history_actions_with_future.append(clone_action(action))\n",
    "\n",
    "                predictor.eval()\n",
    "                with torch.no_grad():\n",
    "                    predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "                    predicted_frame = decode_features_to_image(autoencoder, predicted_features)\n",
    "\n",
    "                predicted_img = tensor_to_numpy_image(predicted_frame)\n",
    "\n",
    "                # Calculate MSE if next frame exists\n",
    "                mse_loss = None\n",
    "                if target_idx + 1 < len(session_state.get(\"observations\", [])):\n",
    "                    next_obs = session_state[\"observations\"][target_idx + 1]\n",
    "                    next_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "                    with torch.no_grad():\n",
    "                        mse_loss = torch.nn.functional.mse_loss(predicted_frame, next_tensor).item()\n",
    "\n",
    "                all_predictions.append({\n",
    "                    \"action\": action,\n",
    "                    \"image\": predicted_img,\n",
    "                    \"label\": format_action_label(action),\n",
    "                    \"mse\": mse_loss\n",
    "                })\n",
    "\n",
    "        display(Markdown(\"### Predictions for All Actions\"))\n",
    "\n",
    "        if all_predictions:\n",
    "            cols = min(4, len(all_predictions))\n",
    "            rows = math.ceil(len(all_predictions) / cols)\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 3.5 * rows))\n",
    "            axes = np.array(axes).reshape(rows, cols)\n",
    "            for idx, prediction in enumerate(all_predictions):\n",
    "                ax = axes[idx // cols][idx % cols]\n",
    "                ax.imshow(prediction[\"image\"])\n",
    "                title = prediction[\"label\"]\n",
    "                if actions_equal(prediction[\"action\"], recorded_action):\n",
    "                    title += \" (recorded)\"\n",
    "                if prediction[\"mse\"] is not None:\n",
    "                    title += f\" MSE: {prediction['mse']:.6f}\"\n",
    "                ax.set_title(title, fontsize=9)\n",
    "                ax.axis(\"off\")\n",
    "            for idx in range(len(all_predictions), rows * cols):\n",
    "                axes[idx // cols][idx % cols].axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            display(Markdown(\"No actions available to visualize predictions.\"))\n",
    "\n",
    "        # Show predictor weight visualization\n",
    "        display(Markdown(\"### Predictor Network Weight Visualization\"))\n",
    "        predictor_weight_stats = visualize_predictor_weights(predictor)\n",
    "\n",
    "        # Display predictor weight statistics\n",
    "        if predictor_weight_stats:\n",
    "            stats_lines = [\"**Predictor Weight Statistics:**\"]\n",
    "            if 'action_embed_mean' in predictor_weight_stats:\n",
    "                stats_lines.append(f\"- Action Embed: Mean={predictor_weight_stats['action_embed_mean']:.6f}, Std={predictor_weight_stats['action_embed_std']:.6f}\")\n",
    "            if 'pos_embed_mean' in predictor_weight_stats:\n",
    "                stats_lines.append(f\"- Position Embed: Mean={predictor_weight_stats['pos_embed_mean']:.6f}, Std={predictor_weight_stats['pos_embed_std']:.6f}\")\n",
    "            if 'first_self_attn_mean' in predictor_weight_stats:\n",
    "                stats_lines.append(f\"- First Self-Attn: Mean={predictor_weight_stats['first_self_attn_mean']:.6f}, Std={predictor_weight_stats['first_self_attn_std']:.6f}\")\n",
    "            if 'first_linear1_mean' in predictor_weight_stats:\n",
    "                stats_lines.append(f\"- First Linear1: Mean={predictor_weight_stats['first_linear1_mean']:.6f}, Std={predictor_weight_stats['first_linear1_std']:.6f}\")\n",
    "            if 'output_head_mean' in predictor_weight_stats:\n",
    "                stats_lines.append(f\"- Output Head: Mean={predictor_weight_stats['output_head_mean']:.6f}, Std={predictor_weight_stats['output_head_std']:.6f}\")\n",
    "            display(Markdown(\"\\n\".join(stats_lines)))\n",
    "\n",
    "# Create session selection widgets\n",
    "session_dirs = list_session_dirs(SESSIONS_BASE_DIR)\n",
    "load_session_dropdown = widgets.Dropdown(\n",
    "    options=[\"\"] + session_dirs,\n",
    "    value=\"\",\n",
    "    description=\"Session:\",\n",
    "    style={'description_width': '100px'}\n",
    ")\n",
    "\n",
    "frame_slider = widgets.IntSlider(value=0, min=0, max=0, description=\"Frame (0-0)\", style={'description_width': '100px'})\n",
    "history_slider = widgets.IntSlider(value=3, min=2, max=10, description=\"History\", style={'description_width': '100px'})\n",
    "\n",
    "# Create model loading widgets\n",
    "autoencoder_path = widgets.Text(value=DEFAULT_AUTOENCODER_PATH, description=\"Autoencoder:\", style={'description_width': '100px'})\n",
    "predictor_path = widgets.Text(value=DEFAULT_PREDICTOR_PATH, description=\"Predictor:\", style={'description_width': '100px'})\n",
    "load_models_button = widgets.Button(description=\"Load Models\", button_style=\"primary\", icon=\"download\")\n",
    "\n",
    "# Create inference buttons\n",
    "run_autoencoder_button = widgets.Button(description=\"Run Autoencoder\", button_style=\"success\", icon=\"play\")\n",
    "run_predictor_button = widgets.Button(description=\"Run Predictor\", button_style=\"info\", icon=\"play\")\n",
    "\n",
    "# Create output widgets\n",
    "session_output = widgets.Output()\n",
    "model_output = widgets.Output()\n",
    "autoencoder_output = widgets.Output()\n",
    "predictor_output = widgets.Output()\n",
    "\n",
    "# Store widgets in global dict\n",
    "session_widgets.update({\n",
    "    \"load_session_dropdown\": load_session_dropdown,\n",
    "    \"frame_slider\": frame_slider,\n",
    "    \"history_slider\": history_slider,\n",
    "    \"autoencoder_path\": autoencoder_path,\n",
    "    \"predictor_path\": predictor_path,\n",
    "    \"output\": session_output,\n",
    "    \"model_output\": model_output,\n",
    "    \"autoencoder_output\": autoencoder_output,\n",
    "    \"predictor_output\": predictor_output,\n",
    "})\n",
    "\n",
    "# Connect event handlers\n",
    "load_session_dropdown.observe(on_load_session_change, names=\"value\")\n",
    "frame_slider.observe(on_frame_change, names=\"value\")\n",
    "load_models_button.on_click(on_load_models)\n",
    "run_autoencoder_button.on_click(on_run_autoencoder)\n",
    "run_predictor_button.on_click(on_run_predictor)\n",
    "\n",
    "# Display interface\n",
    "display(Markdown(\"## Session Explorer Interface\"))\n",
    "\n",
    "display(Markdown(\"### Session Selection\"))\n",
    "display(widgets.HBox([load_session_dropdown]))\n",
    "display(widgets.HBox([frame_slider, history_slider]))\n",
    "display(session_output)\n",
    "\n",
    "display(Markdown(\"### Model Loading\"))\n",
    "display(widgets.VBox([autoencoder_path, predictor_path]))\n",
    "display(load_models_button)\n",
    "display(model_output)\n",
    "\n",
    "display(Markdown(\"### Inference\"))\n",
    "display(widgets.HBox([run_autoencoder_button, run_predictor_button]))\n",
    "\n",
    "display(Markdown(\"### Autoencoder Results\"))\n",
    "display(autoencoder_output)\n",
    "\n",
    "display(Markdown(\"### Predictor Results\"))\n",
    "display(predictor_output)\n",
    "\n",
    "display(Markdown(\"### Training Sections\"))\n",
    "display(Markdown(\"#### Autoencoder Training\"))\n",
    "display(widgets.HBox([training_widgets[\"autoencoder_threshold\"], training_widgets[\"autoencoder_max_steps\"]]))\n",
    "display(widgets.HBox([training_widgets[\"autoencoder_steps\"]]))\n",
    "display(widgets.HBox([train_autoencoder_threshold_button, train_autoencoder_steps_button]))\n",
    "display(training_widgets[\"autoencoder_training_output\"])\n",
    "\n",
    "display(Markdown(\"#### Predictor Training\"))\n",
    "display(widgets.HBox([training_widgets[\"predictor_threshold\"], training_widgets[\"predictor_max_steps\"]]))\n",
    "display(widgets.HBox([training_widgets[\"predictor_steps\"]]))\n",
    "display(widgets.HBox([train_predictor_threshold_button, train_predictor_steps_button]))\n",
    "display(training_widgets[\"predictor_training_output\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f381ecb-0e7a-4e2c-a124-779a5a2ddd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
