{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6bc8f2",
   "metadata": {},
   "source": [
    "# Session Explorer\n",
    "This notebook lets you explore recorded robot sessions, scrub through frames, run the autoencoder and predictor models on stored observations, and train the models on selected frames/history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957658",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "1. Pick a session and load it.\n",
    "2. Use the playback controls to scrub through frames.\n",
    "3. (Optional) Load model checkpoints, then run the Autoencoder and Predictor sections using the current frame selection.\n",
    "4. (Optional) Use the Training sections to train models on current frame/history until a loss threshold is met or for a specified number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8527353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from functools import lru_cache\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import config\n",
    "from models import MaskedAutoencoderViT, TransformerActionConditionedPredictor\n",
    "from adaptive_world_model import AdaptiveWorldModel\n",
    "from robot_interface import RobotInterface\n",
    "\n",
    "# Additional imports for training\n",
    "import time\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6399c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Loading from primary checkpoint files...\n",
      "Autoencoder checkpoint loaded\n",
      "Learning progress loaded: 430632 autoencoder steps, 922800 predictor steps, 121844 actions\n"
     ]
    }
   ],
   "source": [
    "SESSIONS_BASE_DIR = config.RECORDING_BASE_DIR\n",
    "DEFAULT_AUTOENCODER_PATH = os.path.join(config.DEFAULT_CHECKPOINT_DIR, \"autoencoder.pth\")\n",
    "DEFAULT_PREDICTOR_PATH = os.path.join(config.DEFAULT_CHECKPOINT_DIR, \"predictor_0.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a stub robot interface for the AdaptiveWorldModel\n",
    "class StubRobotInterface(RobotInterface):\n",
    "    \"\"\"Stub robot interface for notebook training purposes\"\"\"\n",
    "    def get_observation(self):\n",
    "        # Return a dummy observation - not used in training\n",
    "        return {\"frame\": np.zeros((224, 224, 3), dtype=np.uint8)}\n",
    "    \n",
    "    def execute_action(self, action):\n",
    "        # Do nothing - not used in training\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        # Return minimal action space - not used in training\n",
    "        return [{\"motor_left\": 0, \"motor_right\": 0, \"duration\": 0.2}]\n",
    "    \n",
    "    def cleanup(self):\n",
    "        pass\n",
    "\n",
    "# Instantiate AdaptiveWorldModel for training access\n",
    "stub_robot = StubRobotInterface()\n",
    "adaptive_world_model = AdaptiveWorldModel(stub_robot, wandb_project=None, checkpoint_dir=config.DEFAULT_CHECKPOINT_DIR, interactive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2f58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility helpers for loading sessions, caching frames, and preparing model inputs\n",
    "def list_session_dirs(base_dir):\n",
    "    # Return sorted session directory names.\n",
    "    if not os.path.exists(base_dir):\n",
    "        return []\n",
    "    entries = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        path = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(path) and name.startswith(\"session_\"):\n",
    "            entries.append(name)\n",
    "    entries.sort()\n",
    "    return entries\n",
    "\n",
    "def load_session_metadata(session_dir):\n",
    "    meta_path = os.path.join(session_dir, \"session_meta.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def load_session_events(session_dir):\n",
    "    # Load all events from shard files and sort them by step.\n",
    "    pattern = os.path.join(session_dir, \"events_shard_*.jsonl\")\n",
    "    shard_files = sorted(glob.glob(pattern))\n",
    "    events = []\n",
    "    for shard_path in shard_files:\n",
    "        with open(shard_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                events.append(json.loads(line))\n",
    "    events.sort(key=lambda evt: evt.get(\"step\", 0))\n",
    "    return events\n",
    "\n",
    "def extract_observations(events, session_dir):\n",
    "    observations = []\n",
    "    for idx, event in enumerate(events):\n",
    "        if event.get(\"type\") != \"observation\":\n",
    "            continue\n",
    "        data = event.get(\"data\", {})\n",
    "        frame_path = data.get(\"frame_path\")\n",
    "        if not frame_path:\n",
    "            continue\n",
    "        observations.append({\n",
    "            \"observation_index\": len(observations),\n",
    "            \"event_index\": idx,\n",
    "            \"step\": event.get(\"step\", idx),\n",
    "            \"timestamp\": event.get(\"timestamp\"),\n",
    "            \"frame_path\": frame_path,\n",
    "            \"full_path\": os.path.join(session_dir, frame_path),\n",
    "        })\n",
    "    return observations\n",
    "\n",
    "def extract_actions(events):\n",
    "    actions = []\n",
    "    for idx, event in enumerate(events):\n",
    "        if event.get(\"type\") != \"action\":\n",
    "            continue\n",
    "        actions.append({\n",
    "            \"action_index\": len(actions),\n",
    "            \"event_index\": idx,\n",
    "            \"step\": event.get(\"step\", idx),\n",
    "            \"timestamp\": event.get(\"timestamp\"),\n",
    "            \"action\": event.get(\"data\", {}),\n",
    "        })\n",
    "    return actions\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def load_frame_bytes(full_path):\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Frame file not found: {full_path}\")\n",
    "    with open(full_path, \"rb\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_frame_image(full_path):\n",
    "    return Image.open(io.BytesIO(load_frame_bytes(full_path))).convert(\"RGB\")\n",
    "\n",
    "tensor_cache = {}\n",
    "\n",
    "def get_frame_tensor(session_dir, frame_path):\n",
    "    # Return normalized (C,H,W) tensor for a frame, cached on CPU.\n",
    "    key = (session_dir, frame_path)\n",
    "    if key not in tensor_cache:\n",
    "        full_path = os.path.join(session_dir, frame_path)\n",
    "        pil_img = load_frame_image(full_path)\n",
    "        tensor_cache[key] = config.TRANSFORM(pil_img)\n",
    "    return tensor_cache[key]\n",
    "\n",
    "def tensor_to_numpy_image(tensor):\n",
    "    if tensor.ndim == 4:\n",
    "        tensor = tensor[0]\n",
    "    tensor = tensor.detach().cpu().float()\n",
    "    tensor = tensor * 0.5 + 0.5\n",
    "    tensor = torch.clamp(tensor, 0.0, 1.0)\n",
    "    return tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "def format_timestamp(ts):\n",
    "    if ts is None:\n",
    "        return \"N/A\"\n",
    "    try:\n",
    "        return datetime.datetime.fromtimestamp(ts).isoformat()\n",
    "    except Exception:\n",
    "        return str(ts)\n",
    "\n",
    "def describe_action(action):\n",
    "    if not action:\n",
    "        return \"{}\"\n",
    "    parts = []\n",
    "    for key in sorted(action.keys()):\n",
    "        parts.append(f\"{key}: {action[key]}\")\n",
    "    return \", \".join(parts)\n",
    "\n",
    "def canonical_action_key(action):\n",
    "    if not action:\n",
    "        return ()\n",
    "    return tuple(sorted(action.items()))\n",
    "\n",
    "def get_action_space(session_state):\n",
    "    metadata_actions = session_state.get(\"metadata\", {}).get(\"action_space\") or []\n",
    "    if metadata_actions:\n",
    "        return metadata_actions\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for action_entry in session_state.get(\"actions\", []):\n",
    "        action = action_entry.get(\"action\", {})\n",
    "        key = canonical_action_key(action)\n",
    "        if key and key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(action)\n",
    "    return unique\n",
    "\n",
    "def format_action_label(action):\n",
    "    if not action:\n",
    "        return \"{}\"\n",
    "    parts = []\n",
    "    for key in sorted(action.keys()):\n",
    "        parts.append(f\"{key}={action[key]}\")\n",
    "    return \", \".join(parts)\n",
    "\n",
    "def clone_action(action):\n",
    "    if not action:\n",
    "        return {}\n",
    "    return {key: float(value) if isinstance(value, (int, float)) else value for key, value in action.items()}\n",
    "\n",
    "def actions_equal(action_a, action_b):\n",
    "    return canonical_action_key(action_a) == canonical_action_key(action_b)\n",
    "\n",
    "def load_autoencoder_model(path, device):\n",
    "    model = MaskedAutoencoderViT()\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_predictor_model(path, device):\n",
    "    model = TransformerActionConditionedPredictor()\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    level = checkpoint.get(\"level\")\n",
    "    if level is not None:\n",
    "        model.level = level\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def decode_features_to_image(autoencoder, predicted_features):\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        num_patches = autoencoder.patch_embed.num_patches\n",
    "        ids_restore = torch.arange(num_patches, device=predicted_features.device).unsqueeze(0).repeat(predicted_features.shape[0], 1)\n",
    "        pred_patches = autoencoder.forward_decoder(predicted_features, ids_restore)\n",
    "        decoded = autoencoder.unpatchify(pred_patches)\n",
    "    return decoded\n",
    "\n",
    "def build_predictor_sequence(session_state, target_obs_index, desired_length):\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    events = session_state.get(\"events\", [])\n",
    "    if not observations:\n",
    "        return [], [], \"No observations loaded.\"\n",
    "    if target_obs_index < 0 or target_obs_index >= len(observations):\n",
    "        return [], [], \"Selected observation is out of range.\"\n",
    "    desired_length = max(2, desired_length)\n",
    "    selected_obs = [observations[target_obs_index]]\n",
    "    action_dicts = []\n",
    "    current_idx = target_obs_index\n",
    "    current_event_index = observations[target_obs_index][\"event_index\"]\n",
    "    while len(selected_obs) < desired_length and current_idx > 0:\n",
    "        prev_idx = current_idx - 1\n",
    "        found = False\n",
    "        while prev_idx >= 0:\n",
    "            prev_obs = observations[prev_idx]\n",
    "            prev_event_index = prev_obs[\"event_index\"]\n",
    "            actions_between = [events[i] for i in range(prev_event_index + 1, current_event_index) if events[i].get(\"type\") == \"action\"]\n",
    "            if len(actions_between) == 1:\n",
    "                action_dicts.insert(0, actions_between[0].get(\"data\", {}))\n",
    "                selected_obs.insert(0, prev_obs)\n",
    "                current_idx = prev_idx\n",
    "                current_event_index = prev_event_index\n",
    "                found = True\n",
    "                break\n",
    "            prev_idx -= 1\n",
    "        if not found:\n",
    "            break\n",
    "    if len(selected_obs) < 2:\n",
    "        return [], [], \"Could not assemble a history with actions between frames. Choose a later frame.\"\n",
    "    return selected_obs, action_dicts, None\n",
    "\n",
    "def find_action_between_events(events, start_event_index, end_event_index):\n",
    "    \"\"\"Return the recorded action between two observation events, falling back to the prior action.\"\"\"\n",
    "    between_actions = [\n",
    "        event for event in events[start_event_index + 1:end_event_index]\n",
    "        if event.get(\"type\") == \"action\"\n",
    "    ]\n",
    "    if between_actions:\n",
    "        return clone_action(between_actions[-1].get(\"data\", {})), \"between\"\n",
    "\n",
    "    for idx in range(start_event_index, -1, -1):\n",
    "        event = events[idx]\n",
    "        if event.get(\"type\") == \"action\":\n",
    "            return clone_action(event.get(\"data\", {})), \"previous\"\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_future_action_for_prediction(session_state, target_obs_index):\n",
    "    \"\"\"Return the action to pair with the next observation for prediction.\"\"\"\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    events = session_state.get(\"events\", [])\n",
    "    if target_obs_index < 0 or target_obs_index >= len(observations) - 1:\n",
    "        return None, None\n",
    "    current_obs = observations[target_obs_index]\n",
    "    next_obs = observations[target_obs_index + 1]\n",
    "    return find_action_between_events(events, current_obs[\"event_index\"], next_obs[\"event_index\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ixrc3gkvood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper functions using AdaptiveWorldModel\n",
    "def train_autoencoder_step_wrapper(frame_tensor):\n",
    "    \"\"\"Single autoencoder training step using AdaptiveWorldModel\"\"\"\n",
    "    # Convert tensor to numpy frame for AdaptiveWorldModel\n",
    "    frame_numpy = tensor_to_numpy_image(frame_tensor)\n",
    "    \n",
    "    # Use AdaptiveWorldModel's train_autoencoder method\n",
    "    loss = adaptive_world_model.train_autoencoder(frame_numpy)\n",
    "    return loss\n",
    "\n",
    "def train_predictor_step_wrapper(target_idx, history_features, history_actions):\n",
    "    \"\"\"Single predictor training step using AdaptiveWorldModel\"\"\"\n",
    "    # Get target frame\n",
    "    next_obs = session_state[\"observations\"][target_idx + 1]\n",
    "    target_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "    target_frame = tensor_to_numpy_image(target_tensor)\n",
    "    \n",
    "    # Set up prediction context in AdaptiveWorldModel\n",
    "    adaptive_world_model.observation_history = []\n",
    "    adaptive_world_model.action_history = []\n",
    "    \n",
    "    # Add history to the world model\n",
    "    for i, (feat, action) in enumerate(zip(history_features, history_actions)):\n",
    "        # Convert feature back to frame if needed\n",
    "        obs = session_state[\"observations\"][target_idx - len(history_features) + 1 + i]\n",
    "        frame_tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0)\n",
    "        frame_numpy = tensor_to_numpy_image(frame_tensor)\n",
    "        \n",
    "        adaptive_world_model.observation_history.append(frame_numpy)\n",
    "        adaptive_world_model.action_history.append(action)\n",
    "    \n",
    "    # Train predictor level 0\n",
    "    loss = adaptive_world_model.train_predictor(0, target_tensor)\n",
    "    return loss\n",
    "\n",
    "def format_loss(loss_value):\n",
    "    \"\"\"Format loss value for display\"\"\"\n",
    "    if loss_value < 0.001:\n",
    "        return f\"{loss_value:.2e}\"\n",
    "    else:\n",
    "        return f\"{loss_value:.6f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ob17o8vzup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Training Section using AdaptiveWorldModel\n",
    "training_widgets = {}\n",
    "\n",
    "def on_train_autoencoder_threshold(_):\n",
    "    \"\"\"Train autoencoder until threshold is met using AdaptiveWorldModel\"\"\"\n",
    "    autoencoder = session_state.get(\"autoencoder\")\n",
    "    if autoencoder is None:\n",
    "        with training_widgets[\"autoencoder_training_output\"]:\n",
    "            training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "        return\n",
    "    \n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with training_widgets[\"autoencoder_training_output\"]:\n",
    "            training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        return\n",
    "    \n",
    "    # Get training parameters\n",
    "    threshold = training_widgets[\"autoencoder_threshold\"].value\n",
    "    max_steps = training_widgets[\"autoencoder_max_steps\"].value\n",
    "    \n",
    "    # Setup for training\n",
    "    idx = frame_slider.value\n",
    "    observation = session_state.get(\"observations\", [])[idx]\n",
    "    frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Load models into AdaptiveWorldModel\n",
    "    adaptive_world_model.autoencoder = autoencoder\n",
    "    adaptive_world_model.device = device\n",
    "    \n",
    "    with training_widgets[\"autoencoder_training_output\"]:\n",
    "        training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "        display(Markdown(f\"**Training autoencoder using AdaptiveWorldModel on frame {idx+1} (step {observation['step']})**\"))\n",
    "        display(Markdown(f\"Target threshold: {format_loss(threshold)}, Max steps: {max_steps}\"))\n",
    "        display(Markdown(\"**Using AdaptiveWorldModel.train_autoencoder() method with randomized masking**\"))\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(range(max_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress:\n",
    "            loss = train_autoencoder_step_wrapper(frame_tensor)\n",
    "            losses.append(loss)\n",
    "            \n",
    "            progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "            \n",
    "            # Check if threshold met\n",
    "            if loss <= threshold:\n",
    "                progress.close()\n",
    "                break\n",
    "        else:\n",
    "            progress.close()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float('inf')\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"**Training completed after {len(losses)} steps in {end_time-start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Final loss: {format_loss(final_loss)}\"))\n",
    "        \n",
    "        if final_loss <= threshold:\n",
    "            display(Markdown(f\"✅ **Target threshold {format_loss(threshold)} achieved!**\"))\n",
    "        else:\n",
    "            display(Markdown(f\"⚠️ **Target threshold {format_loss(threshold)} not reached after {max_steps} steps**\"))\n",
    "        \n",
    "        # Plot training progress\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Reconstruction Loss\")\n",
    "            ax.set_title(\"Autoencoder Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.axhline(y=threshold, color='r', linestyle='--', alpha=0.7, label=f'Target: {format_loss(threshold)}')\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show final reconstruction\n",
    "        adaptive_world_model.autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = adaptive_world_model.autoencoder.reconstruct(frame_tensor)\n",
    "        \n",
    "        original_img = tensor_to_numpy_image(frame_tensor)\n",
    "        reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"After Training (Loss: {format_loss(final_loss)})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def on_train_autoencoder_steps(_):\n",
    "    \"\"\"Train autoencoder for specified number of steps using AdaptiveWorldModel\"\"\"\n",
    "    autoencoder = session_state.get(\"autoencoder\")\n",
    "    if autoencoder is None:\n",
    "        with training_widgets[\"autoencoder_training_output\"]:\n",
    "            training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "        return\n",
    "    \n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with training_widgets[\"autoencoder_training_output\"]:\n",
    "            training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        return\n",
    "    \n",
    "    # Get training parameters\n",
    "    num_steps = training_widgets[\"autoencoder_steps\"].value\n",
    "    \n",
    "    # Setup for training\n",
    "    idx = frame_slider.value\n",
    "    observation = session_state.get(\"observations\", [])[idx]\n",
    "    frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "    \n",
    "    # Load models into AdaptiveWorldModel\n",
    "    adaptive_world_model.autoencoder = autoencoder\n",
    "    adaptive_world_model.device = device\n",
    "    \n",
    "    with training_widgets[\"autoencoder_training_output\"]:\n",
    "        training_widgets[\"autoencoder_training_output\"].clear_output()\n",
    "        display(Markdown(f\"**Training autoencoder using AdaptiveWorldModel on frame {idx+1} (step {observation['step']}) for {num_steps} steps**\"))\n",
    "        display(Markdown(\"**Using AdaptiveWorldModel.train_autoencoder() method with randomized masking**\"))\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(range(num_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress:\n",
    "            loss = train_autoencoder_step_wrapper(frame_tensor)\n",
    "            losses.append(loss)\n",
    "            progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "        \n",
    "        progress.close()\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float('inf')\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"**Training completed in {end_time-start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Initial loss: {format_loss(losses[0])}, Final loss: {format_loss(final_loss)}\"))\n",
    "        \n",
    "        # Plot training progress\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Reconstruction Loss\")\n",
    "            ax.set_title(\"Autoencoder Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show final reconstruction\n",
    "        adaptive_world_model.autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = adaptive_world_model.autoencoder.reconstruct(frame_tensor)\n",
    "        \n",
    "        original_img = tensor_to_numpy_image(frame_tensor)\n",
    "        reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"After Training (Loss: {format_loss(final_loss)})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create autoencoder training widgets\n",
    "autoencoder_threshold = widgets.FloatText(value=0.0005, description=\"Threshold\", step=0.0001, style={'description_width': '100px'})\n",
    "autoencoder_max_steps = widgets.IntText(value=1000, description=\"Max Steps\", style={'description_width': '100px'})\n",
    "autoencoder_steps = widgets.IntText(value=100, description=\"Steps\", style={'description_width': '100px'})\n",
    "\n",
    "train_autoencoder_threshold_button = widgets.Button(description=\"Train to Threshold\", button_style=\"warning\", icon=\"target\")\n",
    "train_autoencoder_steps_button = widgets.Button(description=\"Train N Steps\", button_style=\"warning\", icon=\"forward\")\n",
    "autoencoder_training_output = widgets.Output()\n",
    "\n",
    "training_widgets.update({\n",
    "    \"autoencoder_threshold\": autoencoder_threshold,\n",
    "    \"autoencoder_max_steps\": autoencoder_max_steps,\n",
    "    \"autoencoder_steps\": autoencoder_steps,\n",
    "    \"autoencoder_training_output\": autoencoder_training_output,\n",
    "})\n",
    "\n",
    "train_autoencoder_threshold_button.on_click(on_train_autoencoder_threshold)\n",
    "train_autoencoder_steps_button.on_click(on_train_autoencoder_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "flq0j1fn1ql",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictor Training Section using AdaptiveWorldModel\n",
    "def on_train_predictor_threshold(_):\n",
    "    \"\"\"Train predictor until threshold is met using AdaptiveWorldModel\"\"\"\n",
    "    autoencoder = session_state.get(\"autoencoder\")\n",
    "    predictor = session_state.get(\"predictor\")\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    \n",
    "    with training_widgets[\"predictor_training_output\"]:\n",
    "        training_widgets[\"predictor_training_output\"].clear_output()\n",
    "        \n",
    "        if autoencoder is None or predictor is None:\n",
    "            display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "            return\n",
    "        if frame_slider is None:\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "            return\n",
    "        \n",
    "        # Get training parameters\n",
    "        threshold = training_widgets[\"predictor_threshold\"].value\n",
    "        max_steps = training_widgets[\"predictor_max_steps\"].value\n",
    "        \n",
    "        target_idx = frame_slider.value\n",
    "        history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "        desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "        \n",
    "        selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "        if error:\n",
    "            display(Markdown(f\"**Cannot train predictor:** {error}\"))\n",
    "            return\n",
    "        \n",
    "        # Check if we have a next frame for training target\n",
    "        if target_idx + 1 >= len(session_state[\"observations\"]):\n",
    "            display(Markdown(\"**Cannot train predictor:** No next frame available as training target.\"))\n",
    "            return\n",
    "        \n",
    "        # Get target frame tensor\n",
    "        next_obs = session_state[\"observations\"][target_idx + 1]\n",
    "        target_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Load models into AdaptiveWorldModel and setup predictors\n",
    "        adaptive_world_model.autoencoder = autoencoder\n",
    "        adaptive_world_model.predictors = [predictor]\n",
    "        adaptive_world_model.device = device\n",
    "        \n",
    "        # Get feature history and setup context\n",
    "        feature_history = []\n",
    "        for obs in selected_obs:\n",
    "            cached = session_state[\"feature_cache\"].get(obs[\"frame_path\"])\n",
    "            if cached is None:\n",
    "                tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "                autoencoder.eval()\n",
    "                with torch.no_grad():\n",
    "                    encoded = autoencoder.encode(tensor)\n",
    "                session_state[\"feature_cache\"][obs[\"frame_path\"]] = encoded.detach().cpu()\n",
    "                cached = session_state[\"feature_cache\"][obs[\"frame_path\"]]\n",
    "            feature_history.append(cached.to(device))\n",
    "\n",
    "        recorded_future_action, action_source = get_future_action_for_prediction(session_state, target_idx)\n",
    "        info_message = None\n",
    "        if recorded_future_action is None:\n",
    "            info_message = \"No recorded action between current and next frame; using empty action.\"\n",
    "            recorded_future_action = {}\n",
    "        elif action_source == \"previous\":\n",
    "            info_message = \"Using the most recent action prior to the current frame.\"\n",
    "        if info_message:\n",
    "            display(Markdown(info_message))\n",
    "\n",
    "        history_actions_with_future = [clone_action(action) for action in action_dicts]\n",
    "        history_actions_with_future.append(clone_action(recorded_future_action))\n",
    "\n",
    "        display(Markdown(f\"**Training predictor using AdaptiveWorldModel on history ending at frame {target_idx+1} (step {selected_obs[-1]['step']})**\"))\n",
    "        display(Markdown(f\"Target threshold: {format_loss(threshold)}, Max steps: {max_steps}\"))\n",
    "        display(Markdown(f\"History length: {len(selected_obs)} frames\"))\n",
    "        display(Markdown(f\"**Using AdaptiveWorldModel.train_predictor() method with joint training**\"))\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(range(max_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress:\n",
    "            # Use AdaptiveWorldModel's train_predictor method\n",
    "            # This method expects current_frame_tensor, predicted_features, history_features, history_actions\n",
    "            try:\n",
    "                # Set up prediction context for fresh predictions\n",
    "                predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "                \n",
    "                loss = adaptive_world_model.train_predictor(\n",
    "                    level=0,\n",
    "                    current_frame_tensor=target_tensor,\n",
    "                    predicted_features=predicted_features,\n",
    "                    history_features=feature_history,\n",
    "                    history_actions=history_actions_with_future\n",
    "                )\n",
    "                losses.append(loss)\n",
    "                \n",
    "                progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "                \n",
    "                # Check if threshold met\n",
    "                if loss <= threshold:\n",
    "                    progress.close()\n",
    "                    break\n",
    "            except Exception as e:\n",
    "                progress.close()\n",
    "                display(Markdown(f\"**Training error:** {str(e)}\"))\n",
    "                return\n",
    "        else:\n",
    "            progress.close()\n",
    "        \n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float('inf')\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"**Training completed after {len(losses)} steps in {end_time-start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Final total loss: {format_loss(final_loss)}\"))\n",
    "        \n",
    "        if final_loss <= threshold:\n",
    "            display(Markdown(f\"✅ **Target threshold {format_loss(threshold)} achieved!**\"))\n",
    "        else:\n",
    "            display(Markdown(f\"⚠️ **Target threshold {format_loss(threshold)} not reached after {max_steps} steps**\"))\n",
    "        \n",
    "        # Plot training progress\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses, label=\"Total Loss\")\n",
    "            ax.axhline(y=threshold, color='r', linestyle='--', alpha=0.7, label=f'Target: {format_loss(threshold)}')\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.set_title(\"Predictor Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show prediction comparison\n",
    "        predictor.eval()\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "            predicted_frame = decode_features_to_image(autoencoder, predicted_features)\n",
    "        \n",
    "        predicted_img = tensor_to_numpy_image(predicted_frame)\n",
    "        target_img = tensor_to_numpy_image(target_tensor)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(predicted_img)\n",
    "        axes[0].set_title(f\"Predicted (Loss: {format_loss(final_loss)})\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(target_img)\n",
    "        axes[1].set_title(f\"Actual (step {next_obs['step']})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def on_train_predictor_steps(_):\n",
    "    \"\"\"Train predictor for specified number of steps using AdaptiveWorldModel\"\"\"\n",
    "    autoencoder = session_state.get(\"autoencoder\")\n",
    "    predictor = session_state.get(\"predictor\")\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    \n",
    "    with training_widgets[\"predictor_training_output\"]:\n",
    "        training_widgets[\"predictor_training_output\"].clear_output()\n",
    "        \n",
    "        if autoencoder is None or predictor is None:\n",
    "            display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "            return\n",
    "        if frame_slider is None:\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "            return\n",
    "        \n",
    "        # Get training parameters\n",
    "        num_steps = training_widgets[\"predictor_steps\"].value\n",
    "        \n",
    "        target_idx = frame_slider.value\n",
    "        history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "        desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "        \n",
    "        selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "        if error:\n",
    "            display(Markdown(f\"**Cannot train predictor:** {error}\"))\n",
    "            return\n",
    "        \n",
    "        # Check if we have a next frame for training target\n",
    "        if target_idx + 1 >= len(session_state[\"observations\"]):\n",
    "            display(Markdown(\"**Cannot train predictor:** No next frame available as training target.\"))\n",
    "            return\n",
    "        \n",
    "        # Get target frame tensor\n",
    "        next_obs = session_state[\"observations\"][target_idx + 1]\n",
    "        target_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "        \n",
    "        # Load models into AdaptiveWorldModel and setup predictors\n",
    "        adaptive_world_model.autoencoder = autoencoder\n",
    "        adaptive_world_model.predictors = [predictor]\n",
    "        adaptive_world_model.device = device\n",
    "        \n",
    "        # Get feature history and setup context\n",
    "        feature_history = []\n",
    "        for obs in selected_obs:\n",
    "            cached = session_state[\"feature_cache\"].get(obs[\"frame_path\"])\n",
    "            if cached is None:\n",
    "                tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "                autoencoder.eval()\n",
    "                with torch.no_grad():\n",
    "                    encoded = autoencoder.encode(tensor)\n",
    "                session_state[\"feature_cache\"][obs[\"frame_path\"]] = encoded.detach().cpu()\n",
    "                cached = session_state[\"feature_cache\"][obs[\"frame_path\"]]\n",
    "            feature_history.append(cached.to(device))\n",
    "\n",
    "        recorded_future_action, action_source = get_future_action_for_prediction(session_state, target_idx)\n",
    "        info_message = None\n",
    "        if recorded_future_action is None:\n",
    "            info_message = \"No recorded action between current and next frame; using empty action.\"\n",
    "            recorded_future_action = {}\n",
    "        elif action_source == \"previous\":\n",
    "            info_message = \"Using the most recent action prior to the current frame.\"\n",
    "        if info_message:\n",
    "            display(Markdown(info_message))\n",
    "\n",
    "        history_actions_with_future = [clone_action(action) for action in action_dicts]\n",
    "        history_actions_with_future.append(clone_action(recorded_future_action))\n",
    "\n",
    "        display(Markdown(f\"**Training predictor using AdaptiveWorldModel on history ending at frame {target_idx+1} (step {selected_obs[-1]['step']}) for {num_steps} steps**\"))\n",
    "        display(Markdown(f\"History length: {len(selected_obs)} frames\"))\n",
    "        display(Markdown(f\"**Using AdaptiveWorldModel.train_predictor() method with joint training**\"))\n",
    "        \n",
    "        losses = []\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Create progress bar\n",
    "        progress = tqdm(range(num_steps), desc=\"Training\")\n",
    "        \n",
    "        for step in progress:\n",
    "            # Use AdaptiveWorldModel's train_predictor method\n",
    "            try:\n",
    "                # Set up prediction context for fresh predictions\n",
    "                predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "                \n",
    "                loss = adaptive_world_model.train_predictor(\n",
    "                    level=0,\n",
    "                    current_frame_tensor=target_tensor,\n",
    "                    predicted_features=predicted_features,\n",
    "                    history_features=feature_history,\n",
    "                    history_actions=history_actions_with_future\n",
    "                )\n",
    "                losses.append(loss)\n",
    "                \n",
    "                progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "            except Exception as e:\n",
    "                progress.close()\n",
    "                display(Markdown(f\"**Training error:** {str(e)}\"))\n",
    "                return\n",
    "        \n",
    "        progress.close()\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float('inf')\n",
    "        \n",
    "        # Display results\n",
    "        display(Markdown(f\"**Training completed in {end_time-start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Initial total loss: {format_loss(losses[0])}, Final total loss: {format_loss(final_loss)}\"))\n",
    "        \n",
    "        # Plot training progress\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Total Loss\")\n",
    "            ax.set_title(\"Predictor Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        \n",
    "        # Show prediction comparison\n",
    "        predictor.eval()\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            predicted_features = predictor(feature_history, history_actions_with_future)\n",
    "            predicted_frame = decode_features_to_image(autoencoder, predicted_features)\n",
    "        \n",
    "        predicted_img = tensor_to_numpy_image(predicted_frame)\n",
    "        target_img = tensor_to_numpy_image(target_tensor)\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(predicted_img)\n",
    "        axes[0].set_title(f\"Predicted (Loss: {format_loss(final_loss)})\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(target_img)\n",
    "        axes[1].set_title(f\"Actual (step {next_obs['step']})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Create predictor training widgets (same as before)\n",
    "predictor_threshold = widgets.FloatText(value=0.0005, description=\"Threshold\", step=0.0001, style={'description_width': '100px'})\n",
    "predictor_max_steps = widgets.IntText(value=1000, description=\"Max Steps\", style={'description_width': '100px'})\n",
    "predictor_steps = widgets.IntText(value=100, description=\"Steps\", style={'description_width': '100px'})\n",
    "\n",
    "train_predictor_threshold_button = widgets.Button(description=\"Train to Threshold\", button_style=\"danger\", icon=\"target\")\n",
    "train_predictor_steps_button = widgets.Button(description=\"Train N Steps\", button_style=\"danger\", icon=\"forward\")\n",
    "predictor_training_output = widgets.Output()\n",
    "\n",
    "training_widgets.update({\n",
    "    \"predictor_threshold\": predictor_threshold,\n",
    "    \"predictor_max_steps\": predictor_max_steps,\n",
    "    \"predictor_steps\": predictor_steps,\n",
    "    \"predictor_training_output\": predictor_training_output,\n",
    "})\n",
    "\n",
    "train_predictor_threshold_button.on_click(on_train_predictor_threshold)\n",
    "train_predictor_steps_button.on_click(on_train_predictor_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ac20c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d5f101a8fbe646b1804b0cf475dec251",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Session', index=6, layout=Layout(width='300px'), options=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive controls and callbacks\n",
    "session_state = {\n",
    "    \"session_name\": None,\n",
    "    \"session_dir\": None,\n",
    "    \"metadata\": {},\n",
    "    \"events\": [],\n",
    "    \"observations\": [],\n",
    "    \"actions\": [],\n",
    "    \"autoencoder\": None,\n",
    "    \"predictor\": None,\n",
    "    \"feature_cache\": {},\n",
    "    \"action_space\": [],\n",
    "}\n",
    "\n",
    "session_widgets = {}\n",
    "\n",
    "def reset_feature_cache():\n",
    "    session_state[\"feature_cache\"] = {}\n",
    "\n",
    "def on_refresh_sessions(_=None):\n",
    "    options = list_session_dirs(SESSIONS_BASE_DIR)\n",
    "    current = session_widgets[\"session_dropdown\"].value if \"session_dropdown\" in session_widgets else None\n",
    "    session_widgets[\"session_dropdown\"].options = options\n",
    "    if not options:\n",
    "        session_widgets[\"session_dropdown\"].value = None\n",
    "    elif current in options:\n",
    "        session_widgets[\"session_dropdown\"].value = current\n",
    "    else:\n",
    "        session_widgets[\"session_dropdown\"].value = options[-1]\n",
    "\n",
    "def on_load_session(_):\n",
    "    dropdown = session_widgets[\"session_dropdown\"]\n",
    "    session_name = dropdown.value\n",
    "    if not session_name:\n",
    "        return\n",
    "    session_dir = os.path.join(SESSIONS_BASE_DIR, session_name)\n",
    "    metadata = load_session_metadata(session_dir)\n",
    "    events = load_session_events(session_dir)\n",
    "    observations = extract_observations(events, session_dir)\n",
    "    actions = extract_actions(events)\n",
    "\n",
    "    session_state.update({\n",
    "        \"session_name\": session_name,\n",
    "        \"session_dir\": session_dir,\n",
    "        \"metadata\": metadata,\n",
    "        \"events\": events,\n",
    "        \"observations\": observations,\n",
    "        \"actions\": actions,\n",
    "    })\n",
    "    session_state[\"action_space\"] = get_action_space(session_state)\n",
    "    reset_feature_cache()\n",
    "    tensor_cache.clear()\n",
    "    load_frame_bytes.cache_clear()\n",
    "\n",
    "    with session_widgets[\"session_area\"]:\n",
    "        session_widgets[\"session_area\"].clear_output()\n",
    "        if not observations:\n",
    "            display(Markdown(f\"**{session_name}** has no observation frames.\"))\n",
    "            return\n",
    "        details = [\n",
    "            f\"**Session:** {session_name}\",\n",
    "            f\"**Total events:** {len(events)}\",\n",
    "            f\"**Observations:** {len(observations)}\",\n",
    "            f\"**Actions:** {len(actions)}\",\n",
    "        ]\n",
    "        if metadata:\n",
    "            start_time = metadata.get(\"start_time\")\n",
    "            if start_time:\n",
    "                details.append(f\"**Start:** {start_time}\")\n",
    "            robot_type = metadata.get(\"robot_type\")\n",
    "            if robot_type:\n",
    "                details.append(f\"**Robot:** {robot_type}\")\n",
    "        display(Markdown(\"<br>\".join(details)))\n",
    "\n",
    "        frame_slider = widgets.IntSlider(value=0, min=0, max=len(observations) - 1, description=\"Frame\", continuous_update=False)\n",
    "        play_widget = widgets.Play(interval=100, value=0, min=0, max=len(observations) - 1, step=1, description=\"Play\")\n",
    "        widgets.jslink((play_widget, \"value\"), (frame_slider, \"value\"))\n",
    "\n",
    "        frame_image = widgets.Image(format=\"jpg\")\n",
    "        frame_image.layout.width = \"448px\"\n",
    "        frame_info = widgets.HTML()\n",
    "        history_preview = widgets.Output()\n",
    "\n",
    "        session_widgets[\"frame_slider\"] = frame_slider\n",
    "        session_widgets[\"play_widget\"] = play_widget\n",
    "        session_widgets[\"frame_image\"] = frame_image\n",
    "        session_widgets[\"frame_info\"] = frame_info\n",
    "        session_widgets[\"history_preview\"] = history_preview\n",
    "\n",
    "        def update_history_preview(idx):\n",
    "            if \"history_preview\" not in session_widgets:\n",
    "                return\n",
    "            history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "            requested = history_slider_widget.value if history_slider_widget else 3\n",
    "            requested = max(1, requested)\n",
    "            requested = min(requested, idx + 1)\n",
    "            start = max(0, idx - requested + 1)\n",
    "            obs_slice = observations[start: idx + 1]\n",
    "            events_local = session_state.get(\"events\", [])\n",
    "            display_items = []\n",
    "            for offset, obs in enumerate(obs_slice):\n",
    "                frame_bytes = load_frame_bytes(obs[\"full_path\"])\n",
    "                border_color = \"#4caf50\" if (start + offset) == idx else \"#cccccc\"\n",
    "                image = widgets.Image(value=frame_bytes, format=\"jpg\", layout=widgets.Layout(width=\"160px\", height=\"120px\", border=f\"2px solid {border_color}\"))\n",
    "                label_text = f\"Step {obs['step']}\"\n",
    "                if (start + offset) == idx:\n",
    "                    label_text += \" (current)\"\n",
    "                label = widgets.HTML(value=f\"<div style='text-align:center; font-size:10px'>{label_text}</div>\")\n",
    "                display_items.append(widgets.VBox([image, label]))\n",
    "                if offset < len(obs_slice) - 1:\n",
    "                    next_obs = obs_slice[offset + 1]\n",
    "                    actions_between = [events_local[i] for i in range(obs[\"event_index\"] + 1, next_obs[\"event_index\"]) if events_local[i].get(\"type\") == \"action\"]\n",
    "                    if actions_between:\n",
    "                        action_text = \"; \".join(format_action_label(act.get(\"data\", {})) for act in actions_between)\n",
    "                    else:\n",
    "                        action_text = \"No action\"\n",
    "                    action_label = widgets.HTML(value=f\"<div style='font-size:10px; padding:0 6px;'>Action: {action_text}</div>\", layout=widgets.Layout(height=\"120px\", display=\"flex\", align_items=\"center\", justify_content=\"center\"))\n",
    "                    display_items.append(action_label)\n",
    "            session_widgets[\"history_preview\"].clear_output()\n",
    "            with session_widgets[\"history_preview\"]:\n",
    "                if display_items:\n",
    "                    layout = widgets.Layout(display=\"flex\", flex_flow=\"row\", align_items=\"center\")\n",
    "                    display(widgets.HBox(display_items, layout=layout))\n",
    "                else:\n",
    "                    display(Markdown(\"History preview unavailable for this frame.\"))\n",
    "\n",
    "        session_widgets[\"update_history_preview\"] = update_history_preview\n",
    "\n",
    "        def update_frame(change):\n",
    "            idx_local = change[\"new\"] if isinstance(change, dict) else change\n",
    "            observation = observations[idx_local]\n",
    "            frame_image.value = load_frame_bytes(observation[\"full_path\"])\n",
    "            frame_info.value = f\"<b>Observation {idx_local + 1} / {len(observations)}</b><br>Step: {observation['step']}<br>Timestamp: {format_timestamp(observation['timestamp'])}\"\n",
    "            update_history_preview(idx_local)\n",
    "\n",
    "        frame_slider.observe(update_frame, names=\"value\")\n",
    "        update_frame({\"new\": frame_slider.value})\n",
    "\n",
    "        display(widgets.VBox([\n",
    "            widgets.HBox([play_widget, frame_slider]),\n",
    "            frame_image,\n",
    "            frame_info,\n",
    "            widgets.HTML(\"<b>History preview</b>\"),\n",
    "            history_preview,\n",
    "        ]))\n",
    "\n",
    "    session_widgets[\"model_status\"].value = \"\"\n",
    "    session_widgets[\"autoencoder_output\"].clear_output()\n",
    "    session_widgets[\"predictor_output\"].clear_output()\n",
    "\n",
    "def on_load_models(_):\n",
    "    messages = []\n",
    "    auto_path = session_widgets[\"autoencoder_path\"].value.strip()\n",
    "    predictor_path = session_widgets[\"predictor_path\"].value.strip()\n",
    "\n",
    "    if auto_path:\n",
    "        if os.path.exists(auto_path):\n",
    "            try:\n",
    "                session_state[\"autoencoder\"] = load_autoencoder_model(auto_path, device)\n",
    "                reset_feature_cache()\n",
    "                messages.append(f\"Autoencoder loaded from `{auto_path}`.\")\n",
    "            except Exception as exc:\n",
    "                session_state[\"autoencoder\"] = None\n",
    "                messages.append(f\"<span style='color:red'>Failed to load autoencoder: {exc}</span>\")\n",
    "        else:\n",
    "            session_state[\"autoencoder\"] = None\n",
    "            messages.append(f\"<span style='color:red'>Autoencoder path not found: {auto_path}</span>\")\n",
    "    else:\n",
    "        session_state[\"autoencoder\"] = None\n",
    "        messages.append(\"Autoencoder path is empty; skipping load.\")\n",
    "\n",
    "    if predictor_path:\n",
    "        if os.path.exists(predictor_path):\n",
    "            try:\n",
    "                session_state[\"predictor\"] = load_predictor_model(predictor_path, device)\n",
    "                messages.append(f\"Predictor loaded from `{predictor_path}`.\")\n",
    "            except Exception as exc:\n",
    "                session_state[\"predictor\"] = None\n",
    "                messages.append(f\"<span style='color:red'>Failed to load predictor: {exc}</span>\")\n",
    "        else:\n",
    "            session_state[\"predictor\"] = None\n",
    "            messages.append(f\"<span style='color:red'>Predictor path not found: {predictor_path}</span>\")\n",
    "    else:\n",
    "        session_state[\"predictor\"] = None\n",
    "        messages.append(\"Predictor path is empty; skipping load.\")\n",
    "\n",
    "    session_widgets[\"model_status\"].value = \"<br>\".join(messages)\n",
    "\n",
    "def on_run_autoencoder(_):\n",
    "    autoencoder = session_state.get(\"autoencoder\")\n",
    "    if autoencoder is None:\n",
    "        with session_widgets[\"autoencoder_output\"]:\n",
    "            session_widgets[\"autoencoder_output\"].clear_output()\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "        return\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with session_widgets[\"autoencoder_output\"]:\n",
    "            session_widgets[\"autoencoder_output\"].clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        return\n",
    "\n",
    "    idx = frame_slider.value\n",
    "    observation = session_state.get(\"observations\", [])[idx]\n",
    "    frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = autoencoder.reconstruct(frame_tensor)\n",
    "    mse = torch.nn.functional.mse_loss(reconstructed, frame_tensor).item()\n",
    "\n",
    "    original_img = tensor_to_numpy_image(frame_tensor)\n",
    "    reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "\n",
    "    with session_widgets[\"autoencoder_output\"]:\n",
    "        session_widgets[\"autoencoder_output\"].clear_output()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Input\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"Reconstruction MSE: {mse:.6f}\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def on_run_predictor(_):\n",
    "    autoencoder = session_state.get(\"autoencoder\")\n",
    "    predictor = session_state.get(\"predictor\")\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "\n",
    "    with session_widgets[\"predictor_output\"]:\n",
    "        session_widgets[\"predictor_output\"].clear_output()\n",
    "        if autoencoder is None or predictor is None:\n",
    "            display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "            return\n",
    "        if frame_slider is None:\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "            return\n",
    "\n",
    "        target_idx = frame_slider.value\n",
    "        history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "        desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "\n",
    "        selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "        if error:\n",
    "            display(Markdown(f\"**Cannot run predictor:** {error}\"))\n",
    "            return\n",
    "\n",
    "        actual_history = len(selected_obs)\n",
    "        if history_slider_widget and actual_history != history_slider_widget.value:\n",
    "            history_slider_widget.value = actual_history\n",
    "\n",
    "        feature_history = []\n",
    "        for obs in selected_obs:\n",
    "            cached = session_state[\"feature_cache\"].get(obs[\"frame_path\"])\n",
    "            if cached is None:\n",
    "                tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "                autoencoder.eval()\n",
    "                with torch.no_grad():\n",
    "                    encoded = autoencoder.encode(tensor)\n",
    "                session_state[\"feature_cache\"][obs[\"frame_path\"]] = encoded.detach().cpu()\n",
    "                cached = session_state[\"feature_cache\"][obs[\"frame_path\"]]\n",
    "            feature_history.append(cached)\n",
    "\n",
    "        feature_history_gpu = [feat.to(device) for feat in feature_history]\n",
    "\n",
    "        recorded_future_action, action_source = get_future_action_for_prediction(session_state, target_idx)\n",
    "        recorded_action = clone_action(recorded_future_action) if recorded_future_action is not None else {}\n",
    "\n",
    "        predictor.eval()\n",
    "        autoencoder.eval()\n",
    "\n",
    "        if recorded_future_action is None:\n",
    "            display(Markdown(\"No recorded action between current and next frame; using an empty action for comparison.\"))\n",
    "        elif action_source == \"previous\":\n",
    "            display(Markdown(\"Using the most recent action prior to the current frame for comparison.\"))\n",
    "        predictor.eval()\n",
    "        autoencoder.eval()\n",
    "\n",
    "        if recorded_future_action is None:\n",
    "            display(Markdown(\"No recorded action between current and next frame; using an empty action for comparison.\"))\n",
    "\n",
    "        def predict_for_action(action_dict):\n",
    "            history_actions = [clone_action(act) for act in action_dicts]\n",
    "            if action_dict is not None:\n",
    "                history_actions.append(clone_action(action_dict))\n",
    "            else:\n",
    "                history_actions.append({})\n",
    "            pred_features = predictor(feature_history_gpu, history_actions)\n",
    "            decoded_candidate = decode_features_to_image(autoencoder, pred_features)\n",
    "            return decoded_candidate\n",
    "\n",
    "        next_obs = session_state[\"observations\"][target_idx + 1] if target_idx + 1 < len(session_state[\"observations\"]) else None\n",
    "        actual_tensor_cpu = None\n",
    "        actual_tensor_gpu = None\n",
    "        actual_img = None\n",
    "        if next_obs is not None:\n",
    "            actual_tensor_cpu = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0)\n",
    "            actual_tensor_gpu = actual_tensor_cpu.to(device)\n",
    "            actual_img = tensor_to_numpy_image(actual_tensor_cpu)\n",
    "\n",
    "        all_predictions = []\n",
    "        with torch.no_grad():\n",
    "            recorded_pred_tensor = predict_for_action(recorded_action if recorded_future_action is not None else None)\n",
    "            recorded_img = tensor_to_numpy_image(recorded_pred_tensor)\n",
    "            recorded_mse = None\n",
    "            if actual_tensor_gpu is not None:\n",
    "                recorded_mse = torch.nn.functional.mse_loss(recorded_pred_tensor, actual_tensor_gpu).item()\n",
    "            recorded_label = \"Recorded action\" if recorded_future_action is not None else \"Recorded action (none)\"\n",
    "            all_predictions.append({\n",
    "                \"label\": recorded_label,\n",
    "                \"action\": clone_action(recorded_action),\n",
    "                \"image\": recorded_img,\n",
    "                \"mse\": recorded_mse,\n",
    "            })\n",
    "\n",
    "            for idx, action in enumerate(session_state.get(\"action_space\", [])):\n",
    "                if actions_equal(action, recorded_action):\n",
    "                    continue\n",
    "                pred_tensor = predict_for_action(action)\n",
    "                pred_img = tensor_to_numpy_image(pred_tensor)\n",
    "                mse_value = None\n",
    "                if actual_tensor_gpu is not None:\n",
    "                    mse_value = torch.nn.functional.mse_loss(pred_tensor, actual_tensor_gpu).item()\n",
    "                all_predictions.append({\n",
    "                    \"label\": f\"{idx + 1}. {format_action_label(action)}\",\n",
    "                    \"action\": clone_action(action),\n",
    "                    \"image\": pred_img,\n",
    "                    \"mse\": mse_value,\n",
    "                })\n",
    "\n",
    "        history_steps_text = \", \".join(str(obs[\"step\"]) for obs in selected_obs)\n",
    "        action_lines = []\n",
    "        for idx, action in enumerate(action_dicts, 1):\n",
    "            action_lines.append(f\"{idx}. {format_action_label(action)}\")\n",
    "        if not action_lines:\n",
    "            action_lines.append(\"(No actions in window)\")\n",
    "\n",
    "        display(Markdown(f\"**History steps:** {history_steps_text}\"))\n",
    "        display(Markdown(\"**Recorded actions:**<br>\" + \"<br>\".join(action_lines)))\n",
    "\n",
    "        history_fig, history_axes = plt.subplots(1, len(selected_obs), figsize=(3 * len(selected_obs), 3))\n",
    "        if isinstance(history_axes, np.ndarray):\n",
    "            axes_list = history_axes.flatten()\n",
    "        else:\n",
    "            axes_list = [history_axes]\n",
    "        for idx, (obs, ax) in enumerate(zip(selected_obs, axes_list)):\n",
    "            img = np.array(load_frame_image(obs[\"full_path\"]))\n",
    "            ax.imshow(img)\n",
    "            ax.set_title(f\"Step {obs['step']}\")\n",
    "            ax.axis(\"off\")\n",
    "            if idx < len(action_dicts):\n",
    "                ax.set_xlabel(format_action_label(action_dicts[idx]), fontsize=9)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        if actual_img is not None and all_predictions:\n",
    "            fig, axes = plt.subplots(1, 2, figsize=(10, 4))\n",
    "            axes[0].imshow(all_predictions[0][\"image\"])\n",
    "            axes[0].set_title(\"Predicted (recorded action)\")\n",
    "            axes[0].axis(\"off\")\n",
    "            title = f\"Actual next frame (step {next_obs['step']})\"\n",
    "            if all_predictions[0][\"mse\"] is not None:\n",
    "                title += f\"MSE: {all_predictions[0]['mse']:.6f}\"\n",
    "            axes[1].imshow(actual_img)\n",
    "            axes[1].set_title(title)\n",
    "            axes[1].axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        elif all_predictions:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(5, 4))\n",
    "            ax.imshow(all_predictions[0][\"image\"])\n",
    "            ax.set_title(\"Predicted next frame (recorded action)\")\n",
    "            ax.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if all_predictions:\n",
    "            cols = min(4, len(all_predictions))\n",
    "            rows = math.ceil(len(all_predictions) / cols)\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 3.5 * rows))\n",
    "            axes = np.array(axes).reshape(rows, cols)\n",
    "            for idx, prediction in enumerate(all_predictions):\n",
    "                ax = axes[idx // cols][idx % cols]\n",
    "                ax.imshow(prediction[\"image\"])\n",
    "                title = prediction[\"label\"]\n",
    "                if actions_equal(prediction[\"action\"], recorded_action):\n",
    "                    title += \" (recorded)\"\n",
    "                if prediction[\"mse\"] is not None:\n",
    "                    title += f\"MSE: {prediction['mse']:.6f}\"\n",
    "                ax.set_title(title, fontsize=9)\n",
    "                ax.axis(\"off\")\n",
    "            for idx in range(len(all_predictions), rows * cols):\n",
    "                axes[idx // cols][idx % cols].axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "        else:\n",
    "            display(Markdown(\"No actions available to visualize predictions.\"))\n",
    "\n",
    "def on_history_slider_change(_):\n",
    "    if \"frame_slider\" in session_widgets and \"update_history_preview\" in session_widgets:\n",
    "        session_widgets[\"update_history_preview\"](session_widgets[\"frame_slider\"].value)\n",
    "\n",
    "session_dropdown = widgets.Dropdown(description=\"Session\", layout=widgets.Layout(width=\"300px\"))\n",
    "session_widgets[\"session_dropdown\"] = session_dropdown\n",
    "\n",
    "refresh_button = widgets.Button(description=\"Refresh\", icon=\"refresh\")\n",
    "load_session_button = widgets.Button(description=\"Load Session\", button_style=\"primary\")\n",
    "\n",
    "session_area = widgets.Output()\n",
    "session_widgets[\"session_area\"] = session_area\n",
    "\n",
    "autoencoder_path = widgets.Text(value=DEFAULT_AUTOENCODER_PATH, description=\"Autoencoder\", layout=widgets.Layout(width=\"520px\"))\n",
    "predictor_path = widgets.Text(value=DEFAULT_PREDICTOR_PATH, description=\"Predictor\", layout=widgets.Layout(width=\"520px\"))\n",
    "session_widgets[\"autoencoder_path\"] = autoencoder_path\n",
    "session_widgets[\"predictor_path\"] = predictor_path\n",
    "\n",
    "model_status = widgets.HTML()\n",
    "session_widgets[\"model_status\"] = model_status\n",
    "\n",
    "run_autoencoder_button = widgets.Button(description=\"Run Autoencoder\", button_style=\"success\", icon=\"play\")\n",
    "autoencoder_output = widgets.Output()\n",
    "session_widgets[\"autoencoder_output\"] = autoencoder_output\n",
    "\n",
    "history_slider = widgets.IntSlider(value=3, min=2, max=8, description=\"History\", continuous_update=False)\n",
    "session_widgets[\"history_slider\"] = history_slider\n",
    "history_slider.observe(on_history_slider_change, names=\"value\")\n",
    "\n",
    "run_predictor_button = widgets.Button(description=\"Run Predictor\", button_style=\"info\", icon=\"forward\")\n",
    "predictor_output = widgets.Output()\n",
    "session_widgets[\"predictor_output\"] = predictor_output\n",
    "\n",
    "refresh_button.on_click(on_refresh_sessions)\n",
    "load_session_button.on_click(on_load_session)\n",
    "load_models_button = widgets.Button(description=\"Load Models\", button_style=\"primary\", icon=\"upload\")\n",
    "load_models_button.on_click(on_load_models)\n",
    "run_autoencoder_button.on_click(on_run_autoencoder)\n",
    "run_predictor_button.on_click(on_run_predictor)\n",
    "\n",
    "on_refresh_sessions()\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([session_dropdown, refresh_button, load_session_button]),\n",
    "    session_area,\n",
    "    widgets.HTML(\"<hr><b>Model Checkpoints</b>\"),\n",
    "    autoencoder_path,\n",
    "    predictor_path,\n",
    "    load_models_button,\n",
    "    model_status,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Autoencoder Inference</b>\"),\n",
    "        widgets.HTML(\"Uses the currently selected frame.\"),\n",
    "        run_autoencoder_button,\n",
    "        autoencoder_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Predictor Inference</b>\"),\n",
    "        widgets.HTML(\"History uses frames leading up to the current selection to predict the next observation.\"),\n",
    "        history_slider,\n",
    "        run_predictor_button,\n",
    "        predictor_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Autoencoder Training (AdaptiveWorldModel)</b>\"),\n",
    "        widgets.HTML(\"Train the autoencoder using AdaptiveWorldModel.train_autoencoder() with randomized masking.\"),\n",
    "        widgets.HBox([autoencoder_threshold, autoencoder_max_steps]),\n",
    "        widgets.HBox([train_autoencoder_threshold_button, train_autoencoder_steps_button, autoencoder_steps]),\n",
    "        autoencoder_training_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Predictor Training (AdaptiveWorldModel)</b>\"),\n",
    "        widgets.HTML(\"Train the predictor using AdaptiveWorldModel.train_predictor() with joint autoencoder training.\"),\n",
    "        widgets.HBox([predictor_threshold, predictor_max_steps]),\n",
    "        widgets.HBox([train_predictor_threshold_button, train_predictor_steps_button, predictor_steps]),\n",
    "        predictor_training_output,\n",
    "    ]),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f381ecb-0e7a-4e2c-a124-779a5a2ddd1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
