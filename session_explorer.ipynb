{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b6bc8f2",
   "metadata": {},
   "source": [
    "# Session Explorer\n",
    "This notebook lets you explore recorded robot sessions, scrub through frames, run the autoencoder and predictor models on stored observations, and train the models on selected frames/history."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e957658",
   "metadata": {},
   "source": [
    "## How to Use\n",
    "1. Pick a session and load it.\n",
    "2. Use the playback controls to scrub through frames.\n",
    "3. (Optional) Load model checkpoints, then run the Autoencoder and Predictor sections using the current frame selection.\n",
    "4. (Optional) Use the Training sections to train models on current frame/history until a loss threshold is met or for a specified number of steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a8527353",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import io\n",
    "import json\n",
    "import glob\n",
    "import datetime\n",
    "from functools import lru_cache\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "from PIL import Image\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "import config\n",
    "from models import MaskedAutoencoderViT, TransformerActionConditionedPredictor\n",
    "from adaptive_world_model import AdaptiveWorldModel, normalize_action_dicts\n",
    "from robot_interface import RobotInterface\n",
    "\n",
    "# Additional imports for training\n",
    "import time\n",
    "from tqdm.auto import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6399c255",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Using device: cuda\n",
      "Loading from primary checkpoint files...\n",
      "Autoencoder checkpoint loaded\n",
      "Learning progress loaded: 278445 autoencoder steps, 10149 predictor steps, 10150 actions\n"
     ]
    }
   ],
   "source": [
    "SESSIONS_BASE_DIR = config.RECORDING_BASE_DIR\n",
    "DEFAULT_AUTOENCODER_PATH = os.path.join(config.DEFAULT_CHECKPOINT_DIR, \"autoencoder.pth\")\n",
    "DEFAULT_PREDICTOR_PATH = os.path.join(config.DEFAULT_CHECKPOINT_DIR, \"predictor_0.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Create a stub robot interface for the AdaptiveWorldModel\n",
    "class StubRobotInterface(RobotInterface):\n",
    "    \"\"\"Stub robot interface for notebook training purposes\"\"\"\n",
    "    def get_observation(self):\n",
    "        # Return a dummy observation - not used in training\n",
    "        return {\"frame\": np.zeros((224, 224, 3), dtype=np.uint8)}\n",
    "    \n",
    "    def execute_action(self, action):\n",
    "        # Do nothing - not used in training\n",
    "        pass\n",
    "    \n",
    "    @property\n",
    "    def action_space(self):\n",
    "        # Return action space matching the JetBot's 2-action space\n",
    "        # (needed for predictor's action classifier to match checkpoint)\n",
    "        return [\n",
    "            {\"motor_left\": 0, \"motor_right\": 0, \"duration\": 0.2},      # Action 0: stop\n",
    "            {\"motor_left\": 0, \"motor_right\": 0.12, \"duration\": 0.2}   # Action 1: forward\n",
    "        ]\n",
    "    \n",
    "    def cleanup(self):\n",
    "        pass\n",
    "\n",
    "# Instantiate AdaptiveWorldModel for training access\n",
    "stub_robot = StubRobotInterface()\n",
    "adaptive_world_model = AdaptiveWorldModel(stub_robot, wandb_project=\"\", checkpoint_dir=config.DEFAULT_CHECKPOINT_DIR, interactive=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bf2f58fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility helpers for loading sessions, caching frames, and preparing model inputs\n",
    "def list_session_dirs(base_dir):\n",
    "    # Return sorted session directory names.\n",
    "    if not os.path.exists(base_dir):\n",
    "        return []\n",
    "    entries = []\n",
    "    for name in os.listdir(base_dir):\n",
    "        path = os.path.join(base_dir, name)\n",
    "        if os.path.isdir(path) and name.startswith(\"session_\"):\n",
    "            entries.append(name)\n",
    "    entries.sort()\n",
    "    return entries\n",
    "\n",
    "def load_session_metadata(session_dir):\n",
    "    meta_path = os.path.join(session_dir, \"session_meta.json\")\n",
    "    if os.path.exists(meta_path):\n",
    "        with open(meta_path, \"r\") as f:\n",
    "            return json.load(f)\n",
    "    return {}\n",
    "\n",
    "def load_session_events(session_dir):\n",
    "    # Load all events from shard files and sort them by step.\n",
    "    pattern = os.path.join(session_dir, \"events_shard_*.jsonl\")\n",
    "    shard_files = sorted(glob.glob(pattern))\n",
    "    events = []\n",
    "    for shard_path in shard_files:\n",
    "        with open(shard_path, \"r\") as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if not line:\n",
    "                    continue\n",
    "                events.append(json.loads(line))\n",
    "    events.sort(key=lambda evt: evt.get(\"step\", 0))\n",
    "    return events\n",
    "\n",
    "def extract_observations(events, session_dir):\n",
    "    observations = []\n",
    "    for idx, event in enumerate(events):\n",
    "        if event.get(\"type\") != \"observation\":\n",
    "            continue\n",
    "        data = event.get(\"data\", {})\n",
    "        frame_path = data.get(\"frame_path\")\n",
    "        if not frame_path:\n",
    "            continue\n",
    "        observations.append({\n",
    "            \"observation_index\": len(observations),\n",
    "            \"event_index\": idx,\n",
    "            \"step\": event.get(\"step\", idx),\n",
    "            \"timestamp\": event.get(\"timestamp\"),\n",
    "            \"frame_path\": frame_path,\n",
    "            \"full_path\": os.path.join(session_dir, frame_path),\n",
    "        })\n",
    "    return observations\n",
    "\n",
    "def extract_actions(events):\n",
    "    actions = []\n",
    "    for idx, event in enumerate(events):\n",
    "        if event.get(\"type\") != \"action\":\n",
    "            continue\n",
    "        actions.append({\n",
    "            \"action_index\": len(actions),\n",
    "            \"event_index\": idx,\n",
    "            \"step\": event.get(\"step\", idx),\n",
    "            \"timestamp\": event.get(\"timestamp\"),\n",
    "            \"action\": event.get(\"data\", {}),\n",
    "        })\n",
    "    return actions\n",
    "\n",
    "@lru_cache(maxsize=4096)\n",
    "def load_frame_bytes(full_path):\n",
    "    if not os.path.exists(full_path):\n",
    "        raise FileNotFoundError(f\"Frame file not found: {full_path}\")\n",
    "    with open(full_path, \"rb\") as f:\n",
    "        return f.read()\n",
    "\n",
    "def load_frame_image(full_path):\n",
    "    return Image.open(io.BytesIO(load_frame_bytes(full_path))).convert(\"RGB\")\n",
    "\n",
    "tensor_cache = {}\n",
    "\n",
    "def get_frame_tensor(session_dir, frame_path):\n",
    "    # Return normalized (C,H,W) tensor for a frame, cached on CPU.\n",
    "    key = (session_dir, frame_path)\n",
    "    if key not in tensor_cache:\n",
    "        full_path = os.path.join(session_dir, frame_path)\n",
    "        pil_img = load_frame_image(full_path)\n",
    "        tensor_cache[key] = config.TRANSFORM(pil_img)\n",
    "    return tensor_cache[key]\n",
    "\n",
    "def tensor_to_numpy_image(tensor):\n",
    "    if tensor.ndim == 4:\n",
    "        tensor = tensor[0]\n",
    "    tensor = tensor.detach().cpu().float()\n",
    "    tmin, tmax = float(tensor.min()), float(tensor.max())\n",
    "    if tmin < -0.01 or tmax > 1.01:\n",
    "        tensor = tensor * 0.5 + 0.5\n",
    "    tensor = tensor.clamp(0.0, 1.0)\n",
    "    return tensor.permute(1, 2, 0).numpy()\n",
    "\n",
    "def format_timestamp(ts):\n",
    "    if ts is None:\n",
    "        return \"N/A\"\n",
    "    try:\n",
    "        return datetime.datetime.fromtimestamp(ts).isoformat()\n",
    "    except Exception:\n",
    "        return str(ts)\n",
    "\n",
    "def describe_action(action):\n",
    "    if not action:\n",
    "        return \"{}\"\n",
    "    parts = []\n",
    "    for key in sorted(action.keys()):\n",
    "        parts.append(f\"{key}: {action[key]}\")\n",
    "    return \", \".join(parts)\n",
    "\n",
    "def canonical_action_key(action):\n",
    "    if not action:\n",
    "        return ()\n",
    "    return tuple(sorted(action.items()))\n",
    "\n",
    "def get_action_space(session_state):\n",
    "    metadata_actions = session_state.get(\"metadata\", {}).get(\"action_space\") or []\n",
    "    if metadata_actions:\n",
    "        return metadata_actions\n",
    "    unique = []\n",
    "    seen = set()\n",
    "    for action_entry in session_state.get(\"actions\", []):\n",
    "        action = action_entry.get(\"action\", {})\n",
    "        key = canonical_action_key(action)\n",
    "        if key and key not in seen:\n",
    "            seen.add(key)\n",
    "            unique.append(action)\n",
    "    return unique\n",
    "\n",
    "def format_action_label(action):\n",
    "    if not action:\n",
    "        return \"{}\"\n",
    "    parts = []\n",
    "    for key in sorted(action.keys()):\n",
    "        parts.append(f\"{key}={action[key]}\")\n",
    "    return \", \".join(parts)\n",
    "\n",
    "def clone_action(action):\n",
    "    if not action:\n",
    "        return {}\n",
    "    return {key: float(value) if isinstance(value, (int, float)) else value for key, value in action.items()}\n",
    "\n",
    "def actions_equal(action_a, action_b):\n",
    "    return canonical_action_key(action_a) == canonical_action_key(action_b)\n",
    "\n",
    "def load_autoencoder_model(path, device):\n",
    "    model = MaskedAutoencoderViT()\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_predictor_model(path, device):\n",
    "    model = TransformerActionConditionedPredictor()\n",
    "    checkpoint = torch.load(path, map_location=device)\n",
    "    state_dict = checkpoint.get(\"model_state_dict\", checkpoint)\n",
    "    model.load_state_dict(state_dict, strict=False)\n",
    "    level = checkpoint.get(\"level\")\n",
    "    if level is not None:\n",
    "        model.level = level\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def build_action_variants(action, action_space=None):\n",
    "    \"\"\"Build action variants using the actual action space instead of percentage variations.\n",
    "\n",
    "    Args:\n",
    "        action: The recorded action (unused, kept for compatibility)\n",
    "        action_space: List of all possible actions from the robot's action space\n",
    "\n",
    "    Returns:\n",
    "        List of action dictionaries from the action space\n",
    "    \"\"\"\n",
    "    if action_space:\n",
    "        return [clone_action(a) for a in action_space]\n",
    "    # Fallback if no action_space provided\n",
    "    if not action:\n",
    "        return []\n",
    "    return [clone_action(action)]\n",
    "\n",
    "\n",
    "def compute_attention_visual_data(attn_info):\n",
    "    if not attn_info or not attn_info.get('attn'):\n",
    "        return None\n",
    "    attn_list = attn_info['attn']\n",
    "    last_layer_attn = attn_list[-1]\n",
    "    if last_layer_attn is None or last_layer_attn.size(0) == 0:\n",
    "        return None\n",
    "    attn_last = last_layer_attn[0]\n",
    "    future_idx = attn_info['future_idx']\n",
    "    if future_idx.numel() == 0:\n",
    "        return None\n",
    "\n",
    "    attn_future = attn_last.mean(dim=0)[future_idx]\n",
    "    total = attn_future.sum(dim=-1, keepdim=True) + 1e-8\n",
    "\n",
    "    if attn_info['last_action_pos'] is not None:\n",
    "        apa = attn_future[..., attn_info['last_action_pos']]\n",
    "    else:\n",
    "        apa = torch.zeros_like(attn_future[:, 0])\n",
    "\n",
    "    if attn_info['last_frame_idx'].numel() > 0:\n",
    "        alf = attn_future[..., attn_info['last_frame_idx']].sum(dim=-1)\n",
    "    else:\n",
    "        alf = torch.zeros_like(attn_future[:, 0])\n",
    "\n",
    "    if attn_info['action_idx'].numel() > 0:\n",
    "        action_mass = attn_future[..., attn_info['action_idx']].sum(dim=-1)\n",
    "    else:\n",
    "        action_mass = torch.zeros_like(attn_future[:, 0])\n",
    "\n",
    "    if attn_info['frame_idx'].numel() > 0:\n",
    "        frame_mass = attn_future[..., attn_info['frame_idx']].sum(dim=-1)\n",
    "    else:\n",
    "        frame_mass = torch.zeros_like(attn_future[:, 0])\n",
    "\n",
    "    ttar = action_mass / (frame_mass + 1e-8)\n",
    "\n",
    "    k = min(16, attn_future.shape[-1])\n",
    "    recent_mass = attn_future[..., -k:].sum(dim=-1)\n",
    "    ri_k = recent_mass / (total.squeeze(-1))\n",
    "\n",
    "    P = (attn_future / total).clamp_min(1e-8)\n",
    "    entropy = -(P * P.log()).sum(dim=-1)\n",
    "\n",
    "    metrics = {\n",
    "        'APA': apa.mean().item(),\n",
    "        'ALF': alf.mean().item(),\n",
    "        'TTAR': ttar.mean().item(),\n",
    "        'RI@16': ri_k.mean().item(),\n",
    "        'Entropy': entropy.mean().item(),\n",
    "        'UniformBaseline': (1.0 / (future_idx.to(torch.float32) + 1.0)).mean().item(),\n",
    "    }\n",
    "\n",
    "    last_action_frac = (apa / total.squeeze(-1)).cpu().numpy()\n",
    "    last_frame_frac = (alf / total.squeeze(-1)).cpu().numpy()\n",
    "    rest_frac = np.clip(1.0 - last_action_frac - last_frame_frac, 0.0, 1.0)\n",
    "\n",
    "    heatmap = attn_last[0][future_idx].cpu().numpy()\n",
    "    token_types = attn_info['token_types'].cpu().numpy()\n",
    "\n",
    "    return {\n",
    "        'metrics': metrics,\n",
    "        'heatmap': heatmap,\n",
    "        'token_types': token_types,\n",
    "        'breakdown': {\n",
    "            'last_action': last_action_frac,\n",
    "            'last_frame': last_frame_frac,\n",
    "            'rest': rest_frac,\n",
    "        },\n",
    "        'future_indices': future_idx.cpu().numpy(),\n",
    "    }\n",
    "\n",
    "\n",
    "def plot_attention_heatmap(heatmap, token_types):\n",
    "    if heatmap.size == 0:\n",
    "        return\n",
    "    fig, (ax_heat, ax_types) = plt.subplots(2, 1, figsize=(10, 4), gridspec_kw={'height_ratios': [4, 0.4]}, sharex=True)\n",
    "    im = ax_heat.imshow(heatmap, aspect='auto', cmap='viridis')\n",
    "    ax_heat.set_ylabel('Future Query')\n",
    "    ax_heat.set_title('Attention (last layer, head 0)')\n",
    "    fig.colorbar(im, ax=ax_heat, fraction=0.046, pad=0.04)\n",
    "    type_cmap = ListedColormap(['#4c72b0', '#dd8452', '#55a868'])\n",
    "    ax_types.imshow(token_types.reshape(1, -1), aspect='auto', cmap=type_cmap, vmin=0, vmax=2)\n",
    "    ax_types.set_yticks([])\n",
    "    ax_types.set_xlabel('Key Token Index')\n",
    "    ax_types.set_title('Token Types (frame/action/future)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_attention_breakdown(breakdown):\n",
    "    if not breakdown['last_action'].size:\n",
    "        return\n",
    "    indices = np.arange(len(breakdown['last_action']))\n",
    "    width = 0.25\n",
    "    plt.figure(figsize=(8, 4))\n",
    "    plt.bar(indices - width, breakdown['last_action'], width, label='Last Action')\n",
    "    plt.bar(indices, breakdown['last_frame'], width, label='Last Frame Block')\n",
    "    plt.bar(indices + width, breakdown['rest'], width, label='Rest')\n",
    "    plt.xlabel('Future Query Index')\n",
    "    plt.ylabel('Attention Fraction')\n",
    "    plt.ylim(0.0, 1.05)\n",
    "    plt.title('Attention Distribution per Future Query')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "def decode_features_to_image(autoencoder, predicted_features):\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        num_patches = autoencoder.patch_embed.num_patches\n",
    "        ids_restore = torch.arange(num_patches, device=predicted_features.device).unsqueeze(0).repeat(predicted_features.shape[0], 1)\n",
    "        pred_patches = autoencoder.forward_decoder(predicted_features, ids_restore)\n",
    "        decoded = autoencoder.unpatchify(pred_patches)\n",
    "    return decoded\n",
    "\n",
    "def build_predictor_sequence(session_state, target_obs_index, desired_length):\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    events = session_state.get(\"events\", [])\n",
    "    if not observations:\n",
    "        return [], [], \"No observations loaded.\"\n",
    "    if target_obs_index < 0 or target_obs_index >= len(observations):\n",
    "        return [], [], \"Selected observation is out of range.\"\n",
    "    desired_length = max(2, desired_length)\n",
    "    selected_obs = [observations[target_obs_index]]\n",
    "    action_dicts = []\n",
    "    current_idx = target_obs_index\n",
    "    current_event_index = observations[target_obs_index][\"event_index\"]\n",
    "    while len(selected_obs) < desired_length and current_idx > 0:\n",
    "        prev_idx = current_idx - 1\n",
    "        found = False\n",
    "        while prev_idx >= 0:\n",
    "            prev_obs = observations[prev_idx]\n",
    "            prev_event_index = prev_obs[\"event_index\"]\n",
    "            actions_between = [events[i] for i in range(prev_event_index + 1, current_event_index) if events[i].get(\"type\") == \"action\"]\n",
    "            if len(actions_between) == 1:\n",
    "                action_dicts.insert(0, actions_between[0].get(\"data\", {}))\n",
    "                selected_obs.insert(0, prev_obs)\n",
    "                current_idx = prev_idx\n",
    "                current_event_index = prev_event_index\n",
    "                found = True\n",
    "                break\n",
    "            prev_idx -= 1\n",
    "        if not found:\n",
    "            break\n",
    "    if len(selected_obs) < 2:\n",
    "        return [], [], \"Could not assemble a history with actions between frames. Choose a later frame.\"\n",
    "    return selected_obs, action_dicts, None\n",
    "\n",
    "def find_action_between_events(events, start_event_index, end_event_index):\n",
    "    \"\"\"Return the recorded action between two observation events, falling back to the prior action.\"\"\"\n",
    "    between_actions = [\n",
    "        event for event in events[start_event_index + 1:end_event_index]\n",
    "        if event.get(\"type\") == \"action\"\n",
    "    ]\n",
    "    if between_actions:\n",
    "        return clone_action(between_actions[-1].get(\"data\", {})), \"between\"\n",
    "\n",
    "    for idx in range(start_event_index, -1, -1):\n",
    "        event = events[idx]\n",
    "        if event.get(\"type\") == \"action\":\n",
    "            return clone_action(event.get(\"data\", {})), \"previous\"\n",
    "\n",
    "    return None, None\n",
    "\n",
    "\n",
    "def get_future_action_for_prediction(session_state, target_obs_index):\n",
    "    \"\"\"Return the action to pair with the next observation for prediction.\"\"\"\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    events = session_state.get(\"events\", [])\n",
    "    if target_obs_index < 0 or target_obs_index >= len(observations) - 1:\n",
    "        return None, None\n",
    "    current_obs = observations[target_obs_index]\n",
    "    next_obs = observations[target_obs_index + 1]\n",
    "    return find_action_between_events(events, current_obs[\"event_index\"], next_obs[\"event_index\"])\n",
    "\n",
    "def visualize_autoencoder_weights(autoencoder):\n",
    "    \"\"\"Visualize key autoencoder weights for monitoring changes\"\"\"\n",
    "    if autoencoder is None:\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get patch embedding weights (first layer)\n",
    "        patch_embed_weight = autoencoder.patch_embed.proj.weight.detach().cpu()\n",
    "        \n",
    "        # Get cls token and pos embed\n",
    "        cls_token = autoencoder.cls_token.detach().cpu()\n",
    "        pos_embed = autoencoder.pos_embed.detach().cpu()\n",
    "        \n",
    "        # Get some decoder weights\n",
    "        decoder_embed_weight = autoencoder.decoder_embed.weight.detach().cpu() if hasattr(autoencoder, 'decoder_embed') else None\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Patch embedding weights - show first 16 filters\n",
    "        patch_weights_vis = patch_embed_weight[:16].view(16, 3, 16, 16)\n",
    "        patch_grid = torch.cat([torch.cat([patch_weights_vis[i*4+j] for j in range(4)], dim=2) for i in range(4)], dim=1)\n",
    "        patch_grid = (patch_grid - patch_grid.min()) / (patch_grid.max() - patch_grid.min())\n",
    "        axes[0, 0].imshow(patch_grid.permute(1, 2, 0))\n",
    "        axes[0, 0].set_title(\"Patch Embed Weights (16 filters)\")\n",
    "        axes[0, 0].axis(\"off\")\n",
    "        \n",
    "        # Patch embedding weight statistics\n",
    "        axes[0, 1].hist(patch_embed_weight.flatten().numpy(), bins=50, alpha=0.7)\n",
    "        axes[0, 1].set_title(f\"Patch Embed Weight Distribution\\nMean: {patch_embed_weight.mean():.6f}, Std: {patch_embed_weight.std():.6f}\")\n",
    "        axes[0, 1].set_xlabel(\"Weight Value\")\n",
    "        axes[0, 1].set_ylabel(\"Count\")\n",
    "        \n",
    "        # CLS token visualization\n",
    "        cls_reshaped = cls_token.view(-1).numpy()\n",
    "        axes[0, 2].plot(cls_reshaped)\n",
    "        axes[0, 2].set_title(f\"CLS Token\\nMean: {cls_token.mean():.6f}, Std: {cls_token.std():.6f}\")\n",
    "        axes[0, 2].set_xlabel(\"Dimension\")\n",
    "        axes[0, 2].set_ylabel(\"Value\")\n",
    "        \n",
    "        # Position embedding visualization (first 100 dimensions)\n",
    "        pos_vis = pos_embed[0, :, :100].numpy()\n",
    "        im = axes[1, 0].imshow(pos_vis, aspect='auto', cmap='coolwarm')\n",
    "        axes[1, 0].set_title(f\"Position Embeddings (first 100 dims)\\nShape: {pos_embed.shape}\")\n",
    "        axes[1, 0].set_xlabel(\"Embedding Dimension\")\n",
    "        axes[1, 0].set_ylabel(\"Position\")\n",
    "        plt.colorbar(im, ax=axes[1, 0])\n",
    "        \n",
    "        # Decoder embedding weights if available\n",
    "        if decoder_embed_weight is not None:\n",
    "            axes[1, 1].hist(decoder_embed_weight.flatten().numpy(), bins=50, alpha=0.7, color='orange')\n",
    "            axes[1, 1].set_title(f\"Decoder Embed Weight Distribution\\nMean: {decoder_embed_weight.mean():.6f}, Std: {decoder_embed_weight.std():.6f}\")\n",
    "            axes[1, 1].set_xlabel(\"Weight Value\")\n",
    "            axes[1, 1].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, \"Decoder weights\\nnot available\", ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title(\"Decoder Weights\")\n",
    "        \n",
    "        # Weight norms across layers\n",
    "        layer_norms = []\n",
    "        layer_names = []\n",
    "        for name, param in autoencoder.named_parameters():\n",
    "            if 'weight' in name and param.dim() >= 2:\n",
    "                layer_norms.append(param.norm().item())\n",
    "                layer_names.append(name.split('.')[-2] if '.' in name else name)\n",
    "        \n",
    "        if layer_norms:\n",
    "            axes[1, 2].bar(range(len(layer_norms)), layer_norms)\n",
    "            axes[1, 2].set_title(\"Layer Weight Norms\")\n",
    "            axes[1, 2].set_xlabel(\"Layer\")\n",
    "            axes[1, 2].set_ylabel(\"L2 Norm\")\n",
    "            axes[1, 2].set_xticks(range(0, len(layer_names), max(1, len(layer_names)//5)))\n",
    "            axes[1, 2].set_xticklabels([layer_names[i] for i in range(0, len(layer_names), max(1, len(layer_names)//5))], rotation=45)\n",
    "        else:\n",
    "            axes[1, 2].text(0.5, 0.5, \"No weight layers\\nfound\", ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "            axes[1, 2].set_title(\"Layer Weight Norms\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Return weight statistics for comparison\n",
    "        stats = {\n",
    "            'patch_embed_mean': patch_embed_weight.mean().item(),\n",
    "            'patch_embed_std': patch_embed_weight.std().item(),\n",
    "            'cls_token_mean': cls_token.mean().item(),\n",
    "            'cls_token_std': cls_token.std().item(),\n",
    "            'pos_embed_mean': pos_embed.mean().item(),\n",
    "            'pos_embed_std': pos_embed.std().item(),\n",
    "            'layer_norms': dict(zip(layer_names, layer_norms)) if layer_norms else {}\n",
    "        }\n",
    "        return stats\n",
    "\n",
    "def visualize_predictor_weights(predictor):\n",
    "    \"\"\"Visualize key predictor (transformer) weights for monitoring changes\"\"\"\n",
    "    if predictor is None:\n",
    "        return None\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Get various transformer weights based on actual architecture\n",
    "        # Action embedding weights  \n",
    "        action_embed_weight = predictor.action_embedding.weight.detach().cpu() if hasattr(predictor, 'action_embedding') else None\n",
    "        \n",
    "        # Position embedding weights\n",
    "        pos_embed_weight = predictor.position_embedding.weight.detach().cpu() if hasattr(predictor, 'position_embedding') else None\n",
    "        \n",
    "        # Token type embedding weights\n",
    "        token_type_weight = predictor.token_type_embedding.weight.detach().cpu() if hasattr(predictor, 'token_type_embedding') else None\n",
    "        \n",
    "        # Future query parameter\n",
    "        future_query_weight = predictor.future_query.detach().cpu() if hasattr(predictor, 'future_query') else None\n",
    "        \n",
    "        # Get first transformer layer weights (TransformerEncoderLayer structure)\n",
    "        first_layer_self_attn_weight = None\n",
    "        first_layer_linear1_weight = None\n",
    "        first_layer_linear2_weight = None\n",
    "        \n",
    "        if hasattr(predictor, 'transformer_layers') and len(predictor.transformer_layers) > 0:\n",
    "            first_layer = predictor.transformer_layers[0]\n",
    "            \n",
    "            # Self-attention weights (in_proj_weight contains Q, K, V)\n",
    "            if hasattr(first_layer, 'self_attn') and hasattr(first_layer.self_attn, 'in_proj_weight'):\n",
    "                first_layer_self_attn_weight = first_layer.self_attn.in_proj_weight.detach().cpu()\n",
    "            \n",
    "            # MLP weights\n",
    "            if hasattr(first_layer, 'linear1') and hasattr(first_layer.linear1, 'weight'):\n",
    "                first_layer_linear1_weight = first_layer.linear1.weight.detach().cpu()\n",
    "            if hasattr(first_layer, 'linear2') and hasattr(first_layer.linear2, 'weight'):\n",
    "                first_layer_linear2_weight = first_layer.linear2.weight.detach().cpu()\n",
    "        \n",
    "        # Output head weights\n",
    "        output_head_weight = predictor.output_head.weight.detach().cpu() if hasattr(predictor, 'output_head') and hasattr(predictor.output_head, 'weight') else None\n",
    "        \n",
    "        # Create visualization\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "        \n",
    "        # Action embedding weights\n",
    "        if action_embed_weight is not None:\n",
    "            axes[0, 0].hist(action_embed_weight.flatten().numpy(), bins=50, alpha=0.7, color='blue')\n",
    "            axes[0, 0].set_title(f\"Action Embed Weight Distribution\\nMean: {action_embed_weight.mean():.6f}, Std: {action_embed_weight.std():.6f}\")\n",
    "            axes[0, 0].set_xlabel(\"Weight Value\")\n",
    "            axes[0, 0].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[0, 0].text(0.5, 0.5, \"Action embedding\\nweights not found\", ha='center', va='center', transform=axes[0, 0].transAxes)\n",
    "            axes[0, 0].set_title(\"Action Embedding Weights\")\n",
    "        \n",
    "        # Position embedding weights\n",
    "        if pos_embed_weight is not None:\n",
    "            axes[0, 1].hist(pos_embed_weight.flatten().numpy(), bins=50, alpha=0.7, color='green')\n",
    "            axes[0, 1].set_title(f\"Position Embed Weight Distribution\\nMean: {pos_embed_weight.mean():.6f}, Std: {pos_embed_weight.std():.6f}\")\n",
    "            axes[0, 1].set_xlabel(\"Weight Value\")\n",
    "            axes[0, 1].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[0, 1].text(0.5, 0.5, \"Position embedding\\nweights not found\", ha='center', va='center', transform=axes[0, 1].transAxes)\n",
    "            axes[0, 1].set_title(\"Position Embedding Weights\")\n",
    "        \n",
    "        # First transformer layer self-attention weights\n",
    "        if first_layer_self_attn_weight is not None:\n",
    "            axes[0, 2].hist(first_layer_self_attn_weight.flatten().numpy(), bins=50, alpha=0.7, color='red')\n",
    "            axes[0, 2].set_title(f\"First Layer Self-Attn Weights\\nMean: {first_layer_self_attn_weight.mean():.6f}, Std: {first_layer_self_attn_weight.std():.6f}\")\n",
    "            axes[0, 2].set_xlabel(\"Weight Value\")\n",
    "            axes[0, 2].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[0, 2].text(0.5, 0.5, \"Self-attention\\nweights not found\", ha='center', va='center', transform=axes[0, 2].transAxes)\n",
    "            axes[0, 2].set_title(\"First Layer Self-Attn\")\n",
    "        \n",
    "        # First transformer layer linear1 (MLP) weights\n",
    "        if first_layer_linear1_weight is not None:\n",
    "            axes[1, 0].hist(first_layer_linear1_weight.flatten().numpy(), bins=50, alpha=0.7, color='purple')\n",
    "            axes[1, 0].set_title(f\"First Layer Linear1 Weights\\nMean: {first_layer_linear1_weight.mean():.6f}, Std: {first_layer_linear1_weight.std():.6f}\")\n",
    "            axes[1, 0].set_xlabel(\"Weight Value\")\n",
    "            axes[1, 0].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[1, 0].text(0.5, 0.5, \"Linear1 weights\\nnot found\", ha='center', va='center', transform=axes[1, 0].transAxes)\n",
    "            axes[1, 0].set_title(\"First Layer Linear1\")\n",
    "        \n",
    "        # Output head weights\n",
    "        if output_head_weight is not None:\n",
    "            axes[1, 1].hist(output_head_weight.flatten().numpy(), bins=50, alpha=0.7, color='orange')\n",
    "            axes[1, 1].set_title(f\"Output Head Weight Distribution\\nMean: {output_head_weight.mean():.6f}, Std: {output_head_weight.std():.6f}\")\n",
    "            axes[1, 1].set_xlabel(\"Weight Value\")\n",
    "            axes[1, 1].set_ylabel(\"Count\")\n",
    "        else:\n",
    "            axes[1, 1].text(0.5, 0.5, \"Output head\\nweights not found\", ha='center', va='center', transform=axes[1, 1].transAxes)\n",
    "            axes[1, 1].set_title(\"Output Head Weights\")\n",
    "        \n",
    "        # Weight norms across all layers\n",
    "        layer_norms = []\n",
    "        layer_names = []\n",
    "        for name, param in predictor.named_parameters():\n",
    "            if 'weight' in name and param.dim() >= 2:\n",
    "                layer_norms.append(param.norm().item())\n",
    "                # Shorten layer names for display\n",
    "                short_name = name.split('.')[-1] if '.' in name else name\n",
    "                if len(short_name) > 15:\n",
    "                    short_name = short_name[:12] + \"...\"\n",
    "                layer_names.append(short_name)\n",
    "        \n",
    "        if layer_norms:\n",
    "            axes[1, 2].bar(range(len(layer_norms)), layer_norms)\n",
    "            axes[1, 2].set_title(\"Layer Weight Norms\")\n",
    "            axes[1, 2].set_xlabel(\"Layer\")\n",
    "            axes[1, 2].set_ylabel(\"L2 Norm\")\n",
    "            # Show every 5th label to avoid overcrowding\n",
    "            step = max(1, len(layer_names)//8)\n",
    "            axes[1, 2].set_xticks(range(0, len(layer_names), step))\n",
    "            axes[1, 2].set_xticklabels([layer_names[i] for i in range(0, len(layer_names), step)], rotation=45, ha='right')\n",
    "        else:\n",
    "            axes[1, 2].text(0.5, 0.5, \"No weight layers\\nfound\", ha='center', va='center', transform=axes[1, 2].transAxes)\n",
    "            axes[1, 2].set_title(\"Layer Weight Norms\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Return weight statistics for comparison\n",
    "        stats = {}\n",
    "        if action_embed_weight is not None:\n",
    "            stats.update({\n",
    "                'action_embed_mean': action_embed_weight.mean().item(),\n",
    "                'action_embed_std': action_embed_weight.std().item(),\n",
    "            })\n",
    "        if pos_embed_weight is not None:\n",
    "            stats.update({\n",
    "                'pos_embed_mean': pos_embed_weight.mean().item(),\n",
    "                'pos_embed_std': pos_embed_weight.std().item(),\n",
    "            })\n",
    "        if first_layer_self_attn_weight is not None:\n",
    "            stats.update({\n",
    "                'first_self_attn_mean': first_layer_self_attn_weight.mean().item(),\n",
    "                'first_self_attn_std': first_layer_self_attn_weight.std().item(),\n",
    "            })\n",
    "        if first_layer_linear1_weight is not None:\n",
    "            stats.update({\n",
    "                'first_linear1_mean': first_layer_linear1_weight.mean().item(),\n",
    "                'first_linear1_std': first_layer_linear1_weight.std().item(),\n",
    "            })\n",
    "        if output_head_weight is not None:\n",
    "            stats.update({\n",
    "                'output_head_mean': output_head_weight.mean().item(),\n",
    "                'output_head_std': output_head_weight.std().item(),\n",
    "            })\n",
    "        if layer_norms:\n",
    "            stats['layer_norms'] = dict(zip(layer_names, layer_norms))\n",
    "        \n",
    "        return stats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ixrc3gkvood",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training helper functions using AdaptiveWorldModel\n",
    "def train_autoencoder_step_wrapper(frame_tensor):\n",
    "    \"\"\"Single autoencoder training step using AdaptiveWorldModel\"\"\"\n",
    "    # Convert tensor to numpy frame for AdaptiveWorldModel\n",
    "    frame_numpy = tensor_to_numpy_image(frame_tensor)\n",
    "    \n",
    "    # Use AdaptiveWorldModel's train_autoencoder method\n",
    "    loss = adaptive_world_model.train_autoencoder(frame_numpy)\n",
    "    return loss\n",
    "\n",
    "def train_predictor_step_wrapper(target_idx, history_features, history_actions):\n",
    "    \"\"\"Single predictor training step using AdaptiveWorldModel\"\"\"\n",
    "    # Get target frame\n",
    "    next_obs = session_state[\"observations\"][target_idx + 1]\n",
    "    target_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "    target_frame = tensor_to_numpy_image(target_tensor)\n",
    "    \n",
    "    # Set up prediction context in AdaptiveWorldModel\n",
    "    adaptive_world_model.observation_history = []\n",
    "    adaptive_world_model.action_history = []\n",
    "    \n",
    "    # Add history to the world model\n",
    "    for i, (feat, action) in enumerate(zip(history_features, history_actions)):\n",
    "        # Convert feature back to frame if needed\n",
    "        obs = session_state[\"observations\"][target_idx - len(history_features) + 1 + i]\n",
    "        frame_tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0)\n",
    "        frame_numpy = tensor_to_numpy_image(frame_tensor)\n",
    "        \n",
    "        adaptive_world_model.observation_history.append(frame_numpy)\n",
    "        adaptive_world_model.action_history.append(action)\n",
    "    \n",
    "    # Train predictor level 0\n",
    "    loss = adaptive_world_model.train_predictor(0, target_tensor)\n",
    "    return loss\n",
    "\n",
    "def format_loss(loss_value):\n",
    "    \"\"\"Format loss value for display\"\"\"\n",
    "    if loss_value < 0.001:\n",
    "        return f\"{loss_value:.2e}\"\n",
    "    else:\n",
    "        return f\"{loss_value:.6f}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ob17o8vzup",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder Training Section using AdaptiveWorldModel\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "training_widgets = {}\n",
    "AUTOENCODER_TASK_KEY = \"autoencoder\"\n",
    "\n",
    "\n",
    "def _prepare_autoencoder_training():\n",
    "    \"\"\"Validate prerequisites and build context for autoencoder training.\"\"\"\n",
    "    output = training_widgets[\"autoencoder_training_output\"]\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    if autoencoder is None:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "        set_training_status(\"autoencoder\", \"error\", \"Autoencoder checkpoint is not loaded.\")\n",
    "        update_training_loss(\"autoencoder\", None)\n",
    "        return None\n",
    "\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        set_training_status(\"autoencoder\", \"error\", \"Load a session to select frames before training.\")\n",
    "        update_training_loss(\"autoencoder\", None)\n",
    "        return None\n",
    "\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    if not observations:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"No session loaded. Load a recording before training.\"))\n",
    "        set_training_status(\"autoencoder\", \"error\", \"No session loaded.\")\n",
    "        update_training_loss(\"autoencoder\", None)\n",
    "        return None\n",
    "\n",
    "    idx = frame_slider.value\n",
    "    observation = observations[idx]\n",
    "    frame_tensor = get_frame_tensor(\n",
    "        session_state[\"session_dir\"],\n",
    "        observation[\"frame_path\"],\n",
    "    ).unsqueeze(0).to(device)\n",
    "\n",
    "    return {\n",
    "        \"autoencoder\": autoencoder,\n",
    "        \"frame_tensor\": frame_tensor,\n",
    "        \"idx\": idx,\n",
    "        \"observation\": observation,\n",
    "        \"output\": output,\n",
    "    }\n",
    "\n",
    "\n",
    "async def autoencoder_threshold_training(context, threshold, max_steps):\n",
    "    autoencoder = context[\"autoencoder\"]\n",
    "    frame_tensor = context[\"frame_tensor\"]\n",
    "    idx = context[\"idx\"]\n",
    "    observation = context[\"observation\"]\n",
    "    output = context[\"output\"]\n",
    "\n",
    "    training_control[\"autoencoder_resume_data\"] = None\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    set_training_status(\"autoencoder\", \"running\", f\"Running to threshold {format_loss(threshold)}\")\n",
    "    update_training_loss(\"autoencoder\", None)\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        display(Markdown(\n",
    "            f\"**Training autoencoder using AdaptiveWorldModel on frame {idx + 1} (step {observation['step']})**\"\n",
    "        ))\n",
    "        display(Markdown(f\"Target threshold: {format_loss(threshold)}, Max steps: {max_steps}\"))\n",
    "        display(Markdown(\"### Pre-Training Network Weights\"))\n",
    "        pre_stats = visualize_autoencoder_weights(autoencoder)\n",
    "\n",
    "        display(Markdown(\"**Using AdaptiveWorldModel.train_autoencoder() method with randomized masking**\"))\n",
    "        display(Markdown(\"**Tip: Click 'Pause' button to interrupt training at any time.**\"))\n",
    "\n",
    "        progress = tqdm(range(max_steps), desc=\"Training\")\n",
    "        try:\n",
    "            for step in progress:\n",
    "                await asyncio.sleep(0)\n",
    "                if training_control[\"autoencoder_paused\"]:\n",
    "                    training_control[\"autoencoder_resume_data\"] = {\n",
    "                        \"frame_tensor\": frame_tensor,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"max_steps\": max_steps,\n",
    "                        \"current_step\": step,\n",
    "                        \"losses\": losses,\n",
    "                        \"start_time\": start_time,\n",
    "                        \"pre_stats\": pre_stats,\n",
    "                    }\n",
    "                    set_training_status(\"autoencoder\", \"paused\", f\"Paused at step {step} of {max_steps}. Resume available.\")\n",
    "                    if losses:\n",
    "                        update_training_loss(\"autoencoder\", losses[-1], len(losses), state=\"paused\")\n",
    "                    else:\n",
    "                        update_training_loss(\"autoencoder\", None)\n",
    "                    display(Markdown(\"**Training paused. Use Resume button to continue.**\"))\n",
    "                    return\n",
    "\n",
    "                loss = train_autoencoder_step_wrapper(frame_tensor)\n",
    "                losses.append(loss)\n",
    "                update_training_loss(\"autoencoder\", loss, len(losses))\n",
    "                progress.set_postfix({\"Loss\": format_loss(loss), \"Step\": f\"{step + 1}/{max_steps}\"})\n",
    "\n",
    "                if loss <= threshold:\n",
    "                    break\n",
    "        finally:\n",
    "            progress.close()\n",
    "\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float(\"inf\")\n",
    "\n",
    "        display(Markdown(f\"**Training completed after {len(losses)} steps in {end_time - start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Final loss: {format_loss(final_loss)}\"))\n",
    "\n",
    "        if final_loss <= threshold:\n",
    "            display(Markdown(f\"**Target threshold {format_loss(threshold)} achieved!**\"))\n",
    "        else:\n",
    "            display(Markdown(f\"**Target threshold {format_loss(threshold)} not reached after {max_steps} steps.**\"))\n",
    "\n",
    "        display(Markdown(\"### Post-Training Network Weights\"))\n",
    "        post_stats = visualize_autoencoder_weights(autoencoder)\n",
    "\n",
    "        if pre_stats and post_stats:\n",
    "            weight_changes = {\n",
    "                \"patch_embed_mean_change\": abs(post_stats[\"patch_embed_mean\"] - pre_stats[\"patch_embed_mean\"]),\n",
    "                \"patch_embed_std_change\": abs(post_stats[\"patch_embed_std\"] - pre_stats[\"patch_embed_std\"]),\n",
    "                \"cls_token_mean_change\": abs(post_stats[\"cls_token_mean\"] - pre_stats[\"cls_token_mean\"]),\n",
    "                \"cls_token_std_change\": abs(post_stats[\"cls_token_std\"] - pre_stats[\"cls_token_std\"]),\n",
    "            }\n",
    "            changes_text = f\"\"\"\n",
    "**Weight Changes:**\n",
    "- Patch Embed Mean: {weight_changes['patch_embed_mean_change']:.8f}\n",
    "- Patch Embed Std: {weight_changes['patch_embed_std_change']:.8f}\n",
    "- CLS Token Mean: {weight_changes['cls_token_mean_change']:.8f}\n",
    "- CLS Token Std: {weight_changes['cls_token_std_change']:.8f}\n",
    "            \"\"\"\n",
    "            display(Markdown(changes_text))\n",
    "\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Reconstruction Loss\")\n",
    "            ax.set_title(\"Autoencoder Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.axhline(y=threshold, color=\"r\", linestyle=\"--\", alpha=0.7, label=f\"Target: {format_loss(threshold)}\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        adaptive_world_model.autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = adaptive_world_model.autoencoder.reconstruct(frame_tensor)\n",
    "\n",
    "        original_img = tensor_to_numpy_image(frame_tensor)\n",
    "        reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"After Training (Loss: {format_loss(final_loss)})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if losses:\n",
    "        update_training_loss(\"autoencoder\", final_loss, len(losses), state=\"completed\")\n",
    "        set_training_status(\"autoencoder\", \"completed\", f\"Completed in {len(losses)} steps (final loss {format_loss(final_loss)}).\")\n",
    "    else:\n",
    "        update_training_loss(\"autoencoder\", None)\n",
    "        set_training_status(\"autoencoder\", \"completed\", \"Completed without updating any steps.\")\n",
    "\n",
    "    training_control[\"autoencoder_resume_data\"] = None\n",
    "\n",
    "\n",
    "async def autoencoder_steps_training(context, num_steps):\n",
    "    autoencoder = context[\"autoencoder\"]\n",
    "    frame_tensor = context[\"frame_tensor\"]\n",
    "    idx = context[\"idx\"]\n",
    "    observation = context[\"observation\"]\n",
    "    output = context[\"output\"]\n",
    "\n",
    "    training_control[\"autoencoder_resume_data\"] = None\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    set_training_status(\"autoencoder\", \"running\", f\"Running for {num_steps} steps\")\n",
    "    update_training_loss(\"autoencoder\", None)\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        display(Markdown(\n",
    "            f\"**Training autoencoder using AdaptiveWorldModel on frame {idx + 1} (step {observation['step']}) for {num_steps} steps**\"\n",
    "        ))\n",
    "        display(Markdown(\"### Pre-Training Network Weights\"))\n",
    "        pre_stats = visualize_autoencoder_weights(autoencoder)\n",
    "\n",
    "        display(Markdown(\"**Using AdaptiveWorldModel.train_autoencoder() method with randomized masking**\"))\n",
    "        display(Markdown(\"**Tip: Click 'Pause' button to interrupt training at any time.**\"))\n",
    "\n",
    "        progress = tqdm(range(num_steps), desc=\"Training\")\n",
    "        try:\n",
    "            for step in progress:\n",
    "                await asyncio.sleep(0)\n",
    "                if training_control[\"autoencoder_paused\"]:\n",
    "                    set_training_status(\"autoencoder\", \"paused\", f\"Paused after {step} of {num_steps} steps; restart to continue.\")\n",
    "                    if losses:\n",
    "                        update_training_loss(\"autoencoder\", losses[-1], len(losses), state=\"paused\")\n",
    "                    else:\n",
    "                        update_training_loss(\"autoencoder\", None)\n",
    "                    display(Markdown(\"**Training paused. Step-based training cannot be resumed.**\"))\n",
    "                    display(Markdown(f\"**Completed {step} out of {num_steps} steps before pausing.**\"))\n",
    "                    return\n",
    "\n",
    "                loss = train_autoencoder_step_wrapper(frame_tensor)\n",
    "                losses.append(loss)\n",
    "                progress.set_postfix({\"Loss\": format_loss(loss), \"Step\": f\"{step + 1}/{num_steps}\"})\n",
    "        finally:\n",
    "            progress.close()\n",
    "\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float(\"inf\")\n",
    "\n",
    "        display(Markdown(f\"**Training completed in {end_time - start_time:.1f}s**\"))\n",
    "        if losses:\n",
    "            display(Markdown(f\"Initial loss: {format_loss(losses[0])}, Final loss: {format_loss(final_loss)}\"))\n",
    "        else:\n",
    "            display(Markdown(\"** **No training steps were executed before the run stopped.**\"))\n",
    "\n",
    "        display(Markdown(\"### Post-Training Network Weights\"))\n",
    "        post_stats = visualize_autoencoder_weights(autoencoder)\n",
    "\n",
    "        if pre_stats and post_stats:\n",
    "            weight_changes = {\n",
    "                \"patch_embed_mean_change\": abs(post_stats[\"patch_embed_mean\"] - pre_stats[\"patch_embed_mean\"]),\n",
    "                \"patch_embed_std_change\": abs(post_stats[\"patch_embed_std\"] - pre_stats[\"patch_embed_std\"]),\n",
    "                \"cls_token_mean_change\": abs(post_stats[\"cls_token_mean\"] - pre_stats[\"cls_token_mean\"]),\n",
    "                \"cls_token_std_change\": abs(post_stats[\"cls_token_std\"] - pre_stats[\"cls_token_std\"]),\n",
    "            }\n",
    "            changes_text = f\"\"\"\n",
    "**Weight Changes:**\n",
    "- Patch Embed Mean: {weight_changes['patch_embed_mean_change']:.8f}\n",
    "- Patch Embed Std: {weight_changes['patch_embed_std_change']:.8f}\n",
    "- CLS Token Mean: {weight_changes['cls_token_mean_change']:.8f}\n",
    "- CLS Token Std: {weight_changes['cls_token_std_change']:.8f}\n",
    "            \"\"\"\n",
    "            display(Markdown(changes_text))\n",
    "\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Reconstruction Loss\")\n",
    "            ax.set_title(\"Autoencoder Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        adaptive_world_model.autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            reconstructed = adaptive_world_model.autoencoder.reconstruct(frame_tensor)\n",
    "\n",
    "        original_img = tensor_to_numpy_image(frame_tensor)\n",
    "        reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Original\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"After Training (Loss: {format_loss(final_loss)})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if losses:\n",
    "        update_training_loss(\"autoencoder\", final_loss, len(losses), state=\"completed\")\n",
    "        set_training_status(\"autoencoder\", \"completed\", f\"Finished {len(losses)} steps (final loss {format_loss(final_loss)}).\")\n",
    "    else:\n",
    "        update_training_loss(\"autoencoder\", None)\n",
    "        set_training_status(\"autoencoder\", \"completed\", \"Completed without updating any steps.\")\n",
    "\n",
    "    training_control[\"autoencoder_resume_data\"] = None\n",
    "\n",
    "\n",
    "async def autoencoder_resume_training(resume_data):\n",
    "    output = training_widgets[\"autoencoder_training_output\"]\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    if autoencoder is None:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"**Autoencoder checkpoint not loaded. Load it before resuming.**\"))\n",
    "        training_control[\"autoencoder_resume_data\"] = None\n",
    "        update_training_loss(\"autoencoder\", None)\n",
    "        return\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        display(Markdown(\"**Resuming autoencoder training...**\"))\n",
    "\n",
    "        frame_tensor = resume_data[\"frame_tensor\"]\n",
    "        threshold = resume_data[\"threshold\"]\n",
    "        max_steps = resume_data[\"max_steps\"]\n",
    "        current_step = resume_data[\"current_step\"]\n",
    "        losses = resume_data[\"losses\"]\n",
    "        start_time = resume_data[\"start_time\"]\n",
    "        pre_stats = resume_data[\"pre_stats\"]\n",
    "\n",
    "        if losses:\n",
    "            update_training_loss(\"autoencoder\", losses[-1], len(losses))\n",
    "        else:\n",
    "            update_training_loss(\"autoencoder\", None)\n",
    "\n",
    "        set_training_status(\"autoencoder\", \"running\", f\"Resuming from step {current_step} of {max_steps}.\")\n",
    "\n",
    "        remaining_steps = max_steps - current_step\n",
    "        if remaining_steps <= 0:\n",
    "            display(Markdown(\"**Training already reached the requested number of steps.**\"))\n",
    "            if losses:\n",
    "                update_training_loss(\"autoencoder\", losses[-1], len(losses), state=\"completed\")\n",
    "            else:\n",
    "                update_training_loss(\"autoencoder\", None)\n",
    "            set_training_status(\"autoencoder\", \"completed\", \"Requested number of steps already reached before resuming.\")\n",
    "            training_control[\"autoencoder_resume_data\"] = None\n",
    "            return\n",
    "\n",
    "        progress = tqdm(range(remaining_steps), desc=f\"Resuming from step {current_step}\")\n",
    "        try:\n",
    "            for step_offset in progress:\n",
    "                await asyncio.sleep(0)\n",
    "                if training_control[\"autoencoder_paused\"]:\n",
    "                    resume_data.update({\n",
    "                        \"current_step\": current_step + step_offset,\n",
    "                        \"losses\": losses,\n",
    "                    })\n",
    "                    set_training_status(\"autoencoder\", \"paused\", f\"Paused at step {current_step + step_offset} of {max_steps}. Resume available.\")\n",
    "                    if losses:\n",
    "                        update_training_loss(\"autoencoder\", losses[-1], len(losses), state=\"paused\")\n",
    "                    else:\n",
    "                        update_training_loss(\"autoencoder\", None)\n",
    "                    display(Markdown(\"**Training paused. Use Resume button to continue.**\"))\n",
    "                    return\n",
    "\n",
    "                loss = train_autoencoder_step_wrapper(frame_tensor)\n",
    "                losses.append(loss)\n",
    "                update_training_loss(\"autoencoder\", loss, len(losses))\n",
    "                progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "\n",
    "                if loss <= threshold:\n",
    "                    break\n",
    "        finally:\n",
    "            progress.close()\n",
    "\n",
    "        training_control[\"autoencoder_resume_data\"] = None\n",
    "\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float(\"inf\")\n",
    "\n",
    "        display(Markdown(f\"**Training completed after {len(losses)} total steps in {end_time - start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Final loss: {format_loss(final_loss)}\"))\n",
    "\n",
    "        if final_loss <= threshold:\n",
    "            display(Markdown(f\"**Target threshold {format_loss(threshold)} achieved!**\"))\n",
    "        else:\n",
    "            display(Markdown(f\"**Target threshold {format_loss(threshold)} not reached after {max_steps} steps.**\"))\n",
    "\n",
    "        display(Markdown(\"### Post-Training Network Weights\"))\n",
    "        post_stats = visualize_autoencoder_weights(adaptive_world_model.autoencoder)\n",
    "\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Reconstruction Loss\")\n",
    "            ax.set_title(\"Autoencoder Training Progress (Resumed)\")\n",
    "            ax.axhline(y=threshold, color=\"r\", linestyle=\"--\", alpha=0.7, label=f\"Target: {format_loss(threshold)}\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if losses:\n",
    "            update_training_loss(\"autoencoder\", final_loss, len(losses), state=\"completed\")\n",
    "            set_training_status(\"autoencoder\", \"completed\", f\"Completed with final loss {format_loss(final_loss)} after {len(losses)} steps.\")\n",
    "        else:\n",
    "            update_training_loss(\"autoencoder\", None)\n",
    "            set_training_status(\"autoencoder\", \"completed\", \"Completed without updating any steps.\")\n",
    "\n",
    "    # Resume function does not render reconstructions to keep output concise.\n",
    "\n",
    "\n",
    "def on_train_autoencoder_threshold(_):\n",
    "    context = _prepare_autoencoder_training()\n",
    "    if context is None:\n",
    "        return\n",
    "\n",
    "    training_control[\"autoencoder_paused\"] = False\n",
    "    training_control[\"autoencoder_resume_data\"] = None\n",
    "\n",
    "    threshold = training_widgets[\"autoencoder_threshold\"].value\n",
    "    max_steps = training_widgets[\"autoencoder_max_steps\"].value\n",
    "\n",
    "    set_training_status(\"autoencoder\", \"running\", f\"Running to threshold {format_loss(threshold)}\")\n",
    "\n",
    "    start_training_task(\n",
    "        AUTOENCODER_TASK_KEY,\n",
    "        autoencoder_threshold_training(context, threshold, max_steps),\n",
    "        context[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def on_train_autoencoder_steps(_):\n",
    "    context = _prepare_autoencoder_training()\n",
    "    if context is None:\n",
    "        return\n",
    "\n",
    "    training_control[\"autoencoder_paused\"] = False\n",
    "    training_control[\"autoencoder_resume_data\"] = None\n",
    "\n",
    "    num_steps = training_widgets[\"autoencoder_steps\"].value\n",
    "\n",
    "    set_training_status(\"autoencoder\", \"running\", f\"Running for {num_steps} steps\")\n",
    "\n",
    "    start_training_task(\n",
    "        AUTOENCODER_TASK_KEY,\n",
    "        autoencoder_steps_training(context, num_steps),\n",
    "        context[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Create autoencoder training widgets\n",
    "autoencoder_threshold = widgets.FloatText(value=0.0005, description=\"Threshold\", step=0.0001, style={'description_width': '100px'})\n",
    "autoencoder_max_steps = widgets.IntText(value=1000, description=\"Max Steps\", style={'description_width': '100px'})\n",
    "autoencoder_steps = widgets.IntText(value=100, description=\"Steps\", style={'description_width': '100px'})\n",
    "\n",
    "train_autoencoder_threshold_button = widgets.Button(description=\"Train to Threshold\", button_style=\"warning\", icon=\"target\")\n",
    "train_autoencoder_steps_button = widgets.Button(description=\"Train N Steps\", button_style=\"warning\", icon=\"forward\")\n",
    "autoencoder_training_output = widgets.Output()\n",
    "\n",
    "training_widgets.update({\n",
    "    \"autoencoder_threshold\": autoencoder_threshold,\n",
    "    \"autoencoder_max_steps\": autoencoder_max_steps,\n",
    "    \"autoencoder_steps\": autoencoder_steps,\n",
    "    \"autoencoder_training_output\": autoencoder_training_output,\n",
    "})\n",
    "\n",
    "train_autoencoder_threshold_button.on_click(on_train_autoencoder_threshold)\n",
    "train_autoencoder_steps_button.on_click(on_train_autoencoder_steps)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ixuxiwf2um",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training control state for pause/resume functionality\n",
    "import asyncio\n",
    "\n",
    "AUTOENCODER_TASK_KEY = globals().get(\"AUTOENCODER_TASK_KEY\", \"autoencoder\")\n",
    "PREDICTOR_TASK_KEY = globals().get(\"PREDICTOR_TASK_KEY\", \"predictor\")\n",
    "\n",
    "training_control = {\n",
    "    \"autoencoder_paused\": False,\n",
    "    \"predictor_paused\": False,\n",
    "    \"autoencoder_resume_data\": None,\n",
    "    \"predictor_resume_data\": None,\n",
    "}\n",
    "\n",
    "training_tasks = {\n",
    "    AUTOENCODER_TASK_KEY: None,\n",
    "    PREDICTOR_TASK_KEY: None,\n",
    "}\n",
    "\n",
    "\n",
    "STATUS_STYLES = {\n",
    "    \"idle\": \"color: #6c757d;\",\n",
    "    \"running\": \"color: #2e7d32;\",\n",
    "    \"pausing\": \"color: #f9a825;\",\n",
    "    \"paused\": \"color: #ef6c00;\",\n",
    "    \"completed\": \"color: #1565c0;\",\n",
    "    \"error\": \"color: #c62828;\",\n",
    "}\n",
    "\n",
    "def _status_html(state: str, message: str) -> str:\n",
    "    style = STATUS_STYLES.get(state, STATUS_STYLES[\"idle\"])\n",
    "    return f\"<b>Status:</b> <span style='{style}'>{message}</span>\"\n",
    "\n",
    "def set_training_status(kind: str, state: str, message: str) -> None:\n",
    "    widget = training_widgets.get(f\"{kind}_status\")\n",
    "    if widget is not None:\n",
    "        widget.value = _status_html(state, message)\n",
    "\n",
    "LOSS_COLORS = {\n",
    "    \"running\": \"#2e7d32\",\n",
    "    \"paused\": \"#ef6c00\",\n",
    "    \"completed\": \"#1565c0\",\n",
    "    \"error\": \"#c62828\",\n",
    "}\n",
    "\n",
    "LOSS_DEFAULT = \"<b>Loss:</b> <span style='color: #6c757d;'>--</span>\"\n",
    "\n",
    "def update_training_loss(kind: str, loss=None, step=None, state=\"running\") -> None:\n",
    "    widget = training_widgets.get(f\"{kind}_loss\")\n",
    "    if widget is None:\n",
    "        return\n",
    "    if loss is None:\n",
    "        widget.value = LOSS_DEFAULT\n",
    "        return\n",
    "    color = LOSS_COLORS.get(state, LOSS_COLORS[\"running\"])\n",
    "    step_text = f\" (step {step})\" if step is not None else \"\"\n",
    "    widget.value = f\"<b>Loss:</b> <span style='color: {color};'>{format_loss(loss)}</span>{step_text}\"\n",
    "\n",
    "\n",
    "TASK_NAME_TO_KIND = {\n",
    "    AUTOENCODER_TASK_KEY: \"autoencoder\",\n",
    "    PREDICTOR_TASK_KEY: \"predictor\",\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "def _get_event_loop():\n",
    "    try:\n",
    "        return asyncio.get_running_loop()\n",
    "    except RuntimeError:\n",
    "        return asyncio.get_event_loop()\n",
    "\n",
    "\n",
    "def start_training_task(task_name, coroutine, output_widget=None):\n",
    "    existing = training_tasks.get(task_name)\n",
    "    if existing and not existing.done():\n",
    "        if output_widget is not None:\n",
    "            with output_widget:\n",
    "                display(Markdown(\"**Training already in progress. Pause it before starting a new run.**\"))\n",
    "        return None\n",
    "\n",
    "    loop = _get_event_loop()\n",
    "    task = loop.create_task(coroutine)\n",
    "    training_tasks[task_name] = task\n",
    "\n",
    "    def _cleanup(future):\n",
    "        training_tasks[task_name] = None\n",
    "        if output_widget is None:\n",
    "            return\n",
    "        if future.cancelled():\n",
    "            return\n",
    "        exc = future.exception()\n",
    "        if exc:\n",
    "            with output_widget:\n",
    "                display(Markdown(f\"**Training error:** {exc}\"))\n",
    "            kind = TASK_NAME_TO_KIND.get(task_name)\n",
    "            if kind:\n",
    "                set_training_status(kind, \"error\", f\"Error: {exc}\")\n",
    "                update_training_loss(kind, None, state=\"error\")\n",
    "\n",
    "    task.add_done_callback(_cleanup)\n",
    "    return task\n",
    "\n",
    "\n",
    "def on_pause_autoencoder(_):\n",
    "    output = training_widgets[\"autoencoder_training_output\"]\n",
    "    task = training_tasks.get(AUTOENCODER_TASK_KEY)\n",
    "    if task is None or task.done():\n",
    "        with output:\n",
    "            display(Markdown(\"**No autoencoder training is currently running.**\"))\n",
    "        set_training_status(\"autoencoder\", \"idle\", \"Idle\")\n",
    "        return\n",
    "\n",
    "    training_control[\"autoencoder_paused\"] = True\n",
    "    set_training_status(\"autoencoder\", \"pausing\", \"Pause requested. Waiting for current step to finish...\")\n",
    "    with output:\n",
    "        display(Markdown(\"**Pause requested. Waiting for current step to finish...**\"))\n",
    "\n",
    "\n",
    "def on_resume_autoencoder(_):\n",
    "    output = training_widgets[\"autoencoder_training_output\"]\n",
    "    resume_data = training_control.get(\"autoencoder_resume_data\")\n",
    "    if resume_data is None:\n",
    "        with output:\n",
    "            display(Markdown(\"**No paused training to resume.**\"))\n",
    "        set_training_status(\"autoencoder\", \"idle\", \"Idle\")\n",
    "        return\n",
    "\n",
    "    task = training_tasks.get(AUTOENCODER_TASK_KEY)\n",
    "    if task and not task.done():\n",
    "        with output:\n",
    "            display(Markdown(\"**Autoencoder training already running. Pause it before resuming.**\"))\n",
    "        return\n",
    "\n",
    "    training_control[\"autoencoder_paused\"] = False\n",
    "    set_training_status(\"autoencoder\", \"running\", \"Resuming training...\")\n",
    "    start_training_task(\n",
    "        AUTOENCODER_TASK_KEY,\n",
    "        autoencoder_resume_training(resume_data),\n",
    "        output,\n",
    "    )\n",
    "\n",
    "\n",
    "def on_pause_predictor(_):\n",
    "    output = training_widgets[\"predictor_training_output\"]\n",
    "    task = training_tasks.get(PREDICTOR_TASK_KEY)\n",
    "    if task is None or task.done():\n",
    "        with output:\n",
    "            display(Markdown(\"**No predictor training is currently running.**\"))\n",
    "        set_training_status(\"predictor\", \"idle\", \"Idle\")\n",
    "        return\n",
    "\n",
    "    training_control[\"predictor_paused\"] = True\n",
    "    set_training_status(\"predictor\", \"pausing\", \"Pause requested. Waiting for current step to finish...\")\n",
    "    with output:\n",
    "        display(Markdown(\"**Pause requested. Waiting for current step to finish...**\"))\n",
    "\n",
    "\n",
    "def on_resume_predictor(_):\n",
    "    output = training_widgets[\"predictor_training_output\"]\n",
    "    resume_data = training_control.get(\"predictor_resume_data\")\n",
    "    if resume_data is None:\n",
    "        with output:\n",
    "            display(Markdown(\"**No paused training to resume.**\"))\n",
    "        set_training_status(\"predictor\", \"idle\", \"Idle\")\n",
    "        return\n",
    "\n",
    "    task = training_tasks.get(PREDICTOR_TASK_KEY)\n",
    "    if task and not task.done():\n",
    "        with output:\n",
    "            display(Markdown(\"**Predictor training already running. Pause it before resuming.**\"))\n",
    "        return\n",
    "\n",
    "    training_control[\"predictor_paused\"] = False\n",
    "    set_training_status(\"predictor\", \"running\", \"Resuming training...\")\n",
    "    start_training_task(\n",
    "        PREDICTOR_TASK_KEY,\n",
    "        predictor_resume_training(resume_data),\n",
    "        output,\n",
    "    )\n",
    "\n",
    "\n",
    "# Create pause/resume buttons\n",
    "pause_autoencoder_button = widgets.Button(description=\"Pause\", button_style=\"warning\", icon=\"pause\")\n",
    "resume_autoencoder_button = widgets.Button(description=\"Resume\", button_style=\"info\", icon=\"play\")\n",
    "pause_predictor_button = widgets.Button(description=\"Pause\", button_style=\"warning\", icon=\"pause\")\n",
    "resume_predictor_button = widgets.Button(description=\"Resume\", button_style=\"info\", icon=\"play\")\n",
    "\n",
    "# Connect pause/resume handlers\n",
    "pause_autoencoder_button.on_click(on_pause_autoencoder)\n",
    "resume_autoencoder_button.on_click(on_resume_autoencoder)\n",
    "pause_predictor_button.on_click(on_pause_predictor)\n",
    "resume_predictor_button.on_click(on_resume_predictor)\n",
    "\n",
    "autoencoder_status = widgets.HTML(value=_status_html(\"idle\", \"Idle\"))\n",
    "autoencoder_loss = widgets.HTML(value=LOSS_DEFAULT)\n",
    "predictor_status = widgets.HTML(value=_status_html(\"idle\", \"Idle\"))\n",
    "predictor_loss = widgets.HTML(value=LOSS_DEFAULT)\n",
    "\n",
    "# Add to training_widgets\n",
    "training_widgets.update({\n",
    "    \"pause_autoencoder_button\": pause_autoencoder_button,\n",
    "    \"resume_autoencoder_button\": resume_autoencoder_button,\n",
    "    \"pause_predictor_button\": pause_predictor_button,\n",
    "    \"resume_predictor_button\": resume_predictor_button,\n",
    "    \"autoencoder_status\": autoencoder_status,\n",
    "    \"autoencoder_loss\": autoencoder_loss,\n",
    "    \"predictor_status\": predictor_status,\n",
    "    \"predictor_loss\": predictor_loss,\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a0ac20c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def on_run_predictor(_):\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    predictor = adaptive_world_model.predictors[0] if adaptive_world_model.predictors else None\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "\n",
    "    with session_widgets[\"predictor_output\"]:\n",
    "        session_widgets[\"predictor_output\"].clear_output()\n",
    "\n",
    "        if autoencoder is None or predictor is None:\n",
    "            display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "            return\n",
    "        if frame_slider is None:\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "            return\n",
    "\n",
    "        target_idx = frame_slider.value  # This is the CURRENT frame we want to predict\n",
    "        history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "        desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "\n",
    "        # Build history ending at target_idx - 1 (the past before current frame)\n",
    "        # We want to predict the CURRENT frame (target_idx) from this past\n",
    "        selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "\n",
    "        if error:\n",
    "            display(Markdown(f\"**Cannot run predictor:** {error}\"))\n",
    "            return\n",
    "\n",
    "        # Remove the last observation (target_idx) from history - we'll predict it\n",
    "        if len(selected_obs) < 2:\n",
    "            display(Markdown(\"**Cannot run predictor:** Need at least 2 frames (one past frame and the current frame to predict)\"))\n",
    "            return\n",
    "\n",
    "        past_obs = selected_obs[:-1]  # Frames up to t-1\n",
    "        current_obs = selected_obs[-1]  # Frame at t (to predict)\n",
    "\n",
    "        # Actions: action_dicts has transitions between frames\n",
    "        # The last action in action_dicts is the one that leads to current_obs\n",
    "        if len(action_dicts) < 1:\n",
    "            display(Markdown(\"**Cannot run predictor:** Need at least one action in history\"))\n",
    "            return\n",
    "\n",
    "        past_actions = action_dicts[:-1] if len(action_dicts) > 1 else []\n",
    "        recorded_current_action = action_dicts[-1]  # The action that led to current frame\n",
    "\n",
    "        # Encode the PAST frames (not including current)\n",
    "        past_feature_history = []\n",
    "        past_images = []\n",
    "        for obs in past_obs:\n",
    "            tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "            autoencoder.eval()\n",
    "            with torch.no_grad():\n",
    "                encoded = autoencoder.encode(tensor).detach()\n",
    "            past_feature_history.append(encoded)\n",
    "            past_images.append(tensor_to_numpy_image(tensor))\n",
    "\n",
    "        # Get the ACTUAL current frame\n",
    "        current_tensor = get_frame_tensor(session_state[\"session_dir\"], current_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "        current_img = tensor_to_numpy_image(current_tensor)\n",
    "\n",
    "        display(Markdown(f\"**Predictor Inference: Predicting CURRENT Frame {target_idx+1} from Past**\"))\n",
    "        display(Markdown(f\"**Past context length:** {len(past_obs)} frames\"))\n",
    "\n",
    "        # Show past context\n",
    "        fig, axes = plt.subplots(1, len(past_images), figsize=(3 * len(past_images), 3))\n",
    "        if len(past_images) == 1:\n",
    "            axes = [axes]\n",
    "        for i, img in enumerate(past_images):\n",
    "            axes[i].imshow(img)\n",
    "            axes[i].set_title(f\"Past Frame {past_obs[i]['observation_index']+1}\")\n",
    "            axes[i].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # =========================================================================\n",
    "        # PART 1: Prediction of CURRENT frame using recorded action\n",
    "        # =========================================================================\n",
    "        display(Markdown(\"## Prediction of CURRENT Frame (uses only past)\"))\n",
    "\n",
    "        predictor.eval()\n",
    "\n",
    "        # Prepare inputs for predicting current frame\n",
    "        actions_for_current = past_actions + [clone_action(recorded_current_action)]\n",
    "\n",
    "        # Normalize the action that leads to current\n",
    "        current_action_norm = normalize_action_dicts([recorded_current_action]).to(device)\n",
    "\n",
    "        # Last features = last past frame (t-1)\n",
    "        last_past_features = past_feature_history[-1] if past_feature_history else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Predict CURRENT frame (t) from past (t-1) + recorded action\n",
    "            predicted_current_features, attn_info = predictor(\n",
    "                past_feature_history,\n",
    "                actions_for_current,\n",
    "                return_attn=True,\n",
    "                action_normalized=current_action_norm,\n",
    "                last_features=last_past_features\n",
    "            )\n",
    "            predicted_current_tensor = decode_features_to_image(autoencoder, predicted_current_features)\n",
    "\n",
    "        predicted_current_img = tensor_to_numpy_image(predicted_current_tensor)\n",
    "\n",
    "        # Compute MSE between predicted and actual current\n",
    "        with torch.no_grad():\n",
    "            current_mse = torch.nn.functional.mse_loss(predicted_current_tensor, current_tensor).item()\n",
    "\n",
    "        # Display side-by-side\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "        axes[0].imshow(current_img)\n",
    "        axes[0].set_title(f\"Actual Current (Frame {target_idx+1})\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(predicted_current_img)\n",
    "        axes[1].set_title(f\"Predicted Current from Past\\nMSE: {current_mse:.6f}\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # =========================================================================\n",
    "        # PART 2: Action sweep - counterfactual predictions of CURRENT frame\n",
    "        # =========================================================================\n",
    "        display(Markdown(\"## Action Sweep (Counterfactual Predictions of CURRENT Frame)\"))\n",
    "        display(Markdown(\"*Each prediction shows what the current frame would look like under a different action.*\"))\n",
    "\n",
    "        action_space = get_action_space(session_state)\n",
    "\n",
    "        if not action_space:\n",
    "            display(Markdown(\"No action space available for sweep.\"))\n",
    "        else:\n",
    "            counterfactual_predictions = []\n",
    "\n",
    "            for action in action_space:\n",
    "                actions_variant = past_actions + [clone_action(action)]\n",
    "                variant_action_norm = normalize_action_dicts([action]).to(device)\n",
    "\n",
    "                # Check if this is the recorded action (reuse previous prediction)\n",
    "                if actions_equal(action, recorded_current_action):\n",
    "                    cf_tensor = predicted_current_tensor\n",
    "                    cf_img = predicted_current_img\n",
    "                    cf_mse = current_mse\n",
    "                else:\n",
    "                    with torch.no_grad():\n",
    "                        cf_features = predictor(\n",
    "                            past_feature_history,\n",
    "                            actions_variant,\n",
    "                            action_normalized=variant_action_norm,\n",
    "                            last_features=last_past_features\n",
    "                        )\n",
    "                        cf_tensor = decode_features_to_image(autoencoder, cf_features)\n",
    "                    cf_img = tensor_to_numpy_image(cf_tensor)\n",
    "                    with torch.no_grad():\n",
    "                        cf_mse = torch.nn.functional.mse_loss(cf_tensor, current_tensor).item()\n",
    "\n",
    "                counterfactual_predictions.append({\n",
    "                    \"action\": action,\n",
    "                    \"image\": cf_img,\n",
    "                    \"label\": format_action_label(action),\n",
    "                    \"mse\": cf_mse,\n",
    "                    \"is_recorded\": actions_equal(action, recorded_current_action)\n",
    "                })\n",
    "\n",
    "            # Grid of all counterfactual predictions\n",
    "            display(Markdown(\"### All Candidate Actions (Counterfactual Sweep)\"))\n",
    "            cols = min(4, len(counterfactual_predictions))\n",
    "            rows = math.ceil(len(counterfactual_predictions) / cols)\n",
    "            fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 3.5 * rows))\n",
    "            axes = np.array(axes).reshape(rows, cols) if rows > 1 or cols > 1 else np.array([[axes]])\n",
    "\n",
    "            for idx, pred in enumerate(counterfactual_predictions):\n",
    "                ax = axes[idx // cols][idx % cols]\n",
    "                ax.imshow(pred[\"image\"])\n",
    "                title = pred[\"label\"]\n",
    "                if pred[\"is_recorded\"]:\n",
    "                    title += \" (recorded)\"\n",
    "                title += f\"\\nMSE: {pred['mse']:.6f}\"\n",
    "                ax.set_title(title, fontsize=9)\n",
    "                ax.axis(\"off\")\n",
    "\n",
    "            # Hide unused subplots\n",
    "            for idx in range(len(counterfactual_predictions), rows * cols):\n",
    "                axes[idx // cols][idx % cols].axis(\"off\")\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        # =========================================================================\n",
    "        # Attention Diagnostics\n",
    "        # =========================================================================\n",
    "        attention_data = compute_attention_visual_data(attn_info)\n",
    "        if attention_data:\n",
    "            metrics = attention_data[\"metrics\"]\n",
    "            display(Markdown(\"## Attention Diagnostics\"))\n",
    "            metric_lines = [\n",
    "                f\"- **APA (Attention to Previous Action)**: {metrics['APA']:.4f}\",\n",
    "                f\"- **ALF (Attention to Last Frame)**: {metrics['ALF']:.4f}\",\n",
    "                f\"- **TTAR (Token-Type Attention Ratio)**: {metrics['TTAR']:.4f}\",\n",
    "                f\"- **RI@16 (Recency Index)**: {metrics['RI@16']:.4f}\",\n",
    "                f\"- **Entropy**: {metrics['Entropy']:.4f}\",\n",
    "                f\"- **Uniform Baseline**: {metrics['UniformBaseline']:.4f}\",\n",
    "            ]\n",
    "            display(Markdown(\"\\n\".join(metric_lines)))\n",
    "            plot_attention_heatmap(attention_data[\"heatmap\"], attention_data[\"token_types\"])\n",
    "            plot_attention_breakdown(attention_data[\"breakdown\"])\n",
    "\n",
    "        # =========================================================================\n",
    "        # Counterfactual testing (shuffle/zero actions)\n",
    "        # =========================================================================\n",
    "        display(Markdown(\"## Counterfactual Testing\"))\n",
    "\n",
    "        baseline_loss = adaptive_world_model.eval_predictor_loss(\n",
    "            predictor,\n",
    "            past_feature_history,\n",
    "            actions_for_current,\n",
    "            current_tensor,\n",
    "        )\n",
    "        shuffle_loss = adaptive_world_model.eval_predictor_loss(\n",
    "            predictor,\n",
    "            past_feature_history,\n",
    "            actions_for_current,\n",
    "            current_tensor,\n",
    "            override_actions=\"shuffle\",\n",
    "        )\n",
    "        zero_loss = adaptive_world_model.eval_predictor_loss(\n",
    "            predictor,\n",
    "            past_feature_history,\n",
    "            actions_for_current,\n",
    "            current_tensor,\n",
    "            override_actions=\"zero\",\n",
    "        )\n",
    "\n",
    "        asg = shuffle_loss - baseline_loss\n",
    "        azg = zero_loss - baseline_loss\n",
    "        display(Markdown(f\"**Counterfactual gaps:** ASG (shuffle) = {asg:.6f}, AZG (zero) = {azg:.6f}\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "flq0j1fn1ql",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e79d9e106e64bcba6f076d07f52c747",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Session', index=1, layout=Layout(width='300px'), options=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Predictor Training Section using AdaptiveWorldModel\n",
    "import asyncio\n",
    "import time\n",
    "\n",
    "PREDICTOR_TASK_KEY = \"predictor\"\n",
    "\n",
    "\n",
    "def _prepare_predictor_training():\n",
    "    \"\"\"Validate prerequisites and gather tensors for predictor training.\"\"\"\n",
    "    output = training_widgets[\"predictor_training_output\"]\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    predictor = adaptive_world_model.predictors[0] if adaptive_world_model.predictors else None\n",
    "    if autoencoder is None or predictor is None:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "        set_training_status(\"predictor\", \"error\", \"Load the autoencoder and predictor checkpoints first.\")\n",
    "        update_training_loss(\"predictor\", None)\n",
    "        return None\n",
    "\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        set_training_status(\"predictor\", \"error\", \"Load a session to select frames before training.\")\n",
    "        update_training_loss(\"predictor\", None)\n",
    "        return None\n",
    "\n",
    "    observations = session_state.get(\"observations\", [])\n",
    "    if not observations:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"No session loaded. Load a recording before training.\"))\n",
    "        set_training_status(\"predictor\", \"error\", \"No session loaded.\")\n",
    "        update_training_loss(\"predictor\", None)\n",
    "        return None\n",
    "\n",
    "    target_idx = frame_slider.value\n",
    "    history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "    desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "\n",
    "    selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "    if error:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(f\"**Cannot train predictor:** {error}\"))\n",
    "        set_training_status(\"predictor\", \"error\", f\"Cannot train: {error}\")\n",
    "        update_training_loss(\"predictor\", None)\n",
    "        return None\n",
    "\n",
    "    if target_idx + 1 >= len(observations):\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"**Cannot train predictor:** No next frame available as training target.\"))\n",
    "        set_training_status(\"predictor\", \"error\", \"No next frame available for training.\")\n",
    "        update_training_loss(\"predictor\", None)\n",
    "        return None\n",
    "\n",
    "    next_obs = observations[target_idx + 1]\n",
    "    target_tensor = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "\n",
    "    feature_history = []\n",
    "    for obs in selected_obs:\n",
    "        tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "        autoencoder.eval()\n",
    "        with torch.no_grad():\n",
    "            encoded = autoencoder.encode(tensor).detach()\n",
    "        feature_history.append(encoded)\n",
    "\n",
    "    recorded_future_action, action_source = get_future_action_for_prediction(session_state, target_idx)\n",
    "    info_message = None\n",
    "    if recorded_future_action is None:\n",
    "        info_message = \"No recorded action between current and next frame; using empty action.\"\n",
    "        recorded_future_action = {}\n",
    "    elif action_source == \"previous\":\n",
    "        info_message = \"Using the most recent action prior to the current frame.\"\n",
    "\n",
    "    history_actions_with_future = [clone_action(action) for action in action_dicts]\n",
    "    history_actions_with_future.append(clone_action(recorded_future_action))\n",
    "\n",
    "    return {\n",
    "        \"output\": output,\n",
    "        \"autoencoder\": autoencoder,\n",
    "        \"predictor\": predictor,\n",
    "        \"target_idx\": target_idx,\n",
    "        \"selected_obs\": selected_obs,\n",
    "        \"info_message\": info_message,\n",
    "        \"feature_history\": feature_history,\n",
    "        \"history_actions_with_future\": history_actions_with_future,\n",
    "        \"target_tensor\": target_tensor,\n",
    "        \"next_obs\": next_obs,\n",
    "    }\n",
    "\n",
    "\n",
    "def _display_predictor_weight_changes(pre_stats, post_stats):\n",
    "    if not (pre_stats and post_stats):\n",
    "        return\n",
    "\n",
    "    weight_changes = {}\n",
    "    if \"action_embed_mean\" in pre_stats and \"action_embed_mean\" in post_stats:\n",
    "        weight_changes.update({\n",
    "            \"action_embed_mean_change\": abs(post_stats[\"action_embed_mean\"] - pre_stats[\"action_embed_mean\"]),\n",
    "            \"action_embed_std_change\": abs(post_stats[\"action_embed_std\"] - pre_stats[\"action_embed_std\"]),\n",
    "        })\n",
    "    if \"pos_embed_mean\" in pre_stats and \"pos_embed_mean\" in post_stats:\n",
    "        weight_changes.update({\n",
    "            \"pos_embed_mean_change\": abs(post_stats[\"pos_embed_mean\"] - pre_stats[\"pos_embed_mean\"]),\n",
    "            \"pos_embed_std_change\": abs(post_stats[\"pos_embed_std\"] - pre_stats[\"pos_embed_std\"]),\n",
    "        })\n",
    "    if \"first_self_attn_mean\" in pre_stats and \"first_self_attn_mean\" in post_stats:\n",
    "        weight_changes.update({\n",
    "            \"first_self_attn_mean_change\": abs(post_stats[\"first_self_attn_mean\"] - pre_stats[\"first_self_attn_mean\"]),\n",
    "            \"first_self_attn_std_change\": abs(post_stats[\"first_self_attn_std\"] - pre_stats[\"first_self_attn_std\"]),\n",
    "        })\n",
    "    if \"first_linear1_mean\" in pre_stats and \"first_linear1_mean\" in post_stats:\n",
    "        weight_changes.update({\n",
    "            \"first_linear1_mean_change\": abs(post_stats[\"first_linear1_mean\"] - pre_stats[\"first_linear1_mean\"]),\n",
    "            \"first_linear1_std_change\": abs(post_stats[\"first_linear1_std\"] - pre_stats[\"first_linear1_std\"]),\n",
    "        })\n",
    "\n",
    "    if not weight_changes:\n",
    "        return\n",
    "\n",
    "    lines = [\"**Weight Changes:**\"]\n",
    "    if \"action_embed_mean_change\" in weight_changes:\n",
    "        lines.extend([\n",
    "            f\"- Action Embed Mean: {weight_changes['action_embed_mean_change']:.8f}\",\n",
    "            f\"- Action Embed Std: {weight_changes['action_embed_std_change']:.8f}\",\n",
    "        ])\n",
    "    if \"pos_embed_mean_change\" in weight_changes:\n",
    "        lines.extend([\n",
    "            f\"- Position Embed Mean: {weight_changes['pos_embed_mean_change']:.8f}\",\n",
    "            f\"- Position Embed Std: {weight_changes['pos_embed_std_change']:.8f}\",\n",
    "        ])\n",
    "    if \"first_self_attn_mean_change\" in weight_changes:\n",
    "        lines.extend([\n",
    "            f\"- First Self-Attn Mean: {weight_changes['first_self_attn_mean_change']:.8f}\",\n",
    "            f\"- First Self-Attn Std: {weight_changes['first_self_attn_std_change']:.8f}\",\n",
    "        ])\n",
    "    if \"first_linear1_mean_change\" in weight_changes:\n",
    "        lines.extend([\n",
    "            f\"- First Linear1 Mean: {weight_changes['first_linear1_mean_change']:.8f}\",\n",
    "            f\"- First Linear1 Std: {weight_changes['first_linear1_std_change']:.8f}\",\n",
    "        ])\n",
    "\n",
    "    display(Markdown(\"\\n\".join(lines)))\n",
    "\n",
    "\n",
    "async def predictor_threshold_training(context, threshold, max_steps):\n",
    "    output = context[\"output\"]\n",
    "    autoencoder = context[\"autoencoder\"]\n",
    "    predictor = context[\"predictor\"]\n",
    "    target_idx = context[\"target_idx\"]\n",
    "    selected_obs = context[\"selected_obs\"]\n",
    "    info_message = context[\"info_message\"]\n",
    "    feature_history = context[\"feature_history\"]\n",
    "    history_actions_with_future = context[\"history_actions_with_future\"]\n",
    "    target_tensor = context[\"target_tensor\"]\n",
    "    next_obs = context[\"next_obs\"]\n",
    "\n",
    "    training_control[\"predictor_resume_data\"] = None\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    set_training_status(\"predictor\", \"running\", f\"Running to threshold {format_loss(threshold)}\")\n",
    "    update_training_loss(\"predictor\", None)\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        display(Markdown(\n",
    "            f\"**Training predictor using AdaptiveWorldModel on history ending at frame {target_idx + 1} (step {selected_obs[-1]['step']})**\"\n",
    "        ))\n",
    "        display(Markdown(f\"Target threshold: {format_loss(threshold)}, Max steps: {max_steps}\"))\n",
    "        display(Markdown(f\"History length: {len(selected_obs)} frames\"))\n",
    "        if info_message:\n",
    "            display(Markdown(info_message))\n",
    "\n",
    "        display(Markdown(\"### Pre-Training Predictor Network Weights\"))\n",
    "        pre_stats = visualize_predictor_weights(predictor)\n",
    "\n",
    "        display(Markdown(\"**Using AdaptiveWorldModel.train_predictor() method with joint training**\"))\n",
    "        display(Markdown(\"**Tip: Click 'Pause' button to interrupt training at any time.**\"))\n",
    "\n",
    "        progress = tqdm(range(max_steps), desc=\"Training\")\n",
    "        try:\n",
    "            for step in progress:\n",
    "                await asyncio.sleep(0)\n",
    "                if training_control[\"predictor_paused\"]:\n",
    "                    training_control[\"predictor_resume_data\"] = {\n",
    "                        \"target_tensor\": target_tensor,\n",
    "                        \"feature_history\": feature_history,\n",
    "                        \"history_actions_with_future\": history_actions_with_future,\n",
    "                        \"threshold\": threshold,\n",
    "                        \"max_steps\": max_steps,\n",
    "                        \"current_step\": step,\n",
    "                        \"losses\": losses,\n",
    "                        \"start_time\": start_time,\n",
    "                    }\n",
    "                    set_training_status(\"predictor\", \"paused\", f\"Paused at step {step} of {max_steps}. Resume available.\")\n",
    "                    if losses:\n",
    "                        update_training_loss(\"predictor\", losses[-1], len(losses), state=\"paused\")\n",
    "                    else:\n",
    "                        update_training_loss(\"predictor\", None)\n",
    "                    display(Markdown(\"**Training paused. Use Resume button to continue.**\"))\n",
    "                    return\n",
    "\n",
    "                try:\n",
    "                    # Normalize action for FiLM conditioning\n",
    "                    if history_actions_with_future:\n",
    "                        action_normalized = normalize_action_dicts([history_actions_with_future[-1]]).to(device)\n",
    "                    else:\n",
    "                        action_normalized = torch.zeros(1, len(config.ACTION_CHANNELS), device=device)\n",
    "\n",
    "                    # Get last features for delta prediction\n",
    "                    last_features = feature_history[-1] if feature_history else None\n",
    "\n",
    "                    predicted_features = predictor(\n",
    "                        feature_history,\n",
    "                        history_actions_with_future,\n",
    "                        action_normalized=action_normalized,\n",
    "                        last_features=last_features\n",
    "                    )\n",
    "                    loss = adaptive_world_model.train_predictor(\n",
    "                        level=0,\n",
    "                        current_frame_tensor=target_tensor,\n",
    "                        predicted_features=predicted_features,\n",
    "                        history_features=feature_history,\n",
    "                        history_actions=history_actions_with_future,\n",
    "                    )\n",
    "                except Exception as exc:\n",
    "                    display(Markdown(f\"**Training error:** {exc}\"))\n",
    "                    set_training_status(\"predictor\", \"error\", f\"Error: {exc}\")\n",
    "                    training_control[\"predictor_resume_data\"] = None\n",
    "                    update_training_loss(\"predictor\", None, state=\"error\")\n",
    "                    return\n",
    "\n",
    "                losses.append(loss)\n",
    "                update_training_loss(\"predictor\", loss, len(losses))\n",
    "                progress.set_postfix({\"Loss\": format_loss(loss), \"Step\": f\"{step + 1}/{max_steps}\"})\n",
    "\n",
    "                if loss <= threshold:\n",
    "                    break\n",
    "        finally:\n",
    "            progress.close()\n",
    "\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float(\"inf\")\n",
    "\n",
    "        display(Markdown(f\"**Training completed after {len(losses)} steps in {end_time - start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Final total loss: {format_loss(final_loss)}\"))\n",
    "\n",
    "        if final_loss <= threshold:\n",
    "            display(Markdown(f\"**Target threshold {format_loss(threshold)} achieved!**\"))\n",
    "        else:\n",
    "            display(Markdown(f\"**Target threshold {format_loss(threshold)} not reached after {max_steps} steps.**\"))\n",
    "\n",
    "        display(Markdown(\"### Post-Training Predictor Network Weights\"))\n",
    "        post_stats = visualize_predictor_weights(predictor)\n",
    "        _display_predictor_weight_changes(pre_stats, post_stats)\n",
    "\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses, label=\"Total Loss\")\n",
    "            ax.axhline(y=threshold, color=\"r\", linestyle=\"--\", alpha=0.7, label=f\"Target: {format_loss(threshold)}\")\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.set_title(\"Predictor Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        predictor.eval()\n",
    "        autoencoder.eval()\n",
    "\n",
    "        # Normalize action for FiLM conditioning\n",
    "        if history_actions_with_future:\n",
    "            action_normalized = normalize_action_dicts([history_actions_with_future[-1]]).to(device)\n",
    "        else:\n",
    "            action_normalized = torch.zeros(1, len(config.ACTION_CHANNELS), device=device)\n",
    "\n",
    "        # Get last features for delta prediction\n",
    "        last_features = feature_history[-1] if feature_history else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_features = predictor(\n",
    "                feature_history,\n",
    "                history_actions_with_future,\n",
    "                action_normalized=action_normalized,\n",
    "                last_features=last_features\n",
    "            )\n",
    "            predicted_frame = decode_features_to_image(autoencoder, predicted_features)\n",
    "\n",
    "        predicted_img = tensor_to_numpy_image(predicted_frame)\n",
    "        target_img = tensor_to_numpy_image(target_tensor)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(predicted_img)\n",
    "        axes[0].set_title(f\"Predicted (Loss: {format_loss(final_loss)})\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(target_img)\n",
    "        axes[1].set_title(f\"Actual (step {next_obs['step']})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if losses:\n",
    "        update_training_loss(\"predictor\", final_loss, len(losses), state=\"completed\")\n",
    "        set_training_status(\"predictor\", \"completed\", f\"Completed in {len(losses)} steps (final loss {format_loss(final_loss)}).\")\n",
    "    else:\n",
    "        update_training_loss(\"predictor\", None)\n",
    "        set_training_status(\"predictor\", \"completed\", \"Completed without updating any steps.\")\n",
    "\n",
    "    training_control[\"predictor_resume_data\"] = None\n",
    "\n",
    "\n",
    "async def predictor_steps_training(context, num_steps):\n",
    "    output = context[\"output\"]\n",
    "    autoencoder = context[\"autoencoder\"]\n",
    "    predictor = context[\"predictor\"]\n",
    "    target_idx = context[\"target_idx\"]\n",
    "    selected_obs = context[\"selected_obs\"]\n",
    "    info_message = context[\"info_message\"]\n",
    "    feature_history = context[\"feature_history\"]\n",
    "    history_actions_with_future = context[\"history_actions_with_future\"]\n",
    "    target_tensor = context[\"target_tensor\"]\n",
    "    next_obs = context[\"next_obs\"]\n",
    "\n",
    "    training_control[\"predictor_resume_data\"] = None\n",
    "    losses = []\n",
    "    start_time = time.time()\n",
    "\n",
    "    set_training_status(\"predictor\", \"running\", f\"Running for {num_steps} steps\")\n",
    "    update_training_loss(\"predictor\", None)\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        display(Markdown(\n",
    "            f\"**Training predictor using AdaptiveWorldModel on history ending at frame {target_idx + 1} (step {selected_obs[-1]['step']}) for {num_steps} steps**\"\n",
    "        ))\n",
    "        display(Markdown(f\"History length: {len(selected_obs)} frames\"))\n",
    "        if info_message:\n",
    "            display(Markdown(info_message))\n",
    "\n",
    "        display(Markdown(\"### Pre-Training Predictor Network Weights\"))\n",
    "        pre_stats = visualize_predictor_weights(predictor)\n",
    "\n",
    "        display(Markdown(\"**Using AdaptiveWorldModel.train_predictor() method with joint training**\"))\n",
    "        display(Markdown(\"**Tip: Click 'Pause' button to interrupt training at any time.**\"))\n",
    "\n",
    "        progress = tqdm(range(num_steps), desc=\"Training\")\n",
    "        try:\n",
    "            for step in progress:\n",
    "                await asyncio.sleep(0)\n",
    "                if training_control[\"predictor_paused\"]:\n",
    "                    set_training_status(\"predictor\", \"paused\", f\"Paused after {step} of {num_steps} steps; restart to continue.\")\n",
    "                    if losses:\n",
    "                        update_training_loss(\"predictor\", losses[-1], len(losses), state=\"paused\")\n",
    "                    else:\n",
    "                        update_training_loss(\"predictor\", None)\n",
    "                    display(Markdown(\"**Training paused. Step-based training cannot be resumed.**\"))\n",
    "                    display(Markdown(f\"**Completed {step} out of {num_steps} steps before pausing.**\"))\n",
    "                    return\n",
    "\n",
    "                try:\n",
    "                    # Normalize action for FiLM conditioning\n",
    "                    if history_actions_with_future:\n",
    "                        action_normalized = normalize_action_dicts([history_actions_with_future[-1]]).to(device)\n",
    "                    else:\n",
    "                        action_normalized = torch.zeros(1, len(config.ACTION_CHANNELS), device=device)\n",
    "\n",
    "                    # Get last features for delta prediction\n",
    "                    last_features = feature_history[-1] if feature_history else None\n",
    "\n",
    "                    predicted_features = predictor(\n",
    "                        feature_history,\n",
    "                        history_actions_with_future,\n",
    "                        action_normalized=action_normalized,\n",
    "                        last_features=last_features\n",
    "                    )\n",
    "                    loss = adaptive_world_model.train_predictor(\n",
    "                        level=0,\n",
    "                        current_frame_tensor=target_tensor,\n",
    "                        predicted_features=predicted_features,\n",
    "                        history_features=feature_history,\n",
    "                        history_actions=history_actions_with_future,\n",
    "                    )\n",
    "                except Exception as exc:\n",
    "                    display(Markdown(f\"**Training error:** {exc}\"))\n",
    "                    set_training_status(\"predictor\", \"error\", f\"Error: {exc}\")\n",
    "                    update_training_loss(\"predictor\", None, state=\"error\")\n",
    "                    return\n",
    "\n",
    "                losses.append(loss)\n",
    "                update_training_loss(\"predictor\", loss, len(losses))\n",
    "                progress.set_postfix({\"Loss\": format_loss(loss), \"Step\": f\"{step + 1}/{num_steps}\"})\n",
    "        finally:\n",
    "            progress.close()\n",
    "\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float(\"inf\")\n",
    "\n",
    "        display(Markdown(f\"**Training completed in {end_time - start_time:.1f}s**\"))\n",
    "        if losses:\n",
    "            display(Markdown(f\"Initial total loss: {format_loss(losses[0])}, Final total loss: {format_loss(final_loss)}\"))\n",
    "        else:\n",
    "            display(Markdown(\"** **No training steps were executed before the run stopped.**\"))\n",
    "\n",
    "        display(Markdown(\"### Post-Training Predictor Network Weights\"))\n",
    "        post_stats = visualize_predictor_weights(predictor)\n",
    "        _display_predictor_weight_changes(pre_stats, post_stats)\n",
    "\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses)\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Total Loss\")\n",
    "            ax.set_title(\"Predictor Training Progress (AdaptiveWorldModel)\")\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        predictor.eval()\n",
    "        autoencoder.eval()\n",
    "\n",
    "        # Normalize action for FiLM conditioning\n",
    "        if history_actions_with_future:\n",
    "            action_normalized = normalize_action_dicts([history_actions_with_future[-1]]).to(device)\n",
    "        else:\n",
    "            action_normalized = torch.zeros(1, len(config.ACTION_CHANNELS), device=device)\n",
    "\n",
    "        # Get last features for delta prediction\n",
    "        last_features = feature_history[-1] if feature_history else None\n",
    "\n",
    "        with torch.no_grad():\n",
    "            predicted_features = predictor(\n",
    "                feature_history,\n",
    "                history_actions_with_future,\n",
    "                action_normalized=action_normalized,\n",
    "                last_features=last_features\n",
    "            )\n",
    "            predicted_frame = decode_features_to_image(autoencoder, predicted_features)\n",
    "\n",
    "        predicted_img = tensor_to_numpy_image(predicted_frame)\n",
    "        target_img = tensor_to_numpy_image(target_tensor)\n",
    "\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(predicted_img)\n",
    "        axes[0].set_title(f\"Predicted (Loss: {format_loss(final_loss)})\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(target_img)\n",
    "        axes[1].set_title(f\"Actual (step {next_obs['step']})\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    if losses:\n",
    "        update_training_loss(\"predictor\", final_loss, len(losses), state=\"completed\")\n",
    "        set_training_status(\"predictor\", \"completed\", f\"Finished {len(losses)} steps (final loss {format_loss(final_loss)}).\")\n",
    "    else:\n",
    "        update_training_loss(\"predictor\", None)\n",
    "        set_training_status(\"predictor\", \"completed\", \"Completed without updating any steps.\")\n",
    "\n",
    "    training_control[\"predictor_resume_data\"] = None\n",
    "\n",
    "\n",
    "async def predictor_resume_training(resume_data):\n",
    "    output = training_widgets[\"predictor_training_output\"]\n",
    "    autoencoder = adaptive_world_model.autoencoder\n",
    "    if autoencoder is None or not adaptive_world_model.predictors:\n",
    "        with output:\n",
    "            output.clear_output()\n",
    "            display(Markdown(\"**Required models are not loaded. Load checkpoints before resuming.**\"))\n",
    "        set_training_status(\"predictor\", \"error\", \"Cannot resume because checkpoints are not loaded.\")\n",
    "        training_control[\"predictor_resume_data\"] = None\n",
    "        return\n",
    "\n",
    "    predictor = adaptive_world_model.predictors[0]\n",
    "\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        display(Markdown(\"**Resuming predictor training...**\"))\n",
    "\n",
    "        target_tensor = resume_data[\"target_tensor\"]\n",
    "        feature_history = resume_data[\"feature_history\"]\n",
    "        history_actions_with_future = resume_data[\"history_actions_with_future\"]\n",
    "        threshold = resume_data[\"threshold\"]\n",
    "        max_steps = resume_data[\"max_steps\"]\n",
    "        current_step = resume_data[\"current_step\"]\n",
    "        losses = resume_data[\"losses\"]\n",
    "        start_time = resume_data[\"start_time\"]\n",
    "\n",
    "        if losses:\n",
    "            update_training_loss(\"predictor\", losses[-1], len(losses))\n",
    "        else:\n",
    "            update_training_loss(\"predictor\", None)\n",
    "\n",
    "        set_training_status(\"predictor\", \"running\", f\"Resuming from step {current_step} of {max_steps}.\")\n",
    "\n",
    "        remaining_steps = max_steps - current_step\n",
    "        if remaining_steps <= 0:\n",
    "            display(Markdown(\"**Training already reached the requested number of steps.**\"))\n",
    "            if losses:\n",
    "                update_training_loss(\"predictor\", losses[-1], len(losses), state=\"completed\")\n",
    "            else:\n",
    "                update_training_loss(\"predictor\", None)\n",
    "            set_training_status(\"predictor\", \"completed\", \"Requested number of steps already reached before resuming.\")\n",
    "            training_control[\"predictor_resume_data\"] = None\n",
    "            return\n",
    "\n",
    "        progress = tqdm(range(remaining_steps), desc=f\"Resuming from step {current_step}\")\n",
    "        try:\n",
    "            for step_offset in progress:\n",
    "                await asyncio.sleep(0)\n",
    "                if training_control[\"predictor_paused\"]:\n",
    "                    resume_data.update({\n",
    "                        \"current_step\": current_step + step_offset,\n",
    "                        \"losses\": losses,\n",
    "                    })\n",
    "                    display(Markdown(\"** **Training paused. Use Resume button to continue.**\"))\n",
    "                    return\n",
    "\n",
    "                try:\n",
    "                    # Normalize action for FiLM conditioning\n",
    "                    if history_actions_with_future:\n",
    "                        action_normalized = normalize_action_dicts([history_actions_with_future[-1]]).to(device)\n",
    "                    else:\n",
    "                        action_normalized = torch.zeros(1, len(config.ACTION_CHANNELS), device=device)\n",
    "\n",
    "                    # Get last features for delta prediction\n",
    "                    last_features = feature_history[-1] if feature_history else None\n",
    "\n",
    "                    predicted_features = predictor(\n",
    "                        feature_history,\n",
    "                        history_actions_with_future,\n",
    "                        action_normalized=action_normalized,\n",
    "                        last_features=last_features\n",
    "                    )\n",
    "                    loss = adaptive_world_model.train_predictor(\n",
    "                        level=0,\n",
    "                        current_frame_tensor=target_tensor,\n",
    "                        predicted_features=predicted_features,\n",
    "                        history_features=feature_history,\n",
    "                        history_actions=history_actions_with_future,\n",
    "                    )\n",
    "                except Exception as exc:\n",
    "                    display(Markdown(f\"**Training error:** {exc}\"))\n",
    "                    set_training_status(\"predictor\", \"error\", f\"Error: {exc}\")\n",
    "                    training_control[\"predictor_resume_data\"] = None\n",
    "                    update_training_loss(\"predictor\", None, state=\"error\")\n",
    "                    return\n",
    "\n",
    "                losses.append(loss)\n",
    "                update_training_loss(\"predictor\", loss, len(losses))\n",
    "                progress.set_postfix({\"Loss\": format_loss(loss)})\n",
    "\n",
    "                if loss <= threshold:\n",
    "                    break\n",
    "        finally:\n",
    "            progress.close()\n",
    "\n",
    "        training_control[\"predictor_resume_data\"] = None\n",
    "\n",
    "        end_time = time.time()\n",
    "        final_loss = losses[-1] if losses else float(\"inf\")\n",
    "\n",
    "        display(Markdown(f\"**Training completed after {len(losses)} total steps in {end_time - start_time:.1f}s**\"))\n",
    "        display(Markdown(f\"Final total loss: {format_loss(final_loss)}\"))\n",
    "\n",
    "        if final_loss <= threshold:\n",
    "            display(Markdown(f\"**Target threshold {format_loss(threshold)} achieved!**\"))\n",
    "        else:\n",
    "            display(Markdown(f\"**Target threshold {format_loss(threshold)} not reached after {max_steps} steps.**\"))\n",
    "\n",
    "        if len(losses) > 1:\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(8, 4))\n",
    "            ax.plot(losses, label=\"Total Loss\")\n",
    "            ax.axhline(y=threshold, color=\"r\", linestyle=\"--\", alpha=0.7, label=f\"Target: {format_loss(threshold)}\")\n",
    "            ax.set_xlabel(\"Training Step\")\n",
    "            ax.set_ylabel(\"Loss\")\n",
    "            ax.set_title(\"Predictor Training Progress (Resumed)\")\n",
    "            ax.legend()\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        if losses:\n",
    "            update_training_loss(\"predictor\", final_loss, len(losses), state=\"completed\")\n",
    "            set_training_status(\"predictor\", \"completed\", f\"Completed with final loss {format_loss(final_loss)} after {len(losses)} steps.\")\n",
    "        else:\n",
    "            update_training_loss(\"predictor\", None)\n",
    "            set_training_status(\"predictor\", \"completed\", \"Completed without updating any steps.\")\n",
    "\n",
    "\n",
    "def on_train_predictor_threshold(_):\n",
    "    context = _prepare_predictor_training()\n",
    "    if context is None:\n",
    "        return\n",
    "\n",
    "    training_control[\"predictor_paused\"] = False\n",
    "    training_control[\"predictor_resume_data\"] = None\n",
    "\n",
    "    threshold = training_widgets[\"predictor_threshold\"].value\n",
    "    max_steps = training_widgets[\"predictor_max_steps\"].value\n",
    "\n",
    "    set_training_status(\"predictor\", \"running\", f\"Running to threshold {format_loss(threshold)}\")\n",
    "\n",
    "    start_training_task(\n",
    "        PREDICTOR_TASK_KEY,\n",
    "        predictor_threshold_training(context, threshold, max_steps),\n",
    "        context[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "def on_train_predictor_steps(_):\n",
    "    context = _prepare_predictor_training()\n",
    "    if context is None:\n",
    "        return\n",
    "\n",
    "    training_control[\"predictor_paused\"] = False\n",
    "    training_control[\"predictor_resume_data\"] = None\n",
    "\n",
    "    num_steps = training_widgets[\"predictor_steps\"].value\n",
    "\n",
    "    set_training_status(\"predictor\", \"running\", f\"Running for {num_steps} steps\")\n",
    "\n",
    "    start_training_task(\n",
    "        PREDICTOR_TASK_KEY,\n",
    "        predictor_steps_training(context, num_steps),\n",
    "        context[\"output\"],\n",
    "    )\n",
    "\n",
    "\n",
    "# Create predictor training widgets (same as before)\n",
    "predictor_threshold = widgets.FloatText(value=0.0005, description=\"Threshold\", step=0.0001, style={'description_width': '100px'})\n",
    "predictor_max_steps = widgets.IntText(value=1000, description=\"Max Steps\", style={'description_width': '100px'})\n",
    "predictor_steps = widgets.IntText(value=100, description=\"Steps\", style={'description_width': '100px'})\n",
    "\n",
    "train_predictor_threshold_button = widgets.Button(description=\"Train to Threshold\", button_style=\"danger\", icon=\"target\")\n",
    "train_predictor_steps_button = widgets.Button(description=\"Train N Steps\", button_style=\"danger\", icon=\"forward\")\n",
    "predictor_training_output = widgets.Output()\n",
    "\n",
    "training_widgets.update({\n",
    "    \"predictor_threshold\": predictor_threshold,\n",
    "    \"predictor_max_steps\": predictor_max_steps,\n",
    "    \"predictor_steps\": predictor_steps,\n",
    "    \"predictor_training_output\": predictor_training_output,\n",
    "})\n",
    "\n",
    "train_predictor_threshold_button.on_click(on_train_predictor_threshold)\n",
    "train_predictor_steps_button.on_click(on_train_predictor_steps)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Interactive controls and callbacks\n",
    "session_state = {\n",
    "    \"session_name\": None,\n",
    "    \"session_dir\": None,\n",
    "    \"metadata\": {},\n",
    "    \"events\": [],\n",
    "    \"observations\": [],\n",
    "    \"actions\": [],\n",
    "    \"autoencoder\": None,\n",
    "    \"predictor\": None,\n",
    "    \"feature_cache\": {},\n",
    "    \"action_space\": [],\n",
    "}\n",
    "\n",
    "session_widgets = {}\n",
    "\n",
    "def reset_feature_cache():\n",
    "    session_state[\"feature_cache\"] = {}\n",
    "\n",
    "def on_refresh_sessions(_=None):\n",
    "    options = list_session_dirs(SESSIONS_BASE_DIR)\n",
    "    current = session_widgets[\"session_dropdown\"].value if \"session_dropdown\" in session_widgets else None\n",
    "    session_widgets[\"session_dropdown\"].options = options\n",
    "    if not options:\n",
    "        session_widgets[\"session_dropdown\"].value = None\n",
    "    elif current in options:\n",
    "        session_widgets[\"session_dropdown\"].value = current\n",
    "    else:\n",
    "        session_widgets[\"session_dropdown\"].value = options[-1]\n",
    "\n",
    "def on_load_session(_):\n",
    "    dropdown = session_widgets[\"session_dropdown\"]\n",
    "    session_name = dropdown.value\n",
    "    if not session_name:\n",
    "        return\n",
    "    session_dir = os.path.join(SESSIONS_BASE_DIR, session_name)\n",
    "    metadata = load_session_metadata(session_dir)\n",
    "    events = load_session_events(session_dir)\n",
    "    observations = extract_observations(events, session_dir)\n",
    "    actions = extract_actions(events)\n",
    "\n",
    "    session_state.update({\n",
    "        \"session_name\": session_name,\n",
    "        \"session_dir\": session_dir,\n",
    "        \"metadata\": metadata,\n",
    "        \"events\": events,\n",
    "        \"observations\": observations,\n",
    "        \"actions\": actions,\n",
    "    })\n",
    "    session_state[\"action_space\"] = get_action_space(session_state)\n",
    "    reset_feature_cache()\n",
    "    tensor_cache.clear()\n",
    "    load_frame_bytes.cache_clear()\n",
    "\n",
    "    with session_widgets[\"session_area\"]:\n",
    "        session_widgets[\"session_area\"].clear_output()\n",
    "        if not observations:\n",
    "            display(Markdown(f\"**{session_name}** has no observation frames.\"))\n",
    "            return\n",
    "        details = [\n",
    "            f\"**Session:** {session_name}\",\n",
    "            f\"**Total events:** {len(events)}\",\n",
    "            f\"**Observations:** {len(observations)}\",\n",
    "            f\"**Actions:** {len(actions)}\",\n",
    "        ]\n",
    "        if metadata:\n",
    "            start_time = metadata.get(\"start_time\")\n",
    "            if start_time:\n",
    "                details.append(f\"**Start:** {start_time}\")\n",
    "            robot_type = metadata.get(\"robot_type\")\n",
    "            if robot_type:\n",
    "                details.append(f\"**Robot:** {robot_type}\")\n",
    "        display(Markdown(\"<br>\".join(details)))\n",
    "\n",
    "        frame_slider = widgets.IntSlider(value=0, min=0, max=len(observations) - 1, description=\"Frame\", continuous_update=False)\n",
    "        play_widget = widgets.Play(interval=100, value=0, min=0, max=len(observations) - 1, step=1, description=\"Play\")\n",
    "        widgets.jslink((play_widget, \"value\"), (frame_slider, \"value\"))\n",
    "\n",
    "        frame_image = widgets.Image(format=\"jpg\")\n",
    "        frame_image.layout.width = \"448px\"\n",
    "        frame_info = widgets.HTML()\n",
    "        history_preview = widgets.Output()\n",
    "\n",
    "        session_widgets[\"frame_slider\"] = frame_slider\n",
    "        session_widgets[\"play_widget\"] = play_widget\n",
    "        session_widgets[\"frame_image\"] = frame_image\n",
    "        session_widgets[\"frame_info\"] = frame_info\n",
    "        session_widgets[\"history_preview\"] = history_preview\n",
    "\n",
    "        def update_history_preview(idx):\n",
    "            if \"history_preview\" not in session_widgets:\n",
    "                return\n",
    "            history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "            requested = history_slider_widget.value if history_slider_widget else 3\n",
    "            requested = max(1, requested)\n",
    "            requested = min(requested, idx + 1)\n",
    "            start = max(0, idx - requested + 1)\n",
    "            obs_slice = observations[start: idx + 1]\n",
    "            events_local = session_state.get(\"events\", [])\n",
    "            display_items = []\n",
    "            for offset, obs in enumerate(obs_slice):\n",
    "                frame_bytes = load_frame_bytes(obs[\"full_path\"])\n",
    "                border_color = \"#4caf50\" if (start + offset) == idx else \"#cccccc\"\n",
    "                image = widgets.Image(value=frame_bytes, format=\"jpg\", layout=widgets.Layout(width=\"160px\", height=\"120px\", border=f\"2px solid {border_color}\"))\n",
    "                label_text = f\"Step {obs['step']}\"\n",
    "                if (start + offset) == idx:\n",
    "                    label_text += \" (current)\"\n",
    "                label = widgets.HTML(value=f\"<div style='text-align:center; font-size:10px'>{label_text}</div>\")\n",
    "                display_items.append(widgets.VBox([image, label]))\n",
    "                if offset < len(obs_slice) - 1:\n",
    "                    next_obs = obs_slice[offset + 1]\n",
    "                    actions_between = [events_local[i] for i in range(obs[\"event_index\"] + 1, next_obs[\"event_index\"]) if events_local[i].get(\"type\") == \"action\"]\n",
    "                    if actions_between:\n",
    "                        action_text = \"; \".join(format_action_label(act.get(\"data\", {})) for act in actions_between)\n",
    "                    else:\n",
    "                        action_text = \"No action\"\n",
    "                    action_label = widgets.HTML(value=f\"<div style='font-size:10px; padding:0 6px;'>Action: {action_text}</div>\", layout=widgets.Layout(height=\"120px\", display=\"flex\", align_items=\"center\", justify_content=\"center\"))\n",
    "                    display_items.append(action_label)\n",
    "            session_widgets[\"history_preview\"].clear_output()\n",
    "            with session_widgets[\"history_preview\"]:\n",
    "                if display_items:\n",
    "                    layout = widgets.Layout(display=\"flex\", flex_flow=\"row\", align_items=\"center\")\n",
    "                    display(widgets.HBox(display_items, layout=layout))\n",
    "                else:\n",
    "                    display(Markdown(\"History preview unavailable for this frame.\"))\n",
    "\n",
    "        session_widgets[\"update_history_preview\"] = update_history_preview\n",
    "\n",
    "        def update_frame(change):\n",
    "            idx_local = change[\"new\"] if isinstance(change, dict) else change\n",
    "            observation = observations[idx_local]\n",
    "            frame_image.value = load_frame_bytes(observation[\"full_path\"])\n",
    "            frame_info.value = f\"<b>Observation {idx_local + 1} / {len(observations)}</b><br>Step: {observation['step']}<br>Timestamp: {format_timestamp(observation['timestamp'])}\"\n",
    "            update_history_preview(idx_local)\n",
    "\n",
    "        frame_slider.observe(update_frame, names=\"value\")\n",
    "        update_frame({\"new\": frame_slider.value})\n",
    "\n",
    "        display(widgets.VBox([\n",
    "            widgets.HBox([play_widget, frame_slider]),\n",
    "            frame_image,\n",
    "            frame_info,\n",
    "            widgets.HTML(\"<b>History preview</b>\"),\n",
    "            history_preview,\n",
    "        ]))\n",
    "\n",
    "    session_widgets[\"model_status\"].value = \"\"\n",
    "    session_widgets[\"autoencoder_output\"].clear_output()\n",
    "    session_widgets[\"predictor_output\"].clear_output()\n",
    "\n",
    "def on_load_models(_):\n",
    "    messages = []\n",
    "    auto_path = session_widgets[\"autoencoder_path\"].value.strip()\n",
    "    predictor_path = session_widgets[\"predictor_path\"].value.strip()\n",
    "\n",
    "    if auto_path:\n",
    "        if os.path.exists(auto_path):\n",
    "            try:\n",
    "                session_state[\"autoencoder\"] = load_autoencoder_model(auto_path, device)\n",
    "                reset_feature_cache()\n",
    "                messages.append(f\"Autoencoder loaded from `{auto_path}`.\")\n",
    "            except Exception as exc:\n",
    "                session_state[\"autoencoder\"] = None\n",
    "                messages.append(f\"<span style='color:red'>Failed to load autoencoder: {exc}</span>\")\n",
    "        else:\n",
    "            session_state[\"autoencoder\"] = None\n",
    "            messages.append(f\"<span style='color:red'>Autoencoder path not found: {auto_path}</span>\")\n",
    "    else:\n",
    "        session_state[\"autoencoder\"] = None\n",
    "        messages.append(\"Autoencoder path is empty; skipping load.\")\n",
    "\n",
    "    if predictor_path:\n",
    "        if os.path.exists(predictor_path):\n",
    "            try:\n",
    "                session_state[\"predictor\"] = load_predictor_model(predictor_path, device)\n",
    "                messages.append(f\"Predictor loaded from `{predictor_path}`.\")\n",
    "            except Exception as exc:\n",
    "                session_state[\"predictor\"] = None\n",
    "                messages.append(f\"<span style='color:red'>Failed to load predictor: {exc}</span>\")\n",
    "        else:\n",
    "            session_state[\"predictor\"] = None\n",
    "            messages.append(f\"<span style='color:red'>Predictor path not found: {predictor_path}</span>\")\n",
    "    else:\n",
    "        session_state[\"predictor\"] = None\n",
    "        messages.append(\"Predictor path is empty; skipping load.\")\n",
    "\n",
    "    session_widgets[\"model_status\"].value = \"<br>\".join(messages)\n",
    "\n",
    "def on_run_autoencoder(_):\n",
    "    autoencoder = session_state.get(\"autoencoder\")\n",
    "    if autoencoder is None:\n",
    "        with session_widgets[\"autoencoder_output\"]:\n",
    "            session_widgets[\"autoencoder_output\"].clear_output()\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "        return\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with session_widgets[\"autoencoder_output\"]:\n",
    "            session_widgets[\"autoencoder_output\"].clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        return\n",
    "\n",
    "    idx = frame_slider.value\n",
    "    observation = session_state.get(\"observations\", [])[idx]\n",
    "    frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = autoencoder.reconstruct(frame_tensor)\n",
    "    mse = torch.nn.functional.mse_loss(reconstructed, frame_tensor).item()\n",
    "\n",
    "    original_img = tensor_to_numpy_image(frame_tensor)\n",
    "    reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "\n",
    "    with session_widgets[\"autoencoder_output\"]:\n",
    "        session_widgets[\"autoencoder_output\"].clear_output()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Input\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"Reconstruction MSE: {mse:.6f}\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "\n",
    "def on_history_slider_change(_):\n",
    "    if \"frame_slider\" in session_widgets and \"update_history_preview\" in session_widgets:\n",
    "        session_widgets[\"update_history_preview\"](session_widgets[\"frame_slider\"].value)\n",
    "\n",
    "session_dropdown = widgets.Dropdown(description=\"Session\", layout=widgets.Layout(width=\"300px\"))\n",
    "session_widgets[\"session_dropdown\"] = session_dropdown\n",
    "\n",
    "refresh_button = widgets.Button(description=\"Refresh\", icon=\"refresh\")\n",
    "load_session_button = widgets.Button(description=\"Load Session\", button_style=\"primary\")\n",
    "\n",
    "session_area = widgets.Output()\n",
    "session_widgets[\"session_area\"] = session_area\n",
    "\n",
    "autoencoder_path = widgets.Text(value=DEFAULT_AUTOENCODER_PATH, description=\"Autoencoder\", layout=widgets.Layout(width=\"520px\"))\n",
    "predictor_path = widgets.Text(value=DEFAULT_PREDICTOR_PATH, description=\"Predictor\", layout=widgets.Layout(width=\"520px\"))\n",
    "session_widgets[\"autoencoder_path\"] = autoencoder_path\n",
    "session_widgets[\"predictor_path\"] = predictor_path\n",
    "\n",
    "model_status = widgets.HTML()\n",
    "session_widgets[\"model_status\"] = model_status\n",
    "\n",
    "run_autoencoder_button = widgets.Button(description=\"Run Autoencoder\", button_style=\"success\", icon=\"play\")\n",
    "autoencoder_output = widgets.Output()\n",
    "session_widgets[\"autoencoder_output\"] = autoencoder_output\n",
    "\n",
    "history_slider = widgets.IntSlider(value=3, min=2, max=8, description=\"History\", continuous_update=False)\n",
    "session_widgets[\"history_slider\"] = history_slider\n",
    "history_slider.observe(on_history_slider_change, names=\"value\")\n",
    "\n",
    "run_predictor_button = widgets.Button(description=\"Run Predictor\", button_style=\"info\", icon=\"forward\")\n",
    "predictor_output = widgets.Output()\n",
    "session_widgets[\"predictor_output\"] = predictor_output\n",
    "\n",
    "refresh_button.on_click(on_refresh_sessions)\n",
    "load_session_button.on_click(on_load_session)\n",
    "load_models_button = widgets.Button(description=\"Load Models\", button_style=\"primary\", icon=\"upload\")\n",
    "load_models_button.on_click(on_load_models)\n",
    "run_autoencoder_button.on_click(on_run_autoencoder)\n",
    "run_predictor_button.on_click(on_run_predictor)\n",
    "\n",
    "on_refresh_sessions()\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([session_dropdown, refresh_button, load_session_button]),\n",
    "    session_area,\n",
    "    widgets.HTML(\"<hr><b>Model Checkpoints</b>\"),\n",
    "    autoencoder_path,\n",
    "    predictor_path,\n",
    "    load_models_button,\n",
    "    model_status,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Autoencoder Inference</b>\"),\n",
    "        widgets.HTML(\"Uses the currently selected frame.\"),\n",
    "        run_autoencoder_button,\n",
    "        autoencoder_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Predictor Inference</b>\"),\n",
    "        widgets.HTML(\"History uses frames leading up to the current selection to predict the next observation.\"),\n",
    "        history_slider,\n",
    "        run_predictor_button,\n",
    "        predictor_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Autoencoder Training (AdaptiveWorldModel)</b>\"),\n",
    "        widgets.HTML(\"Train the autoencoder using AdaptiveWorldModel.train_autoencoder() with randomized masking.\"),\n",
    "        widgets.HBox([autoencoder_threshold, autoencoder_max_steps]),\n",
    "        widgets.HBox([train_autoencoder_threshold_button, train_autoencoder_steps_button, autoencoder_steps]),\n",
    "        autoencoder_training_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Predictor Training (AdaptiveWorldModel)</b>\"),\n",
    "        widgets.HTML(\"Train the predictor using AdaptiveWorldModel.train_predictor() with joint autoencoder training.\"),\n",
    "        widgets.HBox([predictor_threshold, predictor_max_steps]),\n",
    "        widgets.HBox([train_predictor_threshold_button, train_predictor_steps_button, predictor_steps]),\n",
    "        predictor_training_output,\n",
    "    ]),\n",
    "]))\n",
    "\n",
    "\n",
    "# In[ ]:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d13c4b9b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac931cb4a07b46bbaee1bfd207d90eab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HBox(children=(Dropdown(description='Session', index=1, layout=Layout(width='300px'), options=(…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Interactive controls and callbacks\n",
    "session_state = {\n",
    "    \"session_name\": None,\n",
    "    \"session_dir\": None,\n",
    "    \"metadata\": {},\n",
    "    \"events\": [],\n",
    "    \"observations\": [],\n",
    "    \"actions\": [],\n",
    "    \"autoencoder\": None,\n",
    "    \"predictor\": None,\n",
    "    \"feature_cache\": {},\n",
    "    \"action_space\": [],\n",
    "}\n",
    "\n",
    "session_widgets = {}\n",
    "\n",
    "def reset_feature_cache():\n",
    "    session_state[\"feature_cache\"] = {}\n",
    "\n",
    "def on_refresh_sessions(_=None):\n",
    "    options = list_session_dirs(SESSIONS_BASE_DIR)\n",
    "    current = session_widgets[\"session_dropdown\"].value if \"session_dropdown\" in session_widgets else None\n",
    "    session_widgets[\"session_dropdown\"].options = options\n",
    "    if not options:\n",
    "        session_widgets[\"session_dropdown\"].value = None\n",
    "    elif current in options:\n",
    "        session_widgets[\"session_dropdown\"].value = current\n",
    "    else:\n",
    "        session_widgets[\"session_dropdown\"].value = options[-1]\n",
    "\n",
    "def on_load_session(_):\n",
    "    dropdown = session_widgets[\"session_dropdown\"]\n",
    "    session_name = dropdown.value\n",
    "    if not session_name:\n",
    "        return\n",
    "    session_dir = os.path.join(SESSIONS_BASE_DIR, session_name)\n",
    "    metadata = load_session_metadata(session_dir)\n",
    "    events = load_session_events(session_dir)\n",
    "    observations = extract_observations(events, session_dir)\n",
    "    actions = extract_actions(events)\n",
    "\n",
    "    session_state.update({\n",
    "        \"session_name\": session_name,\n",
    "        \"session_dir\": session_dir,\n",
    "        \"metadata\": metadata,\n",
    "        \"events\": events,\n",
    "        \"observations\": observations,\n",
    "        \"actions\": actions,\n",
    "    })\n",
    "    session_state[\"action_space\"] = get_action_space(session_state)\n",
    "    reset_feature_cache()\n",
    "    tensor_cache.clear()\n",
    "    load_frame_bytes.cache_clear()\n",
    "\n",
    "    with session_widgets[\"session_area\"]:\n",
    "        session_widgets[\"session_area\"].clear_output()\n",
    "        if not observations:\n",
    "            display(Markdown(f\"**{session_name}** has no observation frames.\"))\n",
    "            return\n",
    "        details = [\n",
    "            f\"**Session:** {session_name}\",\n",
    "            f\"**Total events:** {len(events)}\",\n",
    "            f\"**Observations:** {len(observations)}\",\n",
    "            f\"**Actions:** {len(actions)}\",\n",
    "        ]\n",
    "        if metadata:\n",
    "            start_time = metadata.get(\"start_time\")\n",
    "            if start_time:\n",
    "                details.append(f\"**Start:** {start_time}\")\n",
    "            robot_type = metadata.get(\"robot_type\")\n",
    "            if robot_type:\n",
    "                details.append(f\"**Robot:** {robot_type}\")\n",
    "        display(Markdown(\"<br>\".join(details)))\n",
    "\n",
    "        # Action distribution analysis\n",
    "        display(Markdown(\"### Action Distribution\"))\n",
    "\n",
    "        # Get all unique actions from the action space\n",
    "        from collections import Counter\n",
    "        action_space_list = session_state.get(\"action_space\", [])\n",
    "        \n",
    "        # Calculate action counts for current session\n",
    "        action_counts_session = Counter()\n",
    "        for action_entry in actions:\n",
    "            action_dict = action_entry.get(\"action\", {})\n",
    "            action_label = format_action_label(action_dict)\n",
    "            action_counts_session[action_label] += 1\n",
    "\n",
    "        # Calculate action counts across all sessions\n",
    "        action_counts_all = Counter()\n",
    "        all_session_dirs = list_session_dirs(SESSIONS_BASE_DIR)\n",
    "        for sess_name in all_session_dirs:\n",
    "            sess_dir = os.path.join(SESSIONS_BASE_DIR, sess_name)\n",
    "            sess_events = load_session_events(sess_dir)\n",
    "            sess_actions = extract_actions(sess_events)\n",
    "            for action_entry in sess_actions:\n",
    "                action_dict = action_entry.get(\"action\", {})\n",
    "                action_label = format_action_label(action_dict)\n",
    "                action_counts_all[action_label] += 1\n",
    "\n",
    "        # Build table showing each action with counts and percentages\n",
    "        if action_space_list:\n",
    "            total_session = sum(action_counts_session.values())\n",
    "            total_all = sum(action_counts_all.values())\n",
    "            \n",
    "            # Create table header\n",
    "            table_lines = [\n",
    "                \"| Action | Current Session Count | Current Session % | All Sessions Count | All Sessions % |\",\n",
    "                \"|--------|----------------------|-------------------|--------------------|--------------  |\"\n",
    "            ]\n",
    "            \n",
    "            # Add row for each action in the action space\n",
    "            for action_dict in action_space_list:\n",
    "                action_label = format_action_label(action_dict)\n",
    "                \n",
    "                # Current session stats\n",
    "                count_session = action_counts_session.get(action_label, 0)\n",
    "                pct_session = (100 * count_session / total_session) if total_session > 0 else 0\n",
    "                \n",
    "                # All sessions stats\n",
    "                count_all = action_counts_all.get(action_label, 0)\n",
    "                pct_all = (100 * count_all / total_all) if total_all > 0 else 0\n",
    "                \n",
    "                table_lines.append(\n",
    "                    f\"| {action_label} | {count_session} | {pct_session:.1f}% | {count_all} | {pct_all:.1f}% |\"\n",
    "                )\n",
    "            \n",
    "            # Add totals row\n",
    "            table_lines.append(\n",
    "                f\"| **TOTAL** | **{total_session}** | **100.0%** | **{total_all}** | **100.0%** |\"\n",
    "            )\n",
    "            \n",
    "            display(Markdown(chr(10).join(table_lines)))\n",
    "            \n",
    "            # Summary\n",
    "            display(Markdown(f\"**Total sessions analyzed:** {len(all_session_dirs)}\"))\n",
    "        else:\n",
    "            display(Markdown(\"*No action space defined or no actions recorded.*\"))\n",
    "\n",
    "\n",
    "        frame_slider = widgets.IntSlider(value=0, min=0, max=len(observations) - 1, description=\"Frame\", continuous_update=False)\n",
    "        play_widget = widgets.Play(interval=100, value=0, min=0, max=len(observations) - 1, step=1, description=\"Play\")\n",
    "        widgets.jslink((play_widget, \"value\"), (frame_slider, \"value\"))\n",
    "\n",
    "        frame_image = widgets.Image(format=\"jpg\")\n",
    "        frame_image.layout.width = \"448px\"\n",
    "        frame_info = widgets.HTML()\n",
    "        history_preview = widgets.Output()\n",
    "\n",
    "        session_widgets[\"frame_slider\"] = frame_slider\n",
    "        session_widgets[\"play_widget\"] = play_widget\n",
    "        session_widgets[\"frame_image\"] = frame_image\n",
    "        session_widgets[\"frame_info\"] = frame_info\n",
    "        session_widgets[\"history_preview\"] = history_preview\n",
    "\n",
    "        def update_history_preview(idx):\n",
    "            if \"history_preview\" not in session_widgets:\n",
    "                return\n",
    "            history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "            requested = history_slider_widget.value if history_slider_widget else 3\n",
    "            requested = max(1, requested)\n",
    "            requested = min(requested, idx + 1)\n",
    "            start = max(0, idx - requested + 1)\n",
    "            obs_slice = observations[start: idx + 1]\n",
    "            events_local = session_state.get(\"events\", [])\n",
    "            display_items = []\n",
    "            for offset, obs in enumerate(obs_slice):\n",
    "                frame_bytes = load_frame_bytes(obs[\"full_path\"])\n",
    "                border_color = \"#4caf50\" if (start + offset) == idx else \"#cccccc\"\n",
    "                image = widgets.Image(value=frame_bytes, format=\"jpg\", layout=widgets.Layout(width=\"160px\", height=\"120px\", border=f\"2px solid {border_color}\"))\n",
    "                label_text = f\"Step {obs['step']}\"\n",
    "                if (start + offset) == idx:\n",
    "                    label_text += \" (current)\"\n",
    "                label = widgets.HTML(value=f\"<div style='text-align:center; font-size:10px'>{label_text}</div>\")\n",
    "                display_items.append(widgets.VBox([image, label]))\n",
    "                if offset < len(obs_slice) - 1:\n",
    "                    next_obs = obs_slice[offset + 1]\n",
    "                    actions_between = [events_local[i] for i in range(obs[\"event_index\"] + 1, next_obs[\"event_index\"]) if events_local[i].get(\"type\") == \"action\"]\n",
    "                    if actions_between:\n",
    "                        action_text = \"; \".join(format_action_label(act.get(\"data\", {})) for act in actions_between)\n",
    "                    else:\n",
    "                        action_text = \"No action\"\n",
    "                    action_label = widgets.HTML(value=f\"<div style='font-size:10px; padding:0 6px;'>Action: {action_text}</div>\", layout=widgets.Layout(height=\"120px\", display=\"flex\", align_items=\"center\", justify_content=\"center\"))\n",
    "                    display_items.append(action_label)\n",
    "            session_widgets[\"history_preview\"].clear_output()\n",
    "            with session_widgets[\"history_preview\"]:\n",
    "                if display_items:\n",
    "                    layout = widgets.Layout(display=\"flex\", flex_flow=\"row\", align_items=\"center\")\n",
    "                    display(widgets.HBox(display_items, layout=layout))\n",
    "                else:\n",
    "                    display(Markdown(\"History preview unavailable for this frame.\"))\n",
    "\n",
    "        session_widgets[\"update_history_preview\"] = update_history_preview\n",
    "\n",
    "        def update_frame(change):\n",
    "            idx_local = change[\"new\"] if isinstance(change, dict) else change\n",
    "            observation = observations[idx_local]\n",
    "            frame_image.value = load_frame_bytes(observation[\"full_path\"])\n",
    "            frame_info.value = f\"<b>Observation {idx_local + 1} / {len(observations)}</b><br>Step: {observation['step']}<br>Timestamp: {format_timestamp(observation['timestamp'])}\"\n",
    "            update_history_preview(idx_local)\n",
    "\n",
    "        frame_slider.observe(update_frame, names=\"value\")\n",
    "        update_frame({\"new\": frame_slider.value})\n",
    "\n",
    "        display(widgets.VBox([\n",
    "            widgets.HBox([play_widget, frame_slider]),\n",
    "            frame_image,\n",
    "            frame_info,\n",
    "            widgets.HTML(\"<b>History preview</b>\"),\n",
    "            history_preview,\n",
    "        ]))\n",
    "\n",
    "    session_widgets[\"model_status\"].value = \"\"\n",
    "    session_widgets[\"autoencoder_output\"].clear_output()\n",
    "    session_widgets[\"predictor_output\"].clear_output()\n",
    "\n",
    "def on_load_models(_):\n",
    "    messages = []\n",
    "    auto_path = session_widgets[\"autoencoder_path\"].value.strip()\n",
    "    predictor_path = session_widgets[\"predictor_path\"].value.strip()\n",
    "\n",
    "    if auto_path:\n",
    "        if os.path.exists(auto_path):\n",
    "            try:\n",
    "                session_state[\"autoencoder\"] = load_autoencoder_model(auto_path, device)\n",
    "                reset_feature_cache()\n",
    "                messages.append(f\"Autoencoder loaded from `{auto_path}`.\")\n",
    "            except Exception as exc:\n",
    "                session_state[\"autoencoder\"] = None\n",
    "                messages.append(f\"<span style='color:red'>Failed to load autoencoder: {exc}</span>\")\n",
    "        else:\n",
    "            session_state[\"autoencoder\"] = None\n",
    "            messages.append(f\"<span style='color:red'>Autoencoder path not found: {auto_path}</span>\")\n",
    "    else:\n",
    "        session_state[\"autoencoder\"] = None\n",
    "        messages.append(\"Autoencoder path is empty; skipping load.\")\n",
    "\n",
    "    if predictor_path:\n",
    "        if os.path.exists(predictor_path):\n",
    "            try:\n",
    "                session_state[\"predictor\"] = load_predictor_model(predictor_path, device)\n",
    "                messages.append(f\"Predictor loaded from `{predictor_path}`.\")\n",
    "            except Exception as exc:\n",
    "                session_state[\"predictor\"] = None\n",
    "                messages.append(f\"<span style='color:red'>Failed to load predictor: {exc}</span>\")\n",
    "        else:\n",
    "            session_state[\"predictor\"] = None\n",
    "            messages.append(f\"<span style='color:red'>Predictor path not found: {predictor_path}</span>\")\n",
    "    else:\n",
    "        session_state[\"predictor\"] = None\n",
    "        messages.append(\"Predictor path is empty; skipping load.\")\n",
    "\n",
    "    session_widgets[\"model_status\"].value = \"<br>\".join(messages)\n",
    "\n",
    "def on_run_autoencoder(_):\n",
    "    autoencoder = session_state.get(\"autoencoder\")\n",
    "    if autoencoder is None:\n",
    "        with session_widgets[\"autoencoder_output\"]:\n",
    "            session_widgets[\"autoencoder_output\"].clear_output()\n",
    "            display(Markdown(\"Load the autoencoder checkpoint first.\"))\n",
    "        return\n",
    "    frame_slider = session_widgets.get(\"frame_slider\")\n",
    "    if frame_slider is None:\n",
    "        with session_widgets[\"autoencoder_output\"]:\n",
    "            session_widgets[\"autoencoder_output\"].clear_output()\n",
    "            display(Markdown(\"Load a session to select frames.\"))\n",
    "        return\n",
    "\n",
    "    idx = frame_slider.value\n",
    "    observation = session_state.get(\"observations\", [])[idx]\n",
    "    frame_tensor = get_frame_tensor(session_state[\"session_dir\"], observation[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "\n",
    "    autoencoder.eval()\n",
    "    with torch.no_grad():\n",
    "        reconstructed = autoencoder.reconstruct(frame_tensor)\n",
    "    mse = torch.nn.functional.mse_loss(reconstructed, frame_tensor).item()\n",
    "\n",
    "    original_img = tensor_to_numpy_image(frame_tensor)\n",
    "    reconstructed_img = tensor_to_numpy_image(reconstructed)\n",
    "\n",
    "    with session_widgets[\"autoencoder_output\"]:\n",
    "        session_widgets[\"autoencoder_output\"].clear_output()\n",
    "        fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        axes[0].imshow(original_img)\n",
    "        axes[0].set_title(\"Input\")\n",
    "        axes[0].axis(\"off\")\n",
    "        axes[1].imshow(reconstructed_img)\n",
    "        axes[1].set_title(f\"Reconstruction MSE: {mse:.6f}\")\n",
    "        axes[1].axis(\"off\")\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# REMOVED: on_run_predictor definition (now in cell 9)\n",
    "# Original function commented out to avoid duplicate definitions\n",
    "# def on_run_predictor(_):\n",
    "#     autoencoder = session_state.get(\"autoencoder\")\n",
    "#     predictor = session_state.get(\"predictor\")\n",
    "#     frame_slider = session_widgets.get(\"frame_slider\")\n",
    "\n",
    "#     with session_widgets[\"predictor_output\"]:\n",
    "#         session_widgets[\"predictor_output\"].clear_output()\n",
    "#         if autoencoder is None or predictor is None:\n",
    "#             display(Markdown(\"Load both autoencoder and predictor checkpoints first.\"))\n",
    "#             return\n",
    "#         if frame_slider is None:\n",
    "#             display(Markdown(\"Load a session to select frames.\"))\n",
    "#             return\n",
    "\n",
    "#         target_idx = frame_slider.value\n",
    "#         history_slider_widget = session_widgets.get(\"history_slider\")\n",
    "#         desired_history = history_slider_widget.value if history_slider_widget else 3\n",
    "\n",
    "#         selected_obs, action_dicts, error = build_predictor_sequence(session_state, target_idx, desired_history)\n",
    "#         if error:\n",
    "#             display(Markdown(f\"**Cannot run predictor:** {error}\"))\n",
    "#             return\n",
    "\n",
    "#         actual_history = len(selected_obs)\n",
    "#         if history_slider_widget and actual_history != history_slider_widget.value:\n",
    "#             history_slider_widget.value = actual_history\n",
    "\n",
    "#         feature_history = []\n",
    "#         for obs in selected_obs:\n",
    "#             cached = session_state[\"feature_cache\"].get(obs[\"frame_path\"])\n",
    "#             if cached is None:\n",
    "#                 tensor = get_frame_tensor(session_state[\"session_dir\"], obs[\"frame_path\"]).unsqueeze(0).to(device)\n",
    "#                 autoencoder.eval()\n",
    "#                 with torch.no_grad():\n",
    "#                     encoded = autoencoder.encode(tensor)\n",
    "#                 session_state[\"feature_cache\"][obs[\"frame_path\"]] = encoded.detach().cpu()\n",
    "#                 cached = session_state[\"feature_cache\"][obs[\"frame_path\"]]\n",
    "#             feature_history.append(cached)\n",
    "\n",
    "#         feature_history_gpu = [feat.to(device) for feat in feature_history]\n",
    "\n",
    "#         recorded_future_action, action_source = get_future_action_for_prediction(session_state, target_idx)\n",
    "#         recorded_action = clone_action(recorded_future_action) if recorded_future_action is not None else {}\n",
    "\n",
    "#         predictor.eval()\n",
    "#         autoencoder.eval()\n",
    "\n",
    "#         if recorded_future_action is None:\n",
    "#             display(Markdown(\"No recorded action between current and next frame; using an empty action for comparison.\"))\n",
    "#         elif action_source == \"previous\":\n",
    "#             display(Markdown(\"Using the most recent action prior to the current frame for comparison.\"))\n",
    "#         predictor.eval()\n",
    "#         autoencoder.eval()\n",
    "\n",
    "#         if recorded_future_action is None:\n",
    "#             display(Markdown(\"No recorded action between current and next frame; using an empty action for comparison.\"))\n",
    "\n",
    "#         def predict_for_action(action_dict):\n",
    "#             history_actions = [clone_action(act) for act in action_dicts]\n",
    "#             if action_dict is not None:\n",
    "#                 history_actions.append(clone_action(action_dict))\n",
    "#             else:\n",
    "#                 history_actions.append({})\n",
    "            \n",
    "#             # Normalize action for FiLM conditioning\n",
    "#             if history_actions:\n",
    "#                 action_normalized = normalize_action_dicts([history_actions[-1]]).to(device)\n",
    "#             else:\n",
    "#                 action_normalized = torch.zeros(1, len(config.ACTION_CHANNELS), device=device)\n",
    "            \n",
    "#             # Get last features for delta prediction\n",
    "#             last_features = feature_history_gpu[-1] if feature_history_gpu else None\n",
    "            \n",
    "#             pred_features = predictor(\n",
    "#                 feature_history_gpu,\n",
    "#                 history_actions,\n",
    "#                 action_normalized=action_normalized,\n",
    "#                 last_features=last_features\n",
    "#             )\n",
    "#             decoded_candidate = decode_features_to_image(autoencoder, pred_features)\n",
    "#             return decoded_candidate\n",
    "\n",
    "#         next_obs = session_state[\"observations\"][target_idx + 1] if target_idx + 1 < len(session_state[\"observations\"]) else None\n",
    "#         actual_tensor_cpu = None\n",
    "#         actual_tensor_gpu = None\n",
    "#         actual_img = None\n",
    "#         if next_obs is not None:\n",
    "#             actual_tensor_cpu = get_frame_tensor(session_state[\"session_dir\"], next_obs[\"frame_path\"]).unsqueeze(0)\n",
    "#             actual_tensor_gpu = actual_tensor_cpu.to(device)\n",
    "#             actual_img = tensor_to_numpy_image(actual_tensor_cpu)\n",
    "\n",
    "#         all_predictions = []\n",
    "#         with torch.no_grad():\n",
    "#             recorded_pred_tensor = predict_for_action(recorded_action if recorded_future_action is not None else None)\n",
    "#             recorded_img = tensor_to_numpy_image(recorded_pred_tensor)\n",
    "#             recorded_mse = None\n",
    "#             if actual_tensor_gpu is not None:\n",
    "#                 recorded_mse = torch.nn.functional.mse_loss(recorded_pred_tensor, actual_tensor_gpu).item()\n",
    "#             recorded_label = f\"Recorded action: {format_action_label(recorded_action)}\" if recorded_future_action is not None else \"Recorded action (none)\"\n",
    "#             all_predictions.append({\n",
    "#                 \"label\": recorded_label,\n",
    "#                 \"action\": clone_action(recorded_action),\n",
    "#                 \"image\": recorded_img,\n",
    "#                 \"mse\": recorded_mse,\n",
    "#             })\n",
    "\n",
    "#             for idx, action in enumerate(session_state.get(\"action_space\", [])):\n",
    "#                 if actions_equal(action, recorded_action):\n",
    "#                     continue\n",
    "#                 pred_tensor = predict_for_action(action)\n",
    "#                 pred_img = tensor_to_numpy_image(pred_tensor)\n",
    "#                 mse_value = None\n",
    "#                 if actual_tensor_gpu is not None:\n",
    "#                     mse_value = torch.nn.functional.mse_loss(pred_tensor, actual_tensor_gpu).item()\n",
    "#                 all_predictions.append({\n",
    "#                     \"label\": f\"{idx + 1}. {format_action_label(action)}\",\n",
    "#                     \"action\": clone_action(action),\n",
    "#                     \"image\": pred_img,\n",
    "#                     \"mse\": mse_value,\n",
    "#                 })\n",
    "\n",
    "#         history_steps_text = \", \".join(str(obs[\"step\"]) for obs in selected_obs)\n",
    "#         action_lines = []\n",
    "#         for idx, action in enumerate(action_dicts, 1):\n",
    "#             action_lines.append(f\"{idx}. {format_action_label(action)}\")\n",
    "#         if not action_lines:\n",
    "#             action_lines.append(\"(No actions in window)\")\n",
    "\n",
    "#         display(Markdown(f\"**History steps:** {history_steps_text}\"))\n",
    "#         display(Markdown(\"**Recorded actions:**<br>\" + \"<br>\".join(action_lines)))\n",
    "\n",
    "#         history_fig, history_axes = plt.subplots(1, len(selected_obs), figsize=(3 * len(selected_obs), 3))\n",
    "#         if isinstance(history_axes, np.ndarray):\n",
    "#             axes_list = history_axes.flatten()\n",
    "#         else:\n",
    "#             axes_list = [history_axes]\n",
    "#         for idx, (obs, ax) in enumerate(zip(selected_obs, axes_list)):\n",
    "#             img = np.array(load_frame_image(obs[\"full_path\"]))\n",
    "#             ax.imshow(img)\n",
    "#             ax.set_title(f\"Step {obs['step']}\")\n",
    "#             ax.axis(\"off\")\n",
    "#             if idx < len(action_dicts):\n",
    "#                 ax.set_xlabel(format_action_label(action_dicts[idx]), fontsize=9)\n",
    "#         plt.tight_layout()\n",
    "#         plt.show()\n",
    "\n",
    "#         display(Markdown(f\"## Predictions for step {selected_obs[-1]['step']}\"))\n",
    "\n",
    "#         if all_predictions:\n",
    "#             cols = min(4, len(all_predictions))\n",
    "#             rows = math.ceil(len(all_predictions) / cols)\n",
    "#             fig, axes = plt.subplots(rows, cols, figsize=(4 * cols, 3.5 * rows))\n",
    "#             axes = np.array(axes).reshape(rows, cols)\n",
    "#             for idx, prediction in enumerate(all_predictions):\n",
    "#                 ax = axes[idx // cols][idx % cols]\n",
    "#                 ax.imshow(prediction[\"image\"])\n",
    "#                 title = prediction[\"label\"]\n",
    "#                 if prediction[\"mse\"] is not None:\n",
    "#                     title += f\"MSE: {prediction['mse']:.6f}\"\n",
    "#                 ax.set_title(title, fontsize=9)\n",
    "#                 ax.axis(\"off\")\n",
    "#             for idx in range(len(all_predictions), rows * cols):\n",
    "#                 axes[idx // cols][idx % cols].axis(\"off\")\n",
    "#             plt.tight_layout()\n",
    "#             plt.show()\n",
    "#         else:\n",
    "#             display(Markdown(\"No actions available to visualize predictions.\"))\n",
    "\n",
    "def on_history_slider_change(_):\n",
    "    if \"frame_slider\" in session_widgets and \"update_history_preview\" in session_widgets:\n",
    "        session_widgets[\"update_history_preview\"](session_widgets[\"frame_slider\"].value)\n",
    "\n",
    "session_dropdown = widgets.Dropdown(description=\"Session\", layout=widgets.Layout(width=\"300px\"))\n",
    "session_widgets[\"session_dropdown\"] = session_dropdown\n",
    "\n",
    "refresh_button = widgets.Button(description=\"Refresh\", icon=\"refresh\")\n",
    "load_session_button = widgets.Button(description=\"Load Session\", button_style=\"primary\")\n",
    "\n",
    "session_area = widgets.Output()\n",
    "session_widgets[\"session_area\"] = session_area\n",
    "\n",
    "autoencoder_path = widgets.Text(value=DEFAULT_AUTOENCODER_PATH, description=\"Autoencoder\", layout=widgets.Layout(width=\"520px\"))\n",
    "predictor_path = widgets.Text(value=DEFAULT_PREDICTOR_PATH, description=\"Predictor\", layout=widgets.Layout(width=\"520px\"))\n",
    "session_widgets[\"autoencoder_path\"] = autoencoder_path\n",
    "session_widgets[\"predictor_path\"] = predictor_path\n",
    "\n",
    "model_status = widgets.HTML()\n",
    "session_widgets[\"model_status\"] = model_status\n",
    "\n",
    "run_autoencoder_button = widgets.Button(description=\"Run Autoencoder\", button_style=\"success\", icon=\"play\")\n",
    "autoencoder_output = widgets.Output()\n",
    "session_widgets[\"autoencoder_output\"] = autoencoder_output\n",
    "\n",
    "history_slider = widgets.IntSlider(value=3, min=2, max=8, description=\"History\", continuous_update=False)\n",
    "session_widgets[\"history_slider\"] = history_slider\n",
    "history_slider.observe(on_history_slider_change, names=\"value\")\n",
    "\n",
    "run_predictor_button = widgets.Button(description=\"Run Predictor\", button_style=\"info\", icon=\"forward\")\n",
    "predictor_output = widgets.Output()\n",
    "session_widgets[\"predictor_output\"] = predictor_output\n",
    "\n",
    "refresh_button.on_click(on_refresh_sessions)\n",
    "load_session_button.on_click(on_load_session)\n",
    "load_models_button = widgets.Button(description=\"Load Models\", button_style=\"primary\", icon=\"upload\")\n",
    "load_models_button.on_click(on_load_models)\n",
    "run_autoencoder_button.on_click(on_run_autoencoder)\n",
    "run_predictor_button.on_click(on_run_predictor)\n",
    "\n",
    "on_refresh_sessions()\n",
    "\n",
    "display(widgets.VBox([\n",
    "    widgets.HBox([session_dropdown, refresh_button, load_session_button]),\n",
    "    session_area,\n",
    "    widgets.HTML(\"<hr><b>Model Checkpoints</b>\"),\n",
    "    autoencoder_path,\n",
    "    predictor_path,\n",
    "    load_models_button,\n",
    "    model_status,\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Autoencoder Inference</b>\"),\n",
    "        widgets.HTML(\"Uses the currently selected frame.\"),\n",
    "        run_autoencoder_button,\n",
    "        autoencoder_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Predictor Inference</b>\"),\n",
    "        widgets.HTML(\"History uses frames leading up to the current selection to predict the next observation.\"),\n",
    "        history_slider,\n",
    "        run_predictor_button,\n",
    "        predictor_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Autoencoder Training (AdaptiveWorldModel)</b>\"),\n",
    "        widgets.HTML(\"Train the autoencoder using AdaptiveWorldModel.train_autoencoder() with randomized masking.\"),\n",
    "        widgets.HBox([autoencoder_threshold, autoencoder_max_steps]),\n",
    "        widgets.HBox([train_autoencoder_threshold_button, train_autoencoder_steps_button, autoencoder_steps]),\n",
    "        autoencoder_training_output,\n",
    "    ]),\n",
    "    widgets.HTML(\"<hr>\"),\n",
    "    widgets.VBox([\n",
    "        widgets.HTML(\"<b>Predictor Training (AdaptiveWorldModel)</b>\"),\n",
    "        widgets.HTML(\"Train the predictor using AdaptiveWorldModel.train_predictor() with joint autoencoder training.\"),\n",
    "        widgets.HBox([predictor_threshold, predictor_max_steps]),\n",
    "        widgets.HBox([train_predictor_threshold_button, train_predictor_steps_button, predictor_steps]),\n",
    "        predictor_training_output,\n",
    "    ]),\n",
    "]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfbc5fdd-62e7-4e37-b5da-36352227707a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33d7265d-df2b-4297-94fd-d020e2a9b475",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
